{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65544ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import wikipedia\n",
    "\n",
    "import ahocorasick\n",
    "import openai\n",
    "import pdfplumber\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "dataDir = \"../data/\"\n",
    "dataName = \"Deep Learning.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad0e7c",
   "metadata": {},
   "source": [
    "# 设置 api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d6d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_base = \"https://api.chatanywhere.com.cn/v1\"\n",
    "# openai.api_base = 'https://api.chatanywhere.cn/v1'\n",
    "openai.api_key = \"sk-D1u13WweY1LhWLqv95Ml7e3y8f8ToSfsTkGnlgvSQLqZJptC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a068fc59-e078-468f-95a0-5937f26aeb6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-7B-Base\", use_fast=False, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Base\", trust_remote_code=True)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# inputs = tokenizer('登鹳雀楼->王之涣\\n夜雨寄北->', return_tensors='pt')\n",
    "# inputs = inputs.to('cuda:0')\n",
    "# pred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\n",
    "# print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b80029",
   "metadata": {},
   "source": [
    "# 一，读取数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590efdbd",
   "metadata": {},
   "source": [
    "## 生成目录架构\n",
    "\n",
    "- 生成**章节名称**与**章节序号**的对应：`name_dict`\n",
    "- 以及**章节序号**与**章节页码**范围的对应：`index_dict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1612cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(dataDir + dataName) as f:\n",
    "    # 目录架构生成\n",
    "    c, p, n = [], [], []\n",
    "    for i in range(7):\n",
    "        page = f.pages[i]\n",
    "        text = page.extract_text()\n",
    "        text_split = text.split(\"\\n\")\n",
    "        for i in text_split:\n",
    "            if bool(re.match(\"[0-9]+\\.[0-9]+\", i.split(\" \")[0])):\n",
    "                c.append(i.split(\" \")[0])\n",
    "                p.append(i.split(\" \")[-1])\n",
    "            if bool(re.match(\"[0-9]+\", i.split(\" \")[0])):\n",
    "                for j in i.split(\" \"):\n",
    "                    if bool(re.match(\"[A-Za-z]+\", j)):\n",
    "                        n.append((i.split(\" \")[0], j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65673d5",
   "metadata": {},
   "source": [
    "### name_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "665533ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {}\n",
    "for i, j in n:\n",
    "    if i in name_dict:\n",
    "        name_dict[i] = name_dict.get(i, \"\") + \" \" + j\n",
    "    else:\n",
    "        name_dict[i] = name_dict.get(i, \"\") + j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6bbf5",
   "metadata": {},
   "source": [
    "### index_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98571f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_range = list(zip(p, p[1:]))\n",
    "p_range.append((720, 800))\n",
    "c_p_range = list(zip(c, p_range))\n",
    "index_dict = collections.defaultdict(list)\n",
    "for k, v in c_p_range:\n",
    "    index_dict[k.split(\".\")[0]].append((k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b056028",
   "metadata": {},
   "source": [
    "## 生成内容表\n",
    "\n",
    "- 段落内容表：`content_dict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7993e527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pdfplumber.open(dataDir + dataName) as f:\n",
    "    content_dict = collections.defaultdict(list)\n",
    "\n",
    "    for k, v in index_dict.items():\n",
    "        for i in v:\n",
    "            page_range = i[-1]\n",
    "\n",
    "            for j in range(int(page_range[0]), int(page_range[1])):\n",
    "                page = f.pages[j]\n",
    "\n",
    "                text = page.extract_text().replace('\\n', ' ')\n",
    "\n",
    "                content_dict[i[0]].append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd1b8",
   "metadata": {},
   "source": [
    "# 二，Build Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30146ad",
   "metadata": {},
   "source": [
    "## 目录 + 前置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dd4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalogue_relations(dataName, name_dict=None, relation_type=[\"目录\", \"前置\"]):\n",
    "    c_relations = []\n",
    "    p_relations = []\n",
    "    if not name_dict:\n",
    "        name_dict = {}\n",
    "    for k, v in name_dict.items():\n",
    "        if bool(re.match(\"[0-9]+\\.[0-9]+\", k)):\n",
    "            p_relations.append(\n",
    "                [\n",
    "                    k.split(\".\")[0] + \" \" + name_dict[k.split(\".\")[0]],\n",
    "                    relation_type[1],\n",
    "                    k + \" \" + v,\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            c_relations.append(\n",
    "                [\n",
    "                    dataName,\n",
    "                    relation_type[0],\n",
    "                    k + \" \" + v,\n",
    "                ]\n",
    "            )\n",
    "    return p_relations, c_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ec1631-fccf-4ae6-8095-aaf1e03e4ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_relations, c_relations = catalogue_relations(\n",
    "    dataName=dataName.split(\".\")[0], name_dict=name_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c6694-6ee9-4347-854a-c2caf41895ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 包含 + 段落共现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33964ebb-e42e-498a-982d-2bdfa5ceafd3",
   "metadata": {},
   "source": [
    "### AC自动机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3374c69-3635-4a74-97d9-0394405c1756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build(patterns):\n",
    "    trie = ahocorasick.Automaton()\n",
    "    for index, word in enumerate(patterns):\n",
    "        trie.add_word(word, (index, word))\n",
    "    trie.make_automaton()\n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "95fa4b1d-495c-4cb7-9ea0-c17e630472c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(1, 21):\n",
    "    with open(dataDir + 'raw_relations/' + 'kb_chapter_' + str(i) + '.json', 'r') as f:\n",
    "        data.append([i, json.load(f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c3da1a4-a7c1-4be7-95a5-a3e39c89ffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 87\n",
      "2 64\n",
      "3 85\n",
      "4 49\n",
      "5 193\n",
      "6 163\n",
      "7 131\n",
      "8 155\n",
      "9 125\n",
      "10 129\n",
      "11 68\n",
      "12 134\n",
      "13 39\n",
      "14 73\n",
      "15 98\n",
      "16 96\n",
      "17 47\n",
      "18 70\n",
      "19 67\n",
      "20 225\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(i[0], len(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2f887bc-6d13-46b8-a844-ea51219142c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entity_set = set()\n",
    "for i in data:\n",
    "    for relation in i[1]:\n",
    "        if relation[0] not in entity_set:\n",
    "            entity_set.add(relation[0])\n",
    "        if relation[2] not in entity_set:\n",
    "            entity_set.add(relation[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d352606b-70db-4ba2-a46d-f5a9270959e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(\n",
    "    i in entity_set\n",
    "    for i in [\n",
    "        \"Adam\",\n",
    "        \"SGD\",\n",
    "        \"RSMProp\",\n",
    "        \"MLP\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "74bcfee9-ce96-4de0-bb45-dc346ea29421",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    page = wikipedia.page([\"MLP\"], auto_suggest=True)\n",
    "except:\n",
    "    print('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d90eb6c7-48f9-4708-a7d5-9f15d17fb702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "page = wikipedia.page(title=\"MLP\", auto_suggest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8ac6e769-fcdb-4e69-aac5-65b733c5e6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mauto_suggest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Get a WikipediaPage object for the page with title `title` or the pageid\n",
       "`pageid` (mutually exclusive).\n",
       "\n",
       "Keyword arguments:\n",
       "\n",
       "* title - the title of the page to load\n",
       "* pageid - the numeric pageid of the page to load\n",
       "* auto_suggest - let Wikipedia find a valid page title for the query\n",
       "* redirect - allow redirection without raising RedirectError\n",
       "* preload - load content, summary, images, references, and links during initialization\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\crime\\.conda\\envs\\env_pytorch\\lib\\site-packages\\wikipedia\\wikipedia.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikipedia.page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7dec8c85-ca9b-4989-a835-818aa2190c47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Light-year'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4f954a52-2b4e-4780-82ab-666a4fb085c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0',\n",
       " '10',\n",
       " '2',\n",
       " '2015',\n",
       " '2020',\n",
       " '3',\n",
       " '7',\n",
       " '8½',\n",
       " '9',\n",
       " 'A priori and a posteriori',\n",
       " 'Algorithm',\n",
       " 'Allele',\n",
       " 'Alpha',\n",
       " 'Ancestor',\n",
       " 'Antipathy',\n",
       " 'Apathy',\n",
       " 'Application-specific integrated circuit',\n",
       " 'Approximate inference',\n",
       " 'Approximation',\n",
       " 'Architecture',\n",
       " 'Arithmetic mean',\n",
       " 'Arithmetic underflow',\n",
       " 'Array',\n",
       " 'Array (data type)',\n",
       " 'Artificial intelligence',\n",
       " 'Artificial neural network',\n",
       " 'Atrophy',\n",
       " 'Autoencoder',\n",
       " 'Backpropagation',\n",
       " 'Backpropagation through time',\n",
       " 'Batch normalization',\n",
       " 'Bayesian linear regression',\n",
       " 'Bayesian probability',\n",
       " 'Bernoulli distribution',\n",
       " 'Beta',\n",
       " 'Bias',\n",
       " 'Biology',\n",
       " 'Bipartite graph',\n",
       " 'Boltzmann machine',\n",
       " 'Brain',\n",
       " 'Branch',\n",
       " 'Breast',\n",
       " 'Broyden–Fletcher–Goldfarb–Shanno algorithm',\n",
       " 'COVID-19 pandemic',\n",
       " 'CUDA',\n",
       " 'Cambridge, Massachusetts',\n",
       " 'Canada',\n",
       " 'Canyon',\n",
       " 'Central processing unit',\n",
       " 'Classification',\n",
       " 'Cluster analysis',\n",
       " 'Code',\n",
       " 'Cognitive science',\n",
       " 'Complex number',\n",
       " 'Computation',\n",
       " 'Computational biology',\n",
       " 'Computational resource',\n",
       " 'Computer science',\n",
       " 'Computer scientist',\n",
       " 'Computer vision',\n",
       " 'Conditional probability distribution',\n",
       " 'Conference on Neural Information Processing Systems',\n",
       " 'Constrained optimization',\n",
       " 'Consumer electronics',\n",
       " 'Converse (logic)',\n",
       " 'Convex optimization',\n",
       " 'Convolution',\n",
       " 'Convolutional neural network',\n",
       " 'Cross-entropy',\n",
       " 'Curvature',\n",
       " 'Cybernetics',\n",
       " 'Cyc',\n",
       " 'Data',\n",
       " 'Data model',\n",
       " 'Data processing',\n",
       " 'Data set',\n",
       " 'Data structure',\n",
       " 'David H. Hubel',\n",
       " 'Decision tree',\n",
       " 'Deep learning',\n",
       " 'Definite quadratic form',\n",
       " 'Derivative',\n",
       " 'Determinism',\n",
       " 'Dialect',\n",
       " 'Dialectic',\n",
       " 'Distributive property',\n",
       " 'Dream',\n",
       " 'Early stopping',\n",
       " 'Education',\n",
       " 'Eigendecomposition of a matrix',\n",
       " 'Eigenvalues and eigenvectors',\n",
       " 'Estimator',\n",
       " 'Exact test',\n",
       " 'Expected value',\n",
       " 'Family',\n",
       " 'Feature learning',\n",
       " 'Feedforward',\n",
       " 'Female',\n",
       " 'G-Eazy discography',\n",
       " 'Genealogy',\n",
       " 'Generative model',\n",
       " 'Geoffrey Hinton',\n",
       " 'Google DeepMind',\n",
       " 'Google Maps',\n",
       " 'Google Street View',\n",
       " 'Gradient',\n",
       " 'Gradient descent',\n",
       " 'Graphics processing unit',\n",
       " 'H',\n",
       " 'Harris chain',\n",
       " 'Hessian matrix',\n",
       " 'Hierarchy',\n",
       " 'Higher-order function',\n",
       " 'Hill climbing',\n",
       " 'Human brain',\n",
       " 'Hyperparameter',\n",
       " 'Hyperparameter optimization',\n",
       " 'IEEE Transactions on Signal Processing',\n",
       " 'ImageNet',\n",
       " 'Inference',\n",
       " 'Inferior temporal gyrus',\n",
       " 'Information',\n",
       " 'Information theory',\n",
       " 'Initial point',\n",
       " 'Institute of Electrical and Electronics Engineers',\n",
       " 'Integral',\n",
       " 'Intelligence',\n",
       " 'International Conference on Computer Vision',\n",
       " 'International Conference on Learning Representations',\n",
       " 'Isotropy',\n",
       " 'Iteration',\n",
       " 'Journal of the ACM',\n",
       " 'Journal of the Royal Statistical Society',\n",
       " 'K-means clustering',\n",
       " 'K-nearest neighbors algorithm',\n",
       " 'Ketamine',\n",
       " 'Knowledge base',\n",
       " 'Lambda',\n",
       " 'Language',\n",
       " 'Language model',\n",
       " 'Las Vegas algorithm',\n",
       " 'Lasso (statistics)',\n",
       " 'Learning',\n",
       " 'Least squares',\n",
       " 'Levelling',\n",
       " 'Levenberg–Marquardt algorithm',\n",
       " 'Library',\n",
       " 'Library (computing)',\n",
       " 'Linear algebra',\n",
       " 'Linear model',\n",
       " 'Linear regression',\n",
       " 'Linguistics',\n",
       " 'List of things named after Carl Friedrich Gauss',\n",
       " 'List of things named after Thomas Bayes',\n",
       " 'List of time periods',\n",
       " 'Logistic regression',\n",
       " 'Long short-term memory',\n",
       " 'Loss function',\n",
       " 'MIT Press',\n",
       " 'Machine learning',\n",
       " 'Machine translation',\n",
       " 'Male',\n",
       " 'Man',\n",
       " 'Markov chain',\n",
       " 'Markov chain Monte Carlo',\n",
       " 'Mathematical optimization',\n",
       " 'Mathematics',\n",
       " 'Matrix multiplication',\n",
       " 'Maximum likelihood estimation',\n",
       " 'McGill University',\n",
       " 'Mean squared error',\n",
       " 'Memory',\n",
       " 'Metadata',\n",
       " 'Metaheuristic',\n",
       " 'Metropolis–Hastings algorithm',\n",
       " 'Microsoft',\n",
       " 'Minimally invasive procedure',\n",
       " 'Miocene',\n",
       " 'Misfit stream',\n",
       " 'Model',\n",
       " 'Momentum',\n",
       " 'Monte Carlo algorithm',\n",
       " 'Monte Carlo method',\n",
       " 'Montreal',\n",
       " 'N-gram',\n",
       " 'Natural language',\n",
       " 'Natural language processing',\n",
       " 'Nearest neighbour algorithm',\n",
       " 'Negative definiteness',\n",
       " 'Nervous system',\n",
       " 'Neural coding',\n",
       " 'Neural machine translation',\n",
       " 'Neural network',\n",
       " 'Neuron',\n",
       " 'Neuroscience',\n",
       " 'Neurotransmitter',\n",
       " 'Nonlinear dimensionality reduction',\n",
       " 'Nonparametric statistics',\n",
       " 'Norm (mathematics)',\n",
       " 'Normal distribution',\n",
       " 'Normalizing constant',\n",
       " 'Nvidia',\n",
       " 'One-dimensional space',\n",
       " 'One-hot',\n",
       " 'Operability',\n",
       " 'Optimization problem',\n",
       " 'Overfitting',\n",
       " 'P',\n",
       " 'Parametric equation',\n",
       " 'Parametric model',\n",
       " 'Parsing',\n",
       " 'Perceptron',\n",
       " 'Phi',\n",
       " 'Phylogenetic tree',\n",
       " 'Phylogenetics',\n",
       " 'Plane (mathematics)',\n",
       " 'Politeness',\n",
       " 'Population density',\n",
       " 'Positive definiteness',\n",
       " 'Prediction',\n",
       " 'Prevalence',\n",
       " 'Principal component analysis',\n",
       " 'Prior probability',\n",
       " 'Probability',\n",
       " 'Probability distribution',\n",
       " 'Probability theory',\n",
       " 'Randomized algorithm',\n",
       " 'Rapid eye movement sleep',\n",
       " 'Recall (memory)',\n",
       " 'Recurrent neural network',\n",
       " 'Reinforcement learning',\n",
       " 'Restricted Boltzmann machine',\n",
       " 'Retina',\n",
       " 'Retinal',\n",
       " 'Robustness',\n",
       " 'Rodent',\n",
       " 'Round-off error',\n",
       " 'Saddle point',\n",
       " 'School',\n",
       " 'Second derivative',\n",
       " 'Sexual reproduction',\n",
       " 'Sigma',\n",
       " 'Sign',\n",
       " 'Simulation',\n",
       " 'Sine and cosine',\n",
       " 'Singular value decomposition',\n",
       " 'Slavery',\n",
       " 'Software',\n",
       " 'Software engineering',\n",
       " 'Software framework',\n",
       " 'Sparse approximation',\n",
       " 'Spearmint',\n",
       " 'Speech recognition',\n",
       " 'Statistical learning theory',\n",
       " 'Statistical model',\n",
       " 'Statistics',\n",
       " 'Stochastic',\n",
       " 'Stochastic gradient descent',\n",
       " 'Strong prior',\n",
       " 'Summation',\n",
       " 'Supervised learning',\n",
       " 'Supine',\n",
       " 'Support vector machine',\n",
       " 'T-distributed stochastic neighbor embedding',\n",
       " 'Tangent',\n",
       " 'Teacher',\n",
       " 'Teacher forcing',\n",
       " 'Television set',\n",
       " 'Tensor',\n",
       " 'TensorFlow',\n",
       " 'Terry Sejnowski',\n",
       " 'Theano',\n",
       " 'Third derivative',\n",
       " 'Three-dimensional space',\n",
       " 'Torch',\n",
       " 'Torsten Wiesel',\n",
       " 'Training',\n",
       " 'Training, validation, and test data sets',\n",
       " 'Transfer learning',\n",
       " 'Tree',\n",
       " 'Uncertainty',\n",
       " 'Unit of observation',\n",
       " 'University of Toronto',\n",
       " 'Université de Montréal',\n",
       " 'Unsupervised',\n",
       " 'Unsupervised learning',\n",
       " 'Usurper',\n",
       " 'Valley',\n",
       " 'Vapnik–Chervonenkis dimension',\n",
       " 'Variance',\n",
       " 'Variational Bayesian methods',\n",
       " 'Variational autoencoder',\n",
       " 'Wakefulness',\n",
       " 'Weak supervision',\n",
       " 'Woman',\n",
       " 'Word n-gram language model',\n",
       " 'WordNet',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Yoshua Bengio',\n",
       " 'Zero-shot learning'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6db9746b-2e5d-4ba1-9acf-d6911be6c9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def include_co_presence(entity_set):\n",
    "    patterns = list(entity_set)\n",
    "    trie = build(patterns)\n",
    "    include_relations = []\n",
    "    co_presence_relations = set()\n",
    "    for chapter, sections in index_dict.items():\n",
    "        for section in sections:\n",
    "            for content in content_dict[section[0]]:\n",
    "                word_set = set(word[1][1] for word in trie.iter(content))\n",
    "                for head in word_set:\n",
    "                    if not bool(re.match(\"^(\\d+|[A-Za-z])$\", head)):\n",
    "                        include_relations.append(\n",
    "                            [section[0] + \" \" + name_dict[section[0]], \"include\", head]\n",
    "                        )\n",
    "                        for tail in word_set:\n",
    "                            if head != tail and not bool(\n",
    "                                re.match(\"^(\\d+|[A-Za-z])$\", tail)\n",
    "                            ):\n",
    "                                if (\n",
    "                                    tail,\n",
    "                                    \"co_presence\",\n",
    "                                    head,\n",
    "                                ) not in co_presence_relations:\n",
    "                                    co_presence_relations.add(\n",
    "                                        (head, \"co_presence\", tail)\n",
    "                                    )\n",
    "    return include_relations, co_presence_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab6d9231-654f-4697-9515-3079672ed128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "include_relations, co_presence_relations = include_co_presence(entity_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319119a-cd22-4a74-b941-cfddd2052873",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 结果展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d66e3ea7-0d49-488c-8dae-394d1b2976d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 Introduction', '前置', '1.1 Who Should Read This Book?']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_relations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abf4941e-e982-41f1-b41c-5322c7e52238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep Learning', '目录', '1 Introduction']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_relations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fa90760-e6bd-4056-a566-092cad016f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1508"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(co_presence_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2779489f-06c4-4af9-8959-daefd2d748f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.1 Who Should Read This Book?', 'include', 'Computation']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_relations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "50b2c125-a610-4ca4-9f86-54189d45bc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_relations = (\n",
    "    p_relations + c_relations + list(co_presence_relations) + include_relations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a648211-88c3-4493-b165-47710300077d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3080"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(structure_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "031f1845-3a55-487c-9da9-555aa5c33aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(dataDir + \"/relations\", f\"structure_relations.json\"), \"w\") as f:\n",
    "    json.dump(structure_relations, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda55af9",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98291941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relations = [\"目录\", \"前置\", \"句子共现\", \"段落共现\", \"频繁项集\"]  # pending\n",
    "\n",
    "# conversation_list = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"请只提取文本中的命名实体，格式为[{entity_A}, {entity_B}, ...], 不要返回任何其他内容\",\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# bot = Chat(conversation_list)\n",
    "# answer = bot.ask(content_dict[\"1.1\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2292fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# res_tiny = [i.strip() for i in re.sub('\"|}|{||\\[|\\]', \"\", answer).split(\",\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5849673",
   "metadata": {},
   "source": [
    "# 一些其他尝试\n",
    "\n",
    "N - Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 暂时没有探索结果\n",
    "# all_text = re.sub('[^A-Za-z0-9\\.]+', ' ', text).lower().split(' ')\n",
    "# ng1 = collections.defaultdict(int)\n",
    "# ng2 = collections.defaultdict(int)\n",
    "# ng3 = collections.defaultdict(int)\n",
    "# ng4 = collections.defaultdict(int)\n",
    "# for i, j in enumerate(all_text):\n",
    "#     ng1[j] += 1\n",
    "#     if i > 0: ng2[(all_text[i-1], j)] += 1\n",
    "#     if i > 1: ng3[(all_text[i-2], all_text[i-1], j)] += 1\n",
    "#     if i > 2: ng4[(all_text[i-3], all_text[i-2], all_text[i-1], j)] += 1\n",
    "\n",
    "# def sort_feq(dic):\n",
    "#     return sorted([(k, v) for k, v in dic.items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64b84f",
   "metadata": {},
   "source": [
    "### 定义 Chat 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e486417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def total_counts(response):\n",
    "#     tokens_nums = int(response[\"usage\"][\"total_tokens\"])\n",
    "#     price = 0.002 / 1000\n",
    "#     cost = \"{:.5f}\".format(price * tokens_nums * 7.5)\n",
    "#     print(f\"tokens: {tokens_nums}, cost: {cost}\")\n",
    "\n",
    "#     return float(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf30deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Chat:\n",
    "#     def __init__(self, conversation_list=[]):\n",
    "#         self.conversation_list = conversation_list\n",
    "#         self.costs_list = []\n",
    "\n",
    "#     def show_conversation(self, msg_list):\n",
    "#         for msg in msg_list[-2:]:\n",
    "#             if msg[\"role\"] == \"user\":\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 message = msg[\"content\"]\n",
    "#                 print(f\"\\U0001f47D: {message}\\n\")\n",
    "#             print()\n",
    "\n",
    "#     def ask(self, prompt):\n",
    "#         self.conversation_list.append({\"role\": \"user\", \"content\": prompt})\n",
    "#         openai.api_key = \"sk-D1u13WweY1LhWLqv95Ml7e3y8f8ToSfsTkGnlgvSQLqZJptC\"\n",
    "#         response = openai.ChatCompletion.create(\n",
    "#             model=\"gpt-3.5-turbo\", messages=self.conversation_list\n",
    "#         )\n",
    "#         answer = response.choices[0].message[\"content\"]\n",
    "\n",
    "#         self.conversation_list.append({\"role\": \"assistant\", \"content\": answer})\n",
    "#         self.show_conversation(self.conversation_list)\n",
    "\n",
    "#         cost = total_counts(response)\n",
    "#         self.costs_list.append(cost)\n",
    "#         return answer\n",
    "#         print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

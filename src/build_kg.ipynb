{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# import re\n",
            "# import collections\n",
            "# import pdfplumber\n",
            "# import os\n",
            "# import json\n",
            "\n",
            "# from classBook import Book\n",
            "\n",
            "\n",
            "# dataDir = \"../data/\"\n",
            "# dataName = \"Deep Learning.pdf\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Reading book:  Deep Learning.pdf from directory:  ../data/\n",
                  "Book loaded successfully\n",
                  "Number of pages:  800\n"
               ]
            }
         ],
         "source": [
            "import json\n",
            "import os\n",
            "import re\n",
            "\n",
            "import pdfplumber\n",
            "\n",
            "dataDir = \"../data/\"\n",
            "dataName = \"Deep Learning.pdf\"\n",
            "\n",
            "\n",
            "class Book:\n",
            "    num_pages = -1\n",
            "    book_end_page = \"735\"\n",
            "    page_offset = 15\n",
            "\n",
            "    def __init__(self, name=dataName, dataDir=dataDir):\n",
            "        self.name = name\n",
            "        self.dataDir = dataDir\n",
            "        self.pdf = self.loader()\n",
            "\n",
            "    def loader(self):\n",
            "        print(\"Reading book: \", self.name, \"from directory: \", self.dataDir)\n",
            "        try:\n",
            "            pdf = pdfplumber.open(os.path.join(self.dataDir, self.name))\n",
            "        except:\n",
            "            print(\"Error: File not found\")\n",
            "            return None\n",
            "        self.num_pages = len(pdf.pages)\n",
            "        print(\"Book loaded successfully\")\n",
            "        print(\"Number of pages: \", self.num_pages)\n",
            "        return pdf\n",
            "\n",
            "    def close(self):\n",
            "        self.pdf.close()\n",
            "\n",
            "    def readPage(self, page=-1):\n",
            "        if page == -1:\n",
            "            return self.pdf.pages\n",
            "        else:\n",
            "            return self.pdf.pages[page - 1]\n",
            "\n",
            "    def readPageInInterval(self, start_page, end_page, offset=page_offset):\n",
            "        return self.pdf.pages[start_page - 1 + offset : end_page - 1 + offset]\n",
            "\n",
            "    def searchStrInPage(self, page, str):\n",
            "        page_text = self.readPage(page)\n",
            "        return page_text.extract_text().lower().find(str.lower())\n",
            "\n",
            "    def getToc(self):\n",
            "        pages = []\n",
            "        for i in range(8):\n",
            "            if self.searchStrInPage(i, \"Contents\") != -1:\n",
            "                pages.append(i)\n",
            "        # reg expression to match '6 Deep Feedforward Networks 168'\n",
            "        pattern_chapter = re.compile(r\"(\\d+)\\s+(.*)\\s+(\\d+)\")\n",
            "        # match '6.1 Example: Learning XOR . . . . . . . . . . . . . . . . . . . . . . . 171',\n",
            "        pattern_section = re.compile(\n",
            "            r\"(\\d+)\\.(\\d+)\\s+([\\?\\,\\'\\â€™\\(\\)a-zA-Z\\:\\s\\-]+)\\s+.*\\s+(\\d+)\"\n",
            "        )\n",
            "\n",
            "        # save to dict\n",
            "        toc = {}\n",
            "        for page in pages:\n",
            "            page_text = self.readPage(page)\n",
            "            text = page_text.extract_text()\n",
            "            lines = text.split(\"\\n\")\n",
            "            for line in lines:\n",
            "                match_chapter = pattern_chapter.match(line)\n",
            "                match_section = pattern_section.match(line)\n",
            "                if match_chapter:\n",
            "                    chapter = {\n",
            "                        \"chapter\": match_chapter.group(1),\n",
            "                        \"title\": match_chapter.group(2),\n",
            "                        \"page\": match_chapter.group(3),\n",
            "                    }\n",
            "                elif match_section:\n",
            "                    section = {\n",
            "                        \"chapter\": match_section.group(1),\n",
            "                        \"section\": match_section.group(2),\n",
            "                        \"title\": match_section.group(3),\n",
            "                        \"page\": match_section.group(4),\n",
            "                    }\n",
            "                    if chapter[\"chapter\"] not in toc:\n",
            "                        toc[chapter[\"chapter\"]] = {\n",
            "                            \"title\": chapter[\"title\"],\n",
            "                            \"page\": chapter[\"page\"],\n",
            "                            \"sections\": [],\n",
            "                        }\n",
            "                    toc[chapter[\"chapter\"]][\"sections\"].append(section)\n",
            "\n",
            "        # add end page\n",
            "        for chapter in toc:\n",
            "            try:\n",
            "                toc[chapter][\"end_page\"] = toc[str(int(chapter) + 1)][\"page\"]\n",
            "            except:\n",
            "                toc[chapter][\"end_page\"] = self.book_end_page\n",
            "            for section in toc[chapter][\"sections\"]:\n",
            "                try:\n",
            "                    section[\"end_page\"] = toc[chapter][\"sections\"][\n",
            "                        int(section[\"section\"])\n",
            "                    ][\"page\"]\n",
            "                except:\n",
            "                    section[\"end_page\"] = toc[chapter][\"end_page\"]\n",
            "        # write to json\n",
            "        with open(os.path.join(self.dataDir, \"toc.json\"), \"w\") as f:\n",
            "            json.dump(toc, f, indent=4)\n",
            "        return len(toc)\n",
            "\n",
            "    def loadToc(self):\n",
            "        with open(os.path.join(self.dataDir, \"toc.json\"), \"r\") as f:\n",
            "            toc = json.load(f)\n",
            "        return toc\n",
            "\n",
            "    def getChapter(self, chapter: str):\n",
            "        toc = self.loadToc()\n",
            "        page = toc[chapter][\"page\"]\n",
            "        end_page = toc[chapter][\"end_page\"]\n",
            "        return self.readPageInInterval(int(page), int(end_page))\n",
            "\n",
            "    # read by sections, since we need to extract the relations between section and entity.\n",
            "    def getSection(self, chapter: str, section: str):\n",
            "        # transform section number to index\n",
            "        section_idx = int(section) - 1\n",
            "        toc = self.loadToc()\n",
            "        page = toc[chapter][\"sections\"][section_idx][\"page\"]\n",
            "        end_page = toc[chapter][\"sections\"][section_idx][\"end_page\"]\n",
            "        return self.readPageInInterval(int(page), int(end_page))\n",
            "\n",
            "\n",
            "book = Book()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "import math\n",
            "\n",
            "import IPython\n",
            "import torch\n",
            "import wikipedia\n",
            "from pyvis.network import Network\n",
            "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "BartForConditionalGeneration(\n",
                     "  (model): BartModel(\n",
                     "    (shared): Embedding(50272, 1024, padding_idx=1)\n",
                     "    (encoder): BartEncoder(\n",
                     "      (embed_tokens): Embedding(50272, 1024, padding_idx=1)\n",
                     "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                     "      (layers): ModuleList(\n",
                     "        (0-11): 12 x BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "    )\n",
                     "    (decoder): BartDecoder(\n",
                     "      (embed_tokens): Embedding(50272, 1024, padding_idx=1)\n",
                     "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                     "      (layers): ModuleList(\n",
                     "        (0-11): 12 x BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "    )\n",
                     "  )\n",
                     "  (lm_head): Linear(in_features=1024, out_features=50272, bias=False)\n",
                     ")"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Load model and tokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
            "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
            "\n",
            "# Specify the device\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# Move the model to the device\n",
            "model.to(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "def extract_relations_from_model_output(text):\n",
            "    relations = []\n",
            "    relation, subject, relation, object_ = \"\", \"\", \"\", \"\"\n",
            "    text = text.strip()\n",
            "    current = \"x\"\n",
            "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
            "    for token in text_replaced.split():\n",
            "        if token == \"<triplet>\":\n",
            "            current = \"t\"\n",
            "            if relation != \"\":\n",
            "                relations.append(\n",
            "                    {\n",
            "                        \"head\": subject.strip(),\n",
            "                        \"type\": relation.strip(),\n",
            "                        \"tail\": object_.strip(),\n",
            "                    }\n",
            "                )\n",
            "                relation = \"\"\n",
            "            subject = \"\"\n",
            "        elif token == \"<subj>\":\n",
            "            current = \"s\"\n",
            "            if relation != \"\":\n",
            "                relations.append(\n",
            "                    {\n",
            "                        \"head\": subject.strip(),\n",
            "                        \"type\": relation.strip(),\n",
            "                        \"tail\": object_.strip(),\n",
            "                    }\n",
            "                )\n",
            "            object_ = \"\"\n",
            "        elif token == \"<obj>\":\n",
            "            current = \"o\"\n",
            "            relation = \"\"\n",
            "        else:\n",
            "            if current == \"t\":\n",
            "                subject += \" \" + token\n",
            "            elif current == \"s\":\n",
            "                object_ += \" \" + token\n",
            "            elif current == \"o\":\n",
            "                relation += \" \" + token\n",
            "    if subject != \"\" and relation != \"\" and object_ != \"\":\n",
            "        relations.append(\n",
            "            {\"head\": subject.strip(), \"type\": relation.strip(), \"tail\": object_.strip()}\n",
            "        )\n",
            "    return relations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "class KB:\n",
            "    def __init__(self):\n",
            "        self.entities = {}\n",
            "        self.relations = set()\n",
            "        self.raw_relations = set()\n",
            "\n",
            "    def get_wikipedia_data(self, candidate_entity):\n",
            "        try:\n",
            "            page = wikipedia.page(candidate_entity, auto_suggest=False)\n",
            "            entity_data = {\n",
            "                \"title\": page.title,\n",
            "                \"url\": page.url,\n",
            "                \"summary\": page.summary,\n",
            "            }\n",
            "            return entity_data\n",
            "        except:\n",
            "            return None\n",
            "\n",
            "    def print(self):\n",
            "        print(\"Entities:\")\n",
            "        for e in self.entities:\n",
            "            print(f\"  {e} -> {self.entities[e]}\")\n",
            "        print(\"Relations:\")\n",
            "        for r in self.relations:\n",
            "            print(f\"  {r}\")\n",
            "\n",
            "    def add_entity(self, e):\n",
            "        self.entities[e[\"title\"]] = {k: v for k, v in e.items() if k != \"title\"}\n",
            "\n",
            "    def add_relation(self, r):\n",
            "        triplet = (r[\"head\"], r[\"type\"], r[\"tail\"])\n",
            "        self.raw_relations.add(triplet)\n",
            "        # check on wikipedia\n",
            "        candidate_entities = [r[\"head\"], r[\"tail\"]]\n",
            "        entities = [self.get_wikipedia_data(ent) for ent in candidate_entities]\n",
            "\n",
            "        # if one entity does not exist, stop\n",
            "        if any(ent is None for ent in entities):\n",
            "            return\n",
            "\n",
            "        # manage new entities\n",
            "        for e in entities:\n",
            "            self.add_entity(e)\n",
            "\n",
            "        # rename relation entities with their wikipedia titles\n",
            "        r[\"head\"] = entities[0][\"title\"]\n",
            "        r[\"tail\"] = entities[1][\"title\"]\n",
            "\n",
            "        # manage new relation\n",
            "        triplet = (r[\"head\"], r[\"type\"], r[\"tail\"])\n",
            "        self.relations.add(triplet)\n",
            "\n",
            "        # print(f\"Added relation: {triplet}\")\n",
            "        # print(self.relations)\n",
            "        # raise Exception and exit\n",
            "        # raise Exception(\"here\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_small_text_to_kb(text, verbose=False, max_length=512):\n",
            "    \"\"\"This function takes a text and returns a KB. Note that the text should be small enough to fit in the model's input.\n",
            "    Specifically, the text should be less than 512 tokens, which corresponds to about 380 English words.\n",
            "\n",
            "    Args:\n",
            "        text (_type_): text used to generate the KB.\n",
            "        verbose (bool, optional): If True, prints the number of tokens in the text. Defaults to False.\n",
            "    Returns:\n",
            "        KB: Knowledge base generated from the text.\n",
            "    \"\"\"\n",
            "\n",
            "    kb = KB()\n",
            "\n",
            "    # Tokenizer text\n",
            "\n",
            "    model_inputs = tokenizer(\n",
            "        text, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\"\n",
            "    ).to(device)\n",
            "\n",
            "    if verbose:\n",
            "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
            "\n",
            "    # Generate\n",
            "\n",
            "    gen_kwargs = {\n",
            "        \"max_length\": 512,\n",
            "        \"length_penalty\": 0,\n",
            "        \"num_beams\": 3,\n",
            "        \"num_return_sequences\": 3,\n",
            "    }\n",
            "\n",
            "    generated_tokens = model.generate(\n",
            "        **model_inputs,\n",
            "        **gen_kwargs,\n",
            "    )\n",
            "\n",
            "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
            "\n",
            "    # create kb\n",
            "\n",
            "    for sentence_pred in decoded_preds:\n",
            "        relations = extract_relations_from_model_output(sentence_pred)\n",
            "\n",
            "        for r in relations:\n",
            "            kb.add_relation(r)\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_text_to_kb(text, span_length=128, verbose=False):\n",
            "    \"\"\"This function takes a text and returns a KB. There is no limit on the text length.\n",
            "    The text is split into spans of length span_length, and a KB is generated for each span.\n",
            "\n",
            "    Args:\n",
            "        text (_type_): text used to generate the KB.\n",
            "        span_length (int, optional): length of each span. Defaults to 128.\n",
            "        verbose (bool, optional): Defaults to False.\n",
            "\n",
            "    Returns:\n",
            "        KB: Knowledge base generated from the text.\n",
            "    \"\"\"\n",
            "\n",
            "    # tokenize whole text\n",
            "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
            "\n",
            "    # compute span boundaries\n",
            "    num_tokens = len(inputs[\"input_ids\"][0])\n",
            "    if verbose:\n",
            "        print(f\"Input has {num_tokens} tokens\")\n",
            "    num_spans = math.ceil(num_tokens / span_length)\n",
            "    if verbose:\n",
            "        print(f\"Input has {num_spans} spans\")\n",
            "    overlap = math.ceil((num_spans * span_length - num_tokens) / max(num_spans - 1, 1))\n",
            "    spans_boundaries = []\n",
            "    start = 0\n",
            "    for i in range(num_spans):\n",
            "        spans_boundaries.append(\n",
            "            [start + span_length * i, start + span_length * (i + 1)]\n",
            "        )\n",
            "        start -= overlap\n",
            "    if verbose:\n",
            "        print(f\"Span boundaries are {spans_boundaries}\")\n",
            "\n",
            "    # transform input with spans\n",
            "    tensor_ids = [\n",
            "        inputs[\"input_ids\"][0][boundary[0] : boundary[1]]\n",
            "        for boundary in spans_boundaries\n",
            "    ]\n",
            "    tensor_masks = [\n",
            "        inputs[\"attention_mask\"][0][boundary[0] : boundary[1]]\n",
            "        for boundary in spans_boundaries\n",
            "    ]\n",
            "    inputs = {\n",
            "        \"input_ids\": torch.stack(tensor_ids),\n",
            "        \"attention_mask\": torch.stack(tensor_masks),\n",
            "    }\n",
            "\n",
            "    # generate relations\n",
            "    num_return_sequences = 3\n",
            "    gen_kwargs = {\n",
            "        \"max_length\": 256,\n",
            "        \"length_penalty\": 0,\n",
            "        \"num_beams\": 3,\n",
            "        \"num_return_sequences\": num_return_sequences,\n",
            "    }\n",
            "    generated_tokens = model.generate(\n",
            "        **inputs,\n",
            "        **gen_kwargs,\n",
            "    )\n",
            "\n",
            "    # decode relations\n",
            "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
            "\n",
            "    # create kb\n",
            "    kb = KB()\n",
            "    i = 0\n",
            "    for sentence_pred in decoded_preds:\n",
            "        current_span_index = i // num_return_sequences\n",
            "        relations = extract_relations_from_model_output(sentence_pred)\n",
            "        for relation in relations:\n",
            "            relation[\"meta\"] = {\"spans\": [spans_boundaries[current_span_index]]}\n",
            "            kb.add_relation(relation)\n",
            "        i += 1\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [],
         "source": [
            "entityDir = os.path.join(dataDir, \"entities\")\n",
            "if not os.path.exists(entityDir):\n",
            "    os.makedirs(entityDir)\n",
            "relationDir = os.path.join(dataDir, \"relations\")\n",
            "if not os.path.exists(relationDir):\n",
            "    os.makedirs(relationDir)\n",
            "rawRelationDir = os.path.join(dataDir, \"raw_relations\")\n",
            "if not os.path.exists(rawRelationDir):\n",
            "    os.makedirs(rawRelationDir)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_chapter_to_kb(chapter_str: str, verbose=False):\n",
            "    print(\"Generating KB for chapter\", chapter_str)\n",
            "\n",
            "    chapter = book.getChapter(chapter_str)\n",
            "\n",
            "    kb = KB()\n",
            "    for page in chapter:\n",
            "        text = page.extract_text()\n",
            "\n",
            "        kb_page = from_small_text_to_kb(text, verbose=verbose, max_length=1024)\n",
            "        kb.entities.update(kb_page.entities)\n",
            "        kb.relations.update(kb_page.relations)\n",
            "        kb.raw_relations.update(kb_page.raw_relations)\n",
            "\n",
            "    with open(os.path.join(entityDir, f\"kb_chapter_{chapter_str}.json\"), \"w\") as f:\n",
            "        json.dump(kb.entities, f, indent=4)\n",
            "    with open(os.path.join(relationDir, f\"kb_chapter_{chapter_str}.json\"), \"w\") as f:\n",
            "        json.dump(list(kb.relations), f, indent=4)\n",
            "    with open(os.path.join(rawRelationDir, f\"kb_chapter_{chapter_str}.json\"), \"w\") as f:\n",
            "        json.dump(list(kb.raw_relations), f, indent=4)\n",
            "\n",
            "    print(f\"number of entities in chapter {chapter}: {len(kb.entities)}\")\n",
            "    print(f\"number of relations in chapter {chapter}: {len(kb.relations)}\")\n",
            "    print(f\"number of raw relations in chapter {chapter}: {len(kb.raw_relations)}\")\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [],
         "source": [
            "chapters = [str(i) for i in range(1, 21)]\n",
            "\n",
            "for chapter in chapters:\n",
            "    # from_chapter_to_kb(chapter, 1)\n",
            "    pass"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "def read_chapter_by_sections(chapter, save=False):\n",
            "    toc = book.loadToc()\n",
            "\n",
            "    res = {}\n",
            "    for section in toc[chapter][\"sections\"]:\n",
            "        print(f\"Reading chapter {chapter}, section {section['section']}\")\n",
            "        section_text = [\n",
            "            page.extract_text().replace(\"\\n\", \" \")\n",
            "            for page in book.getSection(chapter, section[\"section\"])\n",
            "        ]\n",
            "        kb = from_small_text_to_kb(section_text, verbose=True)\n",
            "        res[\n",
            "            section[\"chapter\"] + \".\" + section[\"section\"] + \" \" + section[\"title\"]\n",
            "        ] = kb.relations\n",
            "\n",
            "    if save:\n",
            "        with open(os.path.join(dataDir, f\"chapter_{chapter}.json\"), \"w\") as f:\n",
            "            json.dump(res, f, indent=4)\n",
            "\n",
            "    return res"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "1914\n",
                  "1302\n"
               ]
            }
         ],
         "source": [
            "# count the data file\n",
            "import os\n",
            "import json\n",
            "\n",
            "dataDir = \"../data/\"\n",
            "rawRelationDir = os.path.join(dataDir, \"raw_relations\")\n",
            "files = os.listdir(rawRelationDir)\n",
            "\n",
            "data = set()\n",
            "for file in files:\n",
            "    with open(os.path.join(rawRelationDir, file), \"r\") as f:\n",
            "        data.update(set(map(tuple, json.load(f))))\n",
            "\n",
            "print(len(data))\n",
            "\n",
            "# extract entities from raw relations\n",
            "entities = set()\n",
            "for relation in data:\n",
            "    entities.add(relation[0])\n",
            "    entities.add(relation[2])\n",
            "print(len(entities))\n",
            "\n",
            "cacheDir = os.path.join(dataDir, \"cache\")\n",
            "if not os.path.exists(cacheDir):\n",
            "    os.makedirs(cacheDir)\n",
            "\n",
            "# save relations and entities to cache\n",
            "with open(os.path.join(cacheDir, \"relations.json\"), \"w\") as f:\n",
            "    json.dump(list(data), f, indent=4)\n",
            "with open(os.path.join(cacheDir, \"entities.json\"), \"w\") as f:\n",
            "    json.dump(list(entities), f, indent=4)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
                  "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                  "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Chapter 1 Introduction Inventors have long dreamed of creating machines that think. This desire dates back to at least the time of ancient Greece. The mythical figures Pygmalion, Daedalus, and Hephaestus may all be interpreted as legendary inventors, and Galatea, Talos, and Pandora may all be regarded as artificial life (Ovid and Martin, 2004; Sparkes, 1996; Tandy, 1997). When programmable computers were first conceived, people wondered whether such machines might become intelligent, over a hundred years before one was built (Lovelace, 1842). Today, artificial intelligence (AI) is a thriving field with many practical applications and active research topics. We look to intelligent software to automate routine labor, understand speech or images, make diagnoses in medicine and support basic scientific research. In the early days of artificial intelligence, the field rapidly tackled and solved problems that are intellectually difficult for human beings but relatively straight- forward for computersâ€”problems that can be described by a list of formal, math- ematical rules. The true challenge to artificial intelligence proved to be solving the tasks that are easy for people to perform but hard for people to describe formallyâ€”problems that we solve intuitively, that feel automatic, like recognizing spoken words or faces in images. This book is about a solution to these more intuitive problems. This solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined in terms of its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all of the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these 1\n"
               ]
            }
         ],
         "source": [
            "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
            "from transformers import pipeline\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
            "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
            "\n",
            "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "The model 'BertModel' is not supported for ner. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n"
               ]
            }
         ],
         "source": [
            "# Load model directly\n",
            "from transformers import AutoModel, AutoTokenizer\n",
            "from transformers import pipeline\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
            "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
            "\n",
            "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_text = \"\".join(\n",
            "    [page.extract_text().replace(\"\\n\", \" \") for page in book.getChapter(\"1\")[:1]]\n",
            ")[:200]\n",
            "\n",
            "print(test_text)\n",
            "\n",
            "# example = \"My name is Wolfgang and I live in Berlin\"\n",
            "\n",
            "ner_results = nlp(test_text)\n",
            "for result in ner_results:\n",
            "    print(result)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# import spacy\n",
            "\n",
            "# test_text = \"\".join(\n",
            "#     [page.extract_text().replace(\"\\n\", \" \") for page in book.getChapter(\"1\")]\n",
            "# )\n",
            "\n",
            "# # Load the English model\n",
            "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
            "\n",
            "# # Process a text\n",
            "# doc = nlp(test_text)\n",
            "\n",
            "# # Iterate over the entities\n",
            "# for ent in doc.ents:\n",
            "#     # Print the entity text and its label\n",
            "#     print(ent.text, ent.label_)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}

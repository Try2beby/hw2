{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "# import re\n",
            "# import collections\n",
            "# import pdfplumber\n",
            "# import os\n",
            "# import json\n",
            "\n",
            "# from classBook import Book\n",
            "\n",
            "\n",
            "# dataDir = \"../data/\"\n",
            "# dataName = \"Deep Learning.pdf\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "import json\n",
            "import os\n",
            "import re\n",
            "\n",
            "import pdfplumber\n",
            "\n",
            "dataDir = \"../data/\"\n",
            "dataName = \"Deep Learning.pdf\"\n",
            "\n",
            "\n",
            "class Book:\n",
            "    num_pages = -1\n",
            "    book_end_page = \"735\"\n",
            "    page_offset = 15\n",
            "\n",
            "    def __init__(self, name=dataName, dataDir=dataDir):\n",
            "        self.name = name\n",
            "        self.dataDir = dataDir\n",
            "        self.pdf = self.loader()\n",
            "\n",
            "    def loader(self):\n",
            "        print(\"Reading book: \", self.name, \"from directory: \", self.dataDir)\n",
            "        try:\n",
            "            pdf = pdfplumber.open(os.path.join(self.dataDir, self.name))\n",
            "        except:\n",
            "            print(\"Error: File not found\")\n",
            "            return None\n",
            "        self.num_pages = len(pdf.pages)\n",
            "        print(\"Book loaded successfully\")\n",
            "        print(\"Number of pages: \", self.num_pages)\n",
            "        return pdf\n",
            "\n",
            "    def close(self):\n",
            "        self.pdf.close()\n",
            "\n",
            "    def readPage(self, page=-1):\n",
            "        if page == -1:\n",
            "            return self.pdf.pages\n",
            "        else:\n",
            "            return self.pdf.pages[page - 1]\n",
            "\n",
            "    def readPageInInterval(self, start_page, end_page, offset=page_offset):\n",
            "        return self.pdf.pages[start_page - 1 + offset : end_page - 1 + offset]\n",
            "\n",
            "    def searchStrInPage(self, page, str):\n",
            "        page_text = self.readPage(page)\n",
            "        return page_text.extract_text().lower().find(str.lower())\n",
            "\n",
            "    def getToc(self):\n",
            "        pages = []\n",
            "        for i in range(8):\n",
            "            if self.searchStrInPage(i, \"Contents\") != -1:\n",
            "                pages.append(i)\n",
            "        # reg expression to match '6 Deep Feedforward Networks 168'\n",
            "        pattern_chapter = re.compile(r\"(\\d+)\\s+(.*)\\s+(\\d+)\")\n",
            "        # match '6.1 Example: Learning XOR . . . . . . . . . . . . . . . . . . . . . . . 171',\n",
            "        pattern_section = re.compile(\n",
            "            r\"(\\d+)\\.(\\d+)\\s+([\\?\\,\\'\\â€™\\(\\)a-zA-Z\\:\\s\\-]+)\\s+.*\\s+(\\d+)\"\n",
            "        )\n",
            "\n",
            "        # save to dict\n",
            "        toc = {}\n",
            "        for page in pages:\n",
            "            page_text = self.readPage(page)\n",
            "            text = page_text.extract_text()\n",
            "            lines = text.split(\"\\n\")\n",
            "            for line in lines:\n",
            "                match_chapter = pattern_chapter.match(line)\n",
            "                match_section = pattern_section.match(line)\n",
            "                if match_chapter:\n",
            "                    chapter = {\n",
            "                        \"chapter\": match_chapter.group(1),\n",
            "                        \"title\": match_chapter.group(2),\n",
            "                        \"page\": match_chapter.group(3),\n",
            "                    }\n",
            "                elif match_section:\n",
            "                    section = {\n",
            "                        \"chapter\": match_section.group(1),\n",
            "                        \"section\": match_section.group(2),\n",
            "                        \"title\": match_section.group(3),\n",
            "                        \"page\": match_section.group(4),\n",
            "                    }\n",
            "                    if chapter[\"chapter\"] not in toc:\n",
            "                        toc[chapter[\"chapter\"]] = {\n",
            "                            \"title\": chapter[\"title\"],\n",
            "                            \"page\": chapter[\"page\"],\n",
            "                            \"sections\": [],\n",
            "                        }\n",
            "                    toc[chapter[\"chapter\"]][\"sections\"].append(section)\n",
            "\n",
            "        # add end page\n",
            "        for chapter in toc:\n",
            "            try:\n",
            "                toc[chapter][\"end_page\"] = toc[str(int(chapter) + 1)][\"page\"]\n",
            "            except:\n",
            "                toc[chapter][\"end_page\"] = self.book_end_page\n",
            "            for section in toc[chapter][\"sections\"]:\n",
            "                try:\n",
            "                    section[\"end_page\"] = toc[chapter][\"sections\"][\n",
            "                        int(section[\"section\"])\n",
            "                    ][\"page\"]\n",
            "                except:\n",
            "                    section[\"end_page\"] = toc[chapter][\"end_page\"]\n",
            "        # write to json\n",
            "        with open(os.path.join(self.dataDir, \"toc.json\"), \"w\") as f:\n",
            "            json.dump(toc, f, indent=4)\n",
            "        return len(toc)\n",
            "\n",
            "    def loadToc(self):\n",
            "        with open(os.path.join(self.dataDir, \"toc.json\"), \"r\") as f:\n",
            "            toc = json.load(f)\n",
            "        return toc\n",
            "\n",
            "    def getChapter(self, chapter: str):\n",
            "        toc = self.loadToc()\n",
            "        page = toc[chapter][\"page\"]\n",
            "        end_page = toc[chapter][\"end_page\"]\n",
            "        return self.readPageInInterval(int(page), int(end_page))\n",
            "\n",
            "    # read by sections, since we need to extract the relations between section and entity.\n",
            "    def getSection(self, chapter: str, section: str):\n",
            "        # transform section number to index\n",
            "        section_idx = int(section) - 1\n",
            "        toc = self.loadToc()\n",
            "        page = toc[chapter][\"sections\"][section_idx][\"page\"]\n",
            "        end_page = toc[chapter][\"sections\"][section_idx][\"end_page\"]\n",
            "        return self.readPageInInterval(int(page), int(end_page))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Reading book:  Deep Learning.pdf from directory:  ../data/\n",
                  "Book loaded successfully\n",
                  "Number of pages:  800\n"
               ]
            }
         ],
         "source": [
            "book = Book()\n",
            "# book.getToc()\n",
            "# toc = book.loadToc()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "import math\n",
            "\n",
            "import IPython\n",
            "import torch\n",
            "import wikipedia\n",
            "from pyvis.network import Network\n",
            "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "BartForConditionalGeneration(\n",
                     "  (model): BartModel(\n",
                     "    (shared): Embedding(50272, 1024, padding_idx=1)\n",
                     "    (encoder): BartEncoder(\n",
                     "      (embed_tokens): Embedding(50272, 1024, padding_idx=1)\n",
                     "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                     "      (layers): ModuleList(\n",
                     "        (0-11): 12 x BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "    )\n",
                     "    (decoder): BartDecoder(\n",
                     "      (embed_tokens): Embedding(50272, 1024, padding_idx=1)\n",
                     "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                     "      (layers): ModuleList(\n",
                     "        (0-11): 12 x BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "    )\n",
                     "  )\n",
                     "  (lm_head): Linear(in_features=1024, out_features=50272, bias=False)\n",
                     ")"
                  ]
               },
               "execution_count": 6,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Load model and tokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
            "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
            "\n",
            "# Specify the device\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# Move the model to the device\n",
            "model.to(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "def extract_relations_from_model_output(text):\n",
            "    relations = []\n",
            "    relation, subject, relation, object_ = \"\", \"\", \"\", \"\"\n",
            "    text = text.strip()\n",
            "    current = \"x\"\n",
            "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
            "    for token in text_replaced.split():\n",
            "        if token == \"<triplet>\":\n",
            "            current = \"t\"\n",
            "            if relation != \"\":\n",
            "                relations.append(\n",
            "                    {\n",
            "                        \"head\": subject.strip(),\n",
            "                        \"type\": relation.strip(),\n",
            "                        \"tail\": object_.strip(),\n",
            "                    }\n",
            "                )\n",
            "                relation = \"\"\n",
            "            subject = \"\"\n",
            "        elif token == \"<subj>\":\n",
            "            current = \"s\"\n",
            "            if relation != \"\":\n",
            "                relations.append(\n",
            "                    {\n",
            "                        \"head\": subject.strip(),\n",
            "                        \"type\": relation.strip(),\n",
            "                        \"tail\": object_.strip(),\n",
            "                    }\n",
            "                )\n",
            "            object_ = \"\"\n",
            "        elif token == \"<obj>\":\n",
            "            current = \"o\"\n",
            "            relation = \"\"\n",
            "        else:\n",
            "            if current == \"t\":\n",
            "                subject += \" \" + token\n",
            "            elif current == \"s\":\n",
            "                object_ += \" \" + token\n",
            "            elif current == \"o\":\n",
            "                relation += \" \" + token\n",
            "    if subject != \"\" and relation != \"\" and object_ != \"\":\n",
            "        relations.append(\n",
            "            {\"head\": subject.strip(), \"type\": relation.strip(), \"tail\": object_.strip()}\n",
            "        )\n",
            "    return relations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "class KB:\n",
            "    def __init__(self):\n",
            "        self.entities = {}\n",
            "        self.relations = set()\n",
            "\n",
            "    def get_wikipedia_data(self, candidate_entity):\n",
            "        try:\n",
            "            page = wikipedia.page(candidate_entity, auto_suggest=False)\n",
            "            entity_data = {\n",
            "                \"title\": page.title,\n",
            "                \"url\": page.url,\n",
            "                \"summary\": page.summary,\n",
            "            }\n",
            "            return entity_data\n",
            "        except:\n",
            "            return None\n",
            "\n",
            "    def print(self):\n",
            "        print(\"Entities:\")\n",
            "        for e in self.entities:\n",
            "            print(f\"  {e} -> {self.entities[e]}\")\n",
            "        print(\"Relations:\")\n",
            "        for r in self.relations:\n",
            "            print(f\"  {r}\")\n",
            "\n",
            "    def add_entity(self, e):\n",
            "        self.entities[e[\"title\"]] = {k: v for k, v in e.items() if k != \"title\"}\n",
            "\n",
            "    def add_relation(self, r):\n",
            "        # check on wikipedia\n",
            "        candidate_entities = [r[\"head\"], r[\"tail\"]]\n",
            "        entities = [self.get_wikipedia_data(ent) for ent in candidate_entities]\n",
            "\n",
            "        # if one entity does not exist, stop\n",
            "        if any(ent is None for ent in entities):\n",
            "            return\n",
            "\n",
            "        # manage new entities\n",
            "        for e in entities:\n",
            "            self.add_entity(e)\n",
            "\n",
            "        # rename relation entities with their wikipedia titles\n",
            "        r[\"head\"] = entities[0][\"title\"]\n",
            "        r[\"tail\"] = entities[1][\"title\"]\n",
            "\n",
            "        # manage new relation\n",
            "        triplet = (r[\"head\"], r[\"type\"], r[\"tail\"])\n",
            "        self.relations.add(triplet)\n",
            "\n",
            "        # print(f\"Added relation: {triplet}\")\n",
            "        # print(self.relations)\n",
            "        # raise Exception and exit\n",
            "        # raise Exception(\"here\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_small_text_to_kb(text, verbose=False, max_length=512):\n",
            "    \"\"\"This function takes a text and returns a KB. Note that the text should be small enough to fit in the model's input.\n",
            "    Specifically, the text should be less than 512 tokens, which corresponds to about 380 English words.\n",
            "\n",
            "    Args:\n",
            "        text (_type_): text used to generate the KB.\n",
            "        verbose (bool, optional): If True, prints the number of tokens in the text. Defaults to False.\n",
            "    Returns:\n",
            "        KB: Knowledge base generated from the text.\n",
            "    \"\"\"\n",
            "\n",
            "    kb = KB()\n",
            "\n",
            "    # Tokenizer text\n",
            "\n",
            "    model_inputs = tokenizer(\n",
            "        text, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\"\n",
            "    ).to(device)\n",
            "\n",
            "    if verbose:\n",
            "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
            "\n",
            "    # Generate\n",
            "\n",
            "    gen_kwargs = {\n",
            "        \"max_length\": 512,\n",
            "        \"length_penalty\": 0,\n",
            "        \"num_beams\": 3,\n",
            "        \"num_return_sequences\": 3,\n",
            "    }\n",
            "\n",
            "    generated_tokens = model.generate(\n",
            "        **model_inputs,\n",
            "        **gen_kwargs,\n",
            "    )\n",
            "\n",
            "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
            "\n",
            "    # create kb\n",
            "\n",
            "    for sentence_pred in decoded_preds:\n",
            "        relations = extract_relations_from_model_output(sentence_pred)\n",
            "\n",
            "        for r in relations:\n",
            "            kb.add_relation(r)\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_text_to_kb(text, span_length=128, verbose=False):\n",
            "    \"\"\"This function takes a text and returns a KB. There is no limit on the text length.\n",
            "    The text is split into spans of length span_length, and a KB is generated for each span.\n",
            "\n",
            "    Args:\n",
            "        text (_type_): text used to generate the KB.\n",
            "        span_length (int, optional): length of each span. Defaults to 128.\n",
            "        verbose (bool, optional): Defaults to False.\n",
            "\n",
            "    Returns:\n",
            "        KB: Knowledge base generated from the text.\n",
            "    \"\"\"\n",
            "\n",
            "    # tokenize whole text\n",
            "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
            "\n",
            "    # compute span boundaries\n",
            "    num_tokens = len(inputs[\"input_ids\"][0])\n",
            "    if verbose:\n",
            "        print(f\"Input has {num_tokens} tokens\")\n",
            "    num_spans = math.ceil(num_tokens / span_length)\n",
            "    if verbose:\n",
            "        print(f\"Input has {num_spans} spans\")\n",
            "    overlap = math.ceil((num_spans * span_length - num_tokens) / max(num_spans - 1, 1))\n",
            "    spans_boundaries = []\n",
            "    start = 0\n",
            "    for i in range(num_spans):\n",
            "        spans_boundaries.append(\n",
            "            [start + span_length * i, start + span_length * (i + 1)]\n",
            "        )\n",
            "        start -= overlap\n",
            "    if verbose:\n",
            "        print(f\"Span boundaries are {spans_boundaries}\")\n",
            "\n",
            "    # transform input with spans\n",
            "    tensor_ids = [\n",
            "        inputs[\"input_ids\"][0][boundary[0] : boundary[1]]\n",
            "        for boundary in spans_boundaries\n",
            "    ]\n",
            "    tensor_masks = [\n",
            "        inputs[\"attention_mask\"][0][boundary[0] : boundary[1]]\n",
            "        for boundary in spans_boundaries\n",
            "    ]\n",
            "    inputs = {\n",
            "        \"input_ids\": torch.stack(tensor_ids),\n",
            "        \"attention_mask\": torch.stack(tensor_masks),\n",
            "    }\n",
            "\n",
            "    # generate relations\n",
            "    num_return_sequences = 3\n",
            "    gen_kwargs = {\n",
            "        \"max_length\": 256,\n",
            "        \"length_penalty\": 0,\n",
            "        \"num_beams\": 3,\n",
            "        \"num_return_sequences\": num_return_sequences,\n",
            "    }\n",
            "    generated_tokens = model.generate(\n",
            "        **inputs,\n",
            "        **gen_kwargs,\n",
            "    )\n",
            "\n",
            "    # decode relations\n",
            "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
            "\n",
            "    # create kb\n",
            "    kb = KB()\n",
            "    i = 0\n",
            "    for sentence_pred in decoded_preds:\n",
            "        current_span_index = i // num_return_sequences\n",
            "        relations = extract_relations_from_model_output(sentence_pred)\n",
            "        for relation in relations:\n",
            "            relation[\"meta\"] = {\"spans\": [spans_boundaries[current_span_index]]}\n",
            "            kb.add_relation(relation)\n",
            "        i += 1\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_chapter_to_kb(chapter: str, verbose=False):\n",
            "    print(\"Generating KB for chapter\", chapter)\n",
            "\n",
            "    chapter = book.getChapter(chapter)\n",
            "    kb = KB()\n",
            "    for page in chapter:\n",
            "        text = page.extract_text()\n",
            "        kb_page = from_small_text_to_kb(text, verbose=verbose, max_length=1024)\n",
            "        kb.entities.update(kb_page.entities)\n",
            "        kb.relations.update(kb_page.relations)\n",
            "\n",
            "    with open(os.path.join(dataDir, f\"kb_entities_chapter_{chapter}.json\"), \"w\") as f:\n",
            "        json.dump(kb.entities, f, indent=4)\n",
            "    with open(os.path.join(dataDir, f\"kb_relations_chapter_{chapter}.json\"), \"w\") as f:\n",
            "        json.dump(list(kb.relations), f, indent=4)\n",
            "\n",
            "    print(f\"number of entities in chapter {chapter}: {len(kb.entities)}\")\n",
            "    print(f\"number of relations in chapter {chapter}: {len(kb.relations)}\")\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "from_chapter_to_kb(\"1\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Generating KB for chapter 1\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "c:\\Users\\HP\\.conda\\envs\\myenv\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
                  "\n",
                  "The code that caused this warning is on line 389 of the file c:\\Users\\HP\\.conda\\envs\\myenv\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
                  "\n",
                  "  lis = BeautifulSoup(html).find_all('li')\n"
               ]
            }
         ],
         "source": [
            "chapters = [str(i) for i in range(1, 21)]\n",
            "\n",
            "for chapter in chapters:\n",
            "    from_chapter_to_kb(chapter)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def read_chapter_by_sections(chapter, save=False):\n",
            "    toc = book.loadToc()\n",
            "\n",
            "    res = {}\n",
            "    for section in toc[chapter][\"sections\"]:\n",
            "        print(f\"Reading chapter {chapter}, section {section['section']}\")\n",
            "        section_text = [\n",
            "            page.extract_text().replace(\"\\n\", \" \")\n",
            "            for page in book.getSection(chapter, section[\"section\"])\n",
            "        ]\n",
            "        kb = from_small_text_to_kb(section_text, verbose=True)\n",
            "        res[\n",
            "            section[\"chapter\"] + \".\" + section[\"section\"] + \" \" + section[\"title\"]\n",
            "        ] = kb.relations\n",
            "\n",
            "    if save:\n",
            "        with open(os.path.join(dataDir, f\"chapter_{chapter}.json\"), \"w\") as f:\n",
            "            json.dump(res, f, indent=4)\n",
            "\n",
            "    return res"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}

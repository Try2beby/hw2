{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import json\n",
            "import importlib\n",
            "\n",
            "classBook = importlib.import_module(\"classBook\")\n",
            "importlib.reload(classBook)\n",
            "\n",
            "Book = classBook.Book\n",
            "\n",
            "dataDir = \"../data/\"\n",
            "dataName = \"Deep Learning.pdf\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Reading book:  Deep Learning.pdf from directory:  ../data/\n",
                  "Book loaded successfully\n",
                  "Number of pages:  800\n"
               ]
            }
         ],
         "source": [
            "book = Book()\n",
            "book.searchStrInPage(250, \"analyticalsolution\")\n",
            "text = book.readPage(250).extract_text()\n",
            "# write text to txt file\n",
            "with open(\"test.txt\", \"w\") as f:\n",
            "    f.write(text)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "import math\n",
            "\n",
            "import IPython\n",
            "import torch\n",
            "import wikipedia\n",
            "from pyvis.network import Network\n",
            "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "BartForConditionalGeneration(\n",
                     "  (model): BartModel(\n",
                     "    (shared): Embedding(50272, 1024, padding_idx=1)\n",
                     "    (encoder): BartEncoder(\n",
                     "      (embed_tokens): Embedding(50272, 1024, padding_idx=1)\n",
                     "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                     "      (layers): ModuleList(\n",
                     "        (0-11): 12 x BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "    )\n",
                     "    (decoder): BartDecoder(\n",
                     "      (embed_tokens): Embedding(50272, 1024, padding_idx=1)\n",
                     "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                     "      (layers): ModuleList(\n",
                     "        (0-11): 12 x BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "    )\n",
                     "  )\n",
                     "  (lm_head): Linear(in_features=1024, out_features=50272, bias=False)\n",
                     ")"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Load model and tokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
            "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
            "\n",
            "# Specify the device\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# Move the model to the device\n",
            "model.to(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "def extract_relations_from_model_output(text):\n",
            "    relations = []\n",
            "    relation, subject, relation, object_ = \"\", \"\", \"\", \"\"\n",
            "    text = text.strip()\n",
            "    current = \"x\"\n",
            "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
            "    for token in text_replaced.split():\n",
            "        if token == \"<triplet>\":\n",
            "            current = \"t\"\n",
            "            if relation != \"\":\n",
            "                relations.append(\n",
            "                    {\n",
            "                        \"head\": subject.strip(),\n",
            "                        \"type\": relation.strip(),\n",
            "                        \"tail\": object_.strip(),\n",
            "                    }\n",
            "                )\n",
            "                relation = \"\"\n",
            "            subject = \"\"\n",
            "        elif token == \"<subj>\":\n",
            "            current = \"s\"\n",
            "            if relation != \"\":\n",
            "                relations.append(\n",
            "                    {\n",
            "                        \"head\": subject.strip(),\n",
            "                        \"type\": relation.strip(),\n",
            "                        \"tail\": object_.strip(),\n",
            "                    }\n",
            "                )\n",
            "            object_ = \"\"\n",
            "        elif token == \"<obj>\":\n",
            "            current = \"o\"\n",
            "            relation = \"\"\n",
            "        else:\n",
            "            if current == \"t\":\n",
            "                subject += \" \" + token\n",
            "            elif current == \"s\":\n",
            "                object_ += \" \" + token\n",
            "            elif current == \"o\":\n",
            "                relation += \" \" + token\n",
            "    if subject != \"\" and relation != \"\" and object_ != \"\":\n",
            "        relations.append(\n",
            "            {\"head\": subject.strip(), \"type\": relation.strip(), \"tail\": object_.strip()}\n",
            "        )\n",
            "    return relations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "class KB:\n",
            "    def __init__(self):\n",
            "        self.entities = {}\n",
            "        self.relations = set()\n",
            "        self.raw_relations = set()\n",
            "\n",
            "    def get_wikipedia_data(self, candidate_entity):\n",
            "        try:\n",
            "            page = wikipedia.page(candidate_entity, auto_suggest=False)\n",
            "            entity_data = {\n",
            "                \"title\": page.title,\n",
            "                \"url\": page.url,\n",
            "                \"summary\": page.summary,\n",
            "            }\n",
            "            return entity_data\n",
            "        except:\n",
            "            return None\n",
            "\n",
            "    def print(self):\n",
            "        print(\"Entities:\")\n",
            "        for e in self.entities:\n",
            "            print(f\"  {e} -> {self.entities[e]}\")\n",
            "        print(\"Relations:\")\n",
            "        for r in self.relations:\n",
            "            print(f\"  {r}\")\n",
            "\n",
            "    def add_entity(self, e):\n",
            "        self.entities[e[\"title\"]] = {k: v for k, v in e.items() if k != \"title\"}\n",
            "\n",
            "    def add_relation(self, r):\n",
            "        triplet = (r[\"head\"], r[\"type\"], r[\"tail\"])\n",
            "        self.raw_relations.add(triplet)\n",
            "        # check on wikipedia\n",
            "        candidate_entities = [r[\"head\"], r[\"tail\"]]\n",
            "        entities = [self.get_wikipedia_data(ent) for ent in candidate_entities]\n",
            "\n",
            "        # if one entity does not exist, stop\n",
            "        if any(ent is None for ent in entities):\n",
            "            return\n",
            "\n",
            "        # manage new entities\n",
            "        for e in entities:\n",
            "            self.add_entity(e)\n",
            "\n",
            "        # rename relation entities with their wikipedia titles\n",
            "        r[\"head\"] = entities[0][\"title\"]\n",
            "        r[\"tail\"] = entities[1][\"title\"]\n",
            "\n",
            "        # manage new relation\n",
            "        triplet = (r[\"head\"], r[\"type\"], r[\"tail\"])\n",
            "        self.relations.add(triplet)\n",
            "\n",
            "        # print(f\"Added relation: {triplet}\")\n",
            "        # print(self.relations)\n",
            "        # raise Exception and exit\n",
            "        # raise Exception(\"here\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_small_text_to_kb(text, verbose=False, max_length=512):\n",
            "    \"\"\"This function takes a text and returns a KB. Note that the text should be small enough to fit in the model's input.\n",
            "    Specifically, the text should be less than 512 tokens, which corresponds to about 380 English words.\n",
            "\n",
            "    Args:\n",
            "        text (_type_): text used to generate the KB.\n",
            "        verbose (bool, optional): If True, prints the number of tokens in the text. Defaults to False.\n",
            "    Returns:\n",
            "        KB: Knowledge base generated from the text.\n",
            "    \"\"\"\n",
            "\n",
            "    kb = KB()\n",
            "\n",
            "    # Tokenizer text\n",
            "\n",
            "    model_inputs = tokenizer(\n",
            "        text, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\"\n",
            "    ).to(device)\n",
            "\n",
            "    if verbose:\n",
            "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
            "\n",
            "    # Generate\n",
            "\n",
            "    gen_kwargs = {\n",
            "        \"max_length\": 512,\n",
            "        \"length_penalty\": 0,\n",
            "        \"num_beams\": 3,\n",
            "        \"num_return_sequences\": 3,\n",
            "    }\n",
            "\n",
            "    generated_tokens = model.generate(\n",
            "        **model_inputs,\n",
            "        **gen_kwargs,\n",
            "    )\n",
            "\n",
            "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
            "\n",
            "    # create kb\n",
            "\n",
            "    for sentence_pred in decoded_preds:\n",
            "        relations = extract_relations_from_model_output(sentence_pred)\n",
            "\n",
            "        for r in relations:\n",
            "            kb.add_relation(r)\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_text_to_kb(text, span_length=128, verbose=False):\n",
            "    \"\"\"This function takes a text and returns a KB. There is no limit on the text length.\n",
            "    The text is split into spans of length span_length, and a KB is generated for each span.\n",
            "\n",
            "    Args:\n",
            "        text (_type_): text used to generate the KB.\n",
            "        span_length (int, optional): length of each span. Defaults to 128.\n",
            "        verbose (bool, optional): Defaults to False.\n",
            "\n",
            "    Returns:\n",
            "        KB: Knowledge base generated from the text.\n",
            "    \"\"\"\n",
            "\n",
            "    # tokenize whole text\n",
            "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
            "\n",
            "    # compute span boundaries\n",
            "    num_tokens = len(inputs[\"input_ids\"][0])\n",
            "    if verbose:\n",
            "        print(f\"Input has {num_tokens} tokens\")\n",
            "    num_spans = math.ceil(num_tokens / span_length)\n",
            "    if verbose:\n",
            "        print(f\"Input has {num_spans} spans\")\n",
            "    overlap = math.ceil((num_spans * span_length - num_tokens) / max(num_spans - 1, 1))\n",
            "    spans_boundaries = []\n",
            "    start = 0\n",
            "    for i in range(num_spans):\n",
            "        spans_boundaries.append(\n",
            "            [start + span_length * i, start + span_length * (i + 1)]\n",
            "        )\n",
            "        start -= overlap\n",
            "    if verbose:\n",
            "        print(f\"Span boundaries are {spans_boundaries}\")\n",
            "\n",
            "    # transform input with spans\n",
            "    tensor_ids = [\n",
            "        inputs[\"input_ids\"][0][boundary[0] : boundary[1]]\n",
            "        for boundary in spans_boundaries\n",
            "    ]\n",
            "    tensor_masks = [\n",
            "        inputs[\"attention_mask\"][0][boundary[0] : boundary[1]]\n",
            "        for boundary in spans_boundaries\n",
            "    ]\n",
            "    inputs = {\n",
            "        \"input_ids\": torch.stack(tensor_ids),\n",
            "        \"attention_mask\": torch.stack(tensor_masks),\n",
            "    }\n",
            "\n",
            "    # generate relations\n",
            "    num_return_sequences = 3\n",
            "    gen_kwargs = {\n",
            "        \"max_length\": 256,\n",
            "        \"length_penalty\": 0,\n",
            "        \"num_beams\": 3,\n",
            "        \"num_return_sequences\": num_return_sequences,\n",
            "    }\n",
            "    generated_tokens = model.generate(\n",
            "        **inputs,\n",
            "        **gen_kwargs,\n",
            "    )\n",
            "\n",
            "    # decode relations\n",
            "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
            "\n",
            "    # create kb\n",
            "    kb = KB()\n",
            "    i = 0\n",
            "    for sentence_pred in decoded_preds:\n",
            "        current_span_index = i // num_return_sequences\n",
            "        relations = extract_relations_from_model_output(sentence_pred)\n",
            "        for relation in relations:\n",
            "            relation[\"meta\"] = {\"spans\": [spans_boundaries[current_span_index]]}\n",
            "            kb.add_relation(relation)\n",
            "        i += 1\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "entityDir = os.path.join(dataDir, \"entities\")\n",
            "if not os.path.exists(entityDir):\n",
            "    os.makedirs(entityDir)\n",
            "relationDir = os.path.join(dataDir, \"relations\")\n",
            "if not os.path.exists(relationDir):\n",
            "    os.makedirs(relationDir)\n",
            "rawRelationDir = os.path.join(dataDir, \"raw_relations\")\n",
            "if not os.path.exists(rawRelationDir):\n",
            "    os.makedirs(rawRelationDir)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "def from_chapter_to_kb(chapter_str: str, verbose=False):\n",
            "    print(\"Generating KB for chapter\", chapter_str)\n",
            "\n",
            "    chapter = book.getChapter(chapter_str)\n",
            "\n",
            "    kb = KB()\n",
            "    for page in chapter:\n",
            "        text = page.extract_text()\n",
            "\n",
            "        kb_page = from_small_text_to_kb(text, verbose=verbose, max_length=1024)\n",
            "        kb.entities.update(kb_page.entities)\n",
            "        kb.relations.update(kb_page.relations)\n",
            "        kb.raw_relations.update(kb_page.raw_relations)\n",
            "\n",
            "    with open(os.path.join(entityDir, f\"kb_chapter_{chapter_str}.json\"), \"w\") as f:\n",
            "        json.dump(kb.entities, f, indent=4)\n",
            "    with open(os.path.join(relationDir, f\"kb_chapter_{chapter_str}.json\"), \"w\") as f:\n",
            "        json.dump(list(kb.relations), f, indent=4)\n",
            "    with open(os.path.join(rawRelationDir, f\"kb_chapter_{chapter_str}.json\"), \"w\") as f:\n",
            "        json.dump(list(kb.raw_relations), f, indent=4)\n",
            "\n",
            "    print(f\"number of entities in chapter {chapter}: {len(kb.entities)}\")\n",
            "    print(f\"number of relations in chapter {chapter}: {len(kb.relations)}\")\n",
            "    print(f\"number of raw relations in chapter {chapter}: {len(kb.raw_relations)}\")\n",
            "\n",
            "    return kb"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Generating KB for chapter 1\n",
                  "Num tokens: 398\n",
                  "Num tokens: 626\n",
                  "Num tokens: 622\n",
                  "Num tokens: 564\n"
               ]
            }
         ],
         "source": [
            "chapters = [str(i) for i in range(1, 21)]\n",
            "\n",
            "for chapter in chapters:\n",
            "    from_chapter_to_kb(chapter, 1)\n",
            "    # pass"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "def read_chapter_by_sections(chapter, save=False):\n",
            "    toc = book.loadToc()\n",
            "\n",
            "    res = {}\n",
            "    for section in toc[chapter][\"sections\"]:\n",
            "        print(f\"Reading chapter {chapter}, section {section['section']}\")\n",
            "        section_text = [\n",
            "            page.extract_text().replace(\"\\n\", \" \")\n",
            "            for page in book.getSection(chapter, section[\"section\"])\n",
            "        ]\n",
            "        kb = from_small_text_to_kb(section_text, verbose=True)\n",
            "        res[\n",
            "            section[\"chapter\"] + \".\" + section[\"section\"] + \" \" + section[\"title\"]\n",
            "        ] = kb.relations\n",
            "\n",
            "    if save:\n",
            "        with open(os.path.join(dataDir, f\"chapter_{chapter}.json\"), \"w\") as f:\n",
            "            json.dump(res, f, indent=4)\n",
            "\n",
            "    return res"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}

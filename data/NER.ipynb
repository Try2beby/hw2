{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c427152e-fb64-4fd2-a1d5-e14f848019aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:57:48.815737Z",
     "iopub.status.busy": "2024-02-21T07:57:48.815737Z",
     "iopub.status.idle": "2024-02-21T07:57:48.819684Z",
     "shell.execute_reply": "2024-02-21T07:57:48.819684Z",
     "shell.execute_reply.started": "2024-02-21T07:57:48.815737Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import math\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import ahocorasick\n",
    "import networkx as nx\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import torch\n",
    "import transformers\n",
    "from fuzzywuzzy import fuzz\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dataDir = \"../data/\"\n",
    "dataName = \"Deep Learning.pdf\"\n",
    "import codecs\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# openai.api_base = \"https://api.chatanywhere.com.cn/\"\n",
    "openai.api_base = \"https://api.chatanywhere.tech\"\n",
    "openai.api_key = \"sk-LzwgVgu5xvNPpwoqCdeeVcAt7Tu7ZoZICXzzkheldIbXA60h\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f4fef-8b2d-41ae-8af6-552e79070234",
   "metadata": {},
   "source": [
    "# 1. 获取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9e3a41-3514-4a05-ba63-0459ef06b1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:33:37.177648Z",
     "iopub.status.busy": "2024-02-21T07:33:37.176648Z",
     "iopub.status.idle": "2024-02-21T07:35:17.816953Z",
     "shell.execute_reply": "2024-02-21T07:35:17.816953Z",
     "shell.execute_reply.started": "2024-02-21T07:33:37.176648Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased', cache_dir=\"../../../BERT/large\")\n",
    "model = BertModel.from_pretrained(\"bert-large-cased\", cache_dir=\"../../../BERT/large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364fd41-4285-4f7d-bdee-b743a822c9de",
   "metadata": {},
   "source": [
    "# 2. 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8647860d-012f-40e4-8eba-7b61399be761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:24:34.772336Z",
     "iopub.status.busy": "2024-02-21T09:24:34.772336Z",
     "iopub.status.idle": "2024-02-21T09:24:34.774880Z",
     "shell.execute_reply": "2024-02-21T09:24:34.774880Z",
     "shell.execute_reply.started": "2024-02-21T09:24:34.772336Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6304d6-f72e-4c38-b3a2-5548ca3a6459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:06:59.601657Z",
     "iopub.status.busy": "2024-02-21T07:06:59.601657Z",
     "iopub.status.idle": "2024-02-21T07:08:29.596720Z",
     "shell.execute_reply": "2024-02-21T07:08:29.596720Z",
     "shell.execute_reply.started": "2024-02-21T07:06:59.601657Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5367942f5d46458aa0a5dfd3eed71f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pdfplumber.open(dataDir + dataName) as f:\n",
    "    # 目录架构生成\n",
    "    c, p, n = [], [], []\n",
    "    for i in range(7):\n",
    "        page = f.pages[i]\n",
    "        text = page.extract_text()\n",
    "        text_split = text.split(\"\\n\")\n",
    "        for i in text_split:\n",
    "            if bool(re.match(\"[0-9]+\\.[0-9]+\", i.split(\" \")[0])):\n",
    "                c.append(i.split(\" \")[0])\n",
    "                p.append(int(i.split(\" \")[-1]) + 15)\n",
    "            if bool(re.match(\"[0-9]+\", i.split(\" \")[0])):\n",
    "                for j in i.split(\" \"):\n",
    "                    if bool(re.match(\"[A-Za-z]+\", j)):\n",
    "                        n.append((i.split(\" \")[0], j))\n",
    "                        \n",
    "p_range = list(zip(p, p[1:]))\n",
    "p_range.append((735, 800))\n",
    "c_p_range = list(zip(c, p_range))\n",
    "index_dict = collections.defaultdict(list)\n",
    "for k, v in c_p_range:\n",
    "    index_dict[k.split(\".\")[0]].append((k, v))\n",
    "\n",
    "with pdfplumber.open(dataDir + dataName) as f:\n",
    "    content_dict = collections.defaultdict(list)\n",
    "\n",
    "    for k, v in tqdm(index_dict.items(), total=len(index_dict)):\n",
    "        for i in v:\n",
    "            page_range = i[-1]\n",
    "            if page_range[0] == page_range[1]:\n",
    "                page_range = (page_range[0], page_range[1] + 1)\n",
    "            for j in range(int(page_range[0]) - 1, int(page_range[1]) - 1):\n",
    "                page = f.pages[j]\n",
    "\n",
    "                text = page.extract_text().replace(\"\\n\", \" \")\n",
    "\n",
    "                content_dict[i[0]].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f8717b-9aa9-42ad-9c14-290092ff25b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:33:34.102084Z",
     "iopub.status.busy": "2024-02-21T09:33:34.102084Z",
     "iopub.status.idle": "2024-02-21T09:33:34.105930Z",
     "shell.execute_reply": "2024-02-21T09:33:34.105930Z",
     "shell.execute_reply.started": "2024-02-21T09:33:34.102084Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, conversation_list=[]):\n",
    "        self.conversation_list = conversation_list\n",
    "        self.costs_list = []\n",
    "\n",
    "    def show_conversation(self, msg_list):\n",
    "        for msg in msg_list[-2:]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                pass\n",
    "            else:\n",
    "                message = msg[\"content\"]\n",
    "                pass\n",
    "                # print(f\"\\U0001f47D: {message}\\n\")\n",
    "\n",
    "    def ask(self, prompt):\n",
    "        self.conversation_list.append({\"role\": \"user\", \"content\": prompt})\n",
    "        openai.api_key = \"sk-LzwgVgu5xvNPpwoqCdeeVcAt7Tu7ZoZICXzzkheldIbXA60h\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-16k\", messages=self.conversation_list\n",
    "        )\n",
    "        answer = response.choices[0].message[\"content\"]\n",
    "\n",
    "        self.conversation_list.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        self.show_conversation(self.conversation_list)\n",
    "\n",
    "        # cost = total_counts(response)\n",
    "        # self.costs_list.append(cost)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3640d992-d4e0-4de4-8209-7ecabd55e5da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:36:08.229147Z",
     "iopub.status.busy": "2024-02-21T09:36:08.228147Z",
     "iopub.status.idle": "2024-02-21T09:36:08.231752Z",
     "shell.execute_reply": "2024-02-21T09:36:08.231752Z",
     "shell.execute_reply.started": "2024-02-21T09:36:08.229147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NER_prompt = f\"\"\"\n",
    "角色：\n",
    "你是一个深度学习领域的实体标注专员\n",
    "\n",
    "任务：\n",
    "给定字符串，请找出全部深度学习领域的实体\n",
    "\n",
    "步骤：\n",
    "请以以下步骤执行：\n",
    "1. 找出句子中的所有深度学习领域的实体\n",
    "2. 依次检查实体是否属于深度学习领域\n",
    "3. 将属于深度学习领域的实体返回\n",
    "4. 若没有深度学习领域的实体，则返回()\n",
    "\n",
    "格式：\n",
    "请以以下格式返回：\n",
    "('entity1', 'entity2', ...)\n",
    "\n",
    "举例如下：\n",
    "input: An illustration of how the gradient descent algorithm uses the derivatives of a function can be used to follow the function downhill to a minimum.\n",
    "output: (gradient descent algorithm)\n",
    "\n",
    "input: an encoder or reader or input RNN processes the input sequence. The encoder emits the context C, usually as a simple function of its final hidden state.\n",
    "output: (encoder, RNN, hidden state)\n",
    "\n",
    "input: There is no constraint that the encoder must have the same size of hidden layer as the decoder\n",
    "output: (hidden layer, decoder)\n",
    "\n",
    "input: Computer vision has traditionally been one of the most active research areas for deep learning applications, because vision is a task that is effortless for humans and many animals but challenging for computers (Ballard et al., 1983)\n",
    "output: (Computer vision, deep learning)\n",
    "\n",
    "input: Dataset augmentation may be seen as a way of preprocessing the training set only.\n",
    "output: (Dataset augmentation)\n",
    "\n",
    "input: CHAPTER 1. INTRODUCTION of the flowchart of the computations needed to compute the representation of each concept may be much deeper than the graph of the concepts themselves.\n",
    "output: ()\n",
    "\n",
    "注意事项：\n",
    "1. 请严格遵循字符串中的原本表述\n",
    "2. 除返回结果外，不要返回任何其他内容\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6277d2bc-5c2b-4aa1-a604-fb4b825ae6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:48:04.087938Z",
     "iopub.status.busy": "2024-02-21T09:48:04.087938Z",
     "iopub.status.idle": "2024-02-21T09:48:04.090724Z",
     "shell.execute_reply": "2024-02-21T09:48:04.090724Z",
     "shell.execute_reply.started": "2024-02-21T09:48:04.087938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_prompt = f\"\"\"\n",
    "任务：\n",
    "请检查所给实体是否属于深度学习领域\n",
    "\n",
    "格式：\n",
    "请以以下格式返回：\n",
    "如果该实体是深度学习领域的实体，返回True，否则返回False\n",
    "\n",
    "举例如下：\n",
    "input: deep learning\n",
    "output: True\n",
    "\n",
    "input: AI system\n",
    "output: True\n",
    "\n",
    "input: image\n",
    "output: False\n",
    "\n",
    "input: Image Net\n",
    "output: True\n",
    "\n",
    "input: face\n",
    "output: False\n",
    "\n",
    "注意事项：\n",
    "1. 请仅返回True或者False，不要返回任何其他内容\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99d20474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_position(content_token, entity_token):\n",
    "    \"\"\"\n",
    "    Finds the position of an entity token within a content token.\n",
    "\n",
    "    Args:\n",
    "        content_token (torch.Tensor): A tensor representing the content token.\n",
    "        entity_token (torch.Tensor): A tensor representing the entity token.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the position of the entity token within the content token.\n",
    "                      Each element in the tensor represents the position of a token in the content token:\n",
    "                      - 0: Token does not match the entity token.\n",
    "                      - 1: Token matches the entity token, but is not the first token.\n",
    "                      - 2: Token matches the entity token and is the first token.\n",
    "    \"\"\"\n",
    "    position = torch.zeros_like(content_token)\n",
    "    for entity in entity_token:\n",
    "        for i in range(len(content_token) - len(entity) + 1):\n",
    "            if torch.all(content_token[i:i + len(entity)] == entity):\n",
    "                position[i] = 2\n",
    "                position[i + 1:i + len(entity)] = 1\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "caabba2f-9476-43b1-8e7f-61c96d38080d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:45:57.615752Z",
     "iopub.status.busy": "2024-02-21T10:45:57.615752Z",
     "iopub.status.idle": "2024-02-21T10:49:32.176168Z",
     "shell.execute_reply": "2024-02-21T10:49:32.176168Z",
     "shell.execute_reply.started": "2024-02-21T10:45:57.615752Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_content(content_dict):\n",
    "    \"\"\"\n",
    "    Process the content dictionary to extract named entities and save them to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        content_dict (dict): A dictionary containing the content to process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for k, v in content_dict.items():\n",
    "        if k in ['4.2', '10.1', '15.1', '20.1']:\n",
    "            if total >= 1500:\n",
    "                break\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "            docs = text_splitter.split_text(' '.join(i for i in v))\n",
    "            \n",
    "            # Process each document\n",
    "            for index, content in enumerate(docs):\n",
    "                total += 1\n",
    "                if total == 1500:\n",
    "                    break\n",
    "                \n",
    "                # Initialize NER chatbot\n",
    "                if index % 5 == 0:\n",
    "                    conversation_list = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": NER_prompt,\n",
    "                        }\n",
    "                    ]\n",
    "                    bot_ner = Chat(conversation_list)\n",
    "                \n",
    "                # Extract named entities using NER chatbot\n",
    "                answer_ner = bot_ner.ask(\"input: \"+content)\n",
    "                entity_list_temp = re.sub(\"\\(|\\)|\", \"\", answer_ner).split(\", \")\n",
    "                \n",
    "                # Initialize check chatbot\n",
    "                conversation_list = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": check_prompt,\n",
    "                    }\n",
    "                ]\n",
    "                bot_check = Chat(conversation_list)\n",
    "                \n",
    "                entity_list = []\n",
    "                # Check if each entity is valid using check chatbot\n",
    "                for e in entity_list_temp:\n",
    "                    answer_check = bot_check.ask(\"input: \"+e)\n",
    "                    if answer_check == 'True':\n",
    "                        entity_list.append(e)\n",
    "                \n",
    "                # Tokenize content and entities\n",
    "                content_token = tokenizer(content, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "                entity_token = []\n",
    "                for e in entity_list:\n",
    "                    entity_token.append(tokenizer(e, return_tensors='pt')['input_ids'].squeeze(0)[1:-1])\n",
    "                \n",
    "                # Find position of entities in content\n",
    "                label = find_position(content_token, entity_token)\n",
    "                \n",
    "                # Save the results to a CSV file\n",
    "                df = pd.DataFrame(\n",
    "                    [[tokenizer.batch_decode(content_token), tokenizer.batch_decode(entity_token), label]],\n",
    "                    columns=[\"text\", \"entity\", \"label\"],\n",
    "                )\n",
    "                df.to_csv(\n",
    "                    os.path.join(dataDir + \"/relations\", f\"sample.csv\"),\n",
    "                    mode=\"a\",\n",
    "                    header=not os.path.exists(\n",
    "                        os.path.join(dataDir + \"/relations\", f\"sample.csv\")\n",
    "                    ),\n",
    "                    index=False,\n",
    "                )\n",
    "                # Print the content, tokens, entity list, and label\n",
    "                print(\"content: \"+ str(content), \"content_token: \"+str(content_token), \"entity_list: \"+str(entity_list), \"entity_token: \"+str(entity_token), sep=\"\\n\")\n",
    "                print(\"label: \"+str(label))\n",
    "                print('----------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87cb9aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: CHAPTER 4. NUMERICAL COMPUTATION stabilized. Theano (Bergstra et al., 2010; Bastien et al., 2012) is\n",
      "content_token: tensor([  101,  8203,   125,   119,   151, 25810,  9637,  9741, 12507, 18732,\n",
      "        12347, 16830, 13821, 24805, 19428, 15892,   119,  1109,  7428,   113,\n",
      "        16218, 16468,  3084,  2393,   119,   117,  1333,   132, 18757,  2050,\n",
      "         8584,  3084,  2393,   119,   117,  1368,   114,  1110,   102])\n",
      "entity_list: ['Theano']\n",
      "entity_token: [tensor([1109, 7428])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: et al., 2012) is an example of a software package that automatically detects and stabilizes many\n",
      "content_token: tensor([  101,  3084,  2393,   119,   117,  1368,   114,  1110,  1126,  1859,\n",
      "         1104,   170,  3594,  7305,  1115,  7743, 11552,  1116,  1105, 19428,\n",
      "        21225,  1116,  1242,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and stabilizes many common numerically unstable expressions that arise in the context of deep\n",
      "content_token: tensor([  101,  1105, 19428, 21225,  1116,  1242,  1887, 18294,  1193, 15443,\n",
      "        11792,  1115, 14368,  1107,  1103,  5618,  1104,  1996,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the context of deep learning. 4.2 Poor Conditioning Conditioning refers to how rapidly a function\n",
      "content_token: tensor([  101,  1103,  5618,  1104,  1996,  3776,   119,   125,   119,   123,\n",
      "        11767, 16752, 14669,  1158, 16752, 14669,  1158,  4431,  1106,  1293,\n",
      "         5223,   170,  3053,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: rapidly a function changes with respect to small changes in its inputs. Functions that\n",
      "content_token: tensor([  101,  5223,   170,  3053,  2607,  1114,  4161,  1106,  1353,  2607,\n",
      "         1107,  1157, 22743,   119, 16068, 13945,  1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Functions that changerapidly when their inputs are perturbed slightly can be problematic for\n",
      "content_token: tensor([  101, 16068, 13945,  1115,  1849, 14543,  2386,  1193,  1165,  1147,\n",
      "        22743,  1132,  1679, 20362,  4774,  2776,  1169,  1129, 20405,  1111,\n",
      "          102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: be problematic for scientific computation because rounding errors in the inputs can result in large\n",
      "content_token: tensor([  101,  1129, 20405,  1111,  3812,  3254, 19675,  1272,  1668,  1158,\n",
      "        11122,  1107,  1103, 22743,  1169,  1871,  1107,  1415,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can result in large changes in the output. Consider the function f(x) = A 1x. When A Rn n has an\n",
      "content_token: tensor([  101,  1169,  1871,  1107,  1415,  2607,  1107,  1103,  5964,   119,\n",
      "        25515,  1103,  3053,   175,   113,   193,   114,   134,   138,   122,\n",
      "         1775,   119,  1332,   138,   155,  1179,   183,  1144,  1126,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: When A Rn n has an eigenvalue − × ∈ decomposition, its condition number is λ i max . (4.2) i,j λ j\n",
      "content_token: tensor([  101,  1332,   138,   155,  1179,   183,  1144,  1126,   174, 13417,\n",
      "         1179,  7501,  4175,   851,   240,   850, 25898,   117,  1157,  3879,\n",
      "         1295,  1110,   428,   178, 12477,  1775,   119,   113,   125,   119,\n",
      "          123,   114,   178,   117,   179,   428,   179,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: max . (4.2) i,j λ j       This is the ratio of the magnitude of the largest and smallest\n",
      "content_token: tensor([  101, 12477,  1775,   119,   113,   125,   119,   123,   114,   178,\n",
      "          117,   179,   428,   179,  1188,  1110,  1103,  6022,  1104,  1103,\n",
      "        10094,  1104,  1103,  2026,  1105, 10471,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and smallest eigenvalue. When   this number is large, matrix inversion is particularly sensitive\n",
      "content_token: tensor([  101,  1105, 10471,   174, 13417,  1179,  7501,  4175,   119,  1332,\n",
      "         1142,  1295,  1110,  1415,   117,  8952,  1107, 12475,  1110,  2521,\n",
      "         7246,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sensitive to error in the input. This sensitivity is an intrinsic property of the matrix itself,\n",
      "content_token: tensor([  101,  7246,  1106,  7353,  1107,  1103,  7758,   119,  1188, 15750,\n",
      "         1110,  1126, 27799,  2400,  1104,  1103,  8952,  2111,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the matrix itself, not the result of rounding error during matrix inversion. Poorly conditioned\n",
      "content_token: tensor([  101,  1103,  8952,  2111,   117,  1136,  1103,  1871,  1104,  1668,\n",
      "         1158,  7353,  1219,  8952,  1107, 12475,   119, 11767,  1193, 25592,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Poorly conditioned matrices amplify pre-existing errors when we multiply by the true matrix\n",
      "content_token: tensor([  101, 11767,  1193, 25592, 24350,  1821,  1643, 22881,  3073,   118,\n",
      "         3685, 11122,  1165,  1195,  4321,  1643,  1193,  1118,  1103,  2276,\n",
      "         8952,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by the true matrix inverse. In practice, the error will be compounded further by numerical errors\n",
      "content_token: tensor([  101,  1118,  1103,  2276,  8952, 22127,   119,  1130,  2415,   117,\n",
      "         1103,  7353,  1209,  1129,  7090,  1174,  1748,  1118, 18294, 11122,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by numerical errors in the inversion process itself. 4.3 Gradient-Based Optimization Most deep\n",
      "content_token: tensor([  101,  1118, 18294, 11122,  1107,  1103,  1107, 12475,  1965,  2111,\n",
      "          119,   125,   119,   124,   144,  9871,  9080,   118,  7457,  9126,\n",
      "         3121,  3080,  8569,  2082,  1996,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of\n",
      "content_token: tensor([  101,  2082,  1996,  3776, 14975,  8803, 25161,  1104,  1199,  3271,\n",
      "          119,  9126,  3121,  3080,  8569,  4431,  1106,  1103,  4579,  1104,\n",
      "          102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to the task of either minimizing or maximizing some function f(x) by altering x. We usually phrase\n",
      "content_token: tensor([  101,  1106,  1103,  4579,  1104,  1719,  8715, 25596,  1137, 12477,\n",
      "         8745, 25596,  1199,  3053,   175,   113,   193,   114,  1118, 25595,\n",
      "          193,   119,  1284,  1932,  7224,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: We usually phrase most optimization problems in terms of minimizing f(x). Maximization may be\n",
      "content_token: tensor([  101,  1284,  1932,  7224,  1211, 25161,  2645,  1107,  2538,  1104,\n",
      "         8715, 25596,   175,   113,   193,   114,   119, 25217,  2734,  1336,\n",
      "         1129,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Maximization may be accomplished via a minimization algorithm by minimizing f(x). − The function we\n",
      "content_token: tensor([  101, 25217,  2734,  1336,  1129,  8587,  2258,   170,  8715,  3080,\n",
      "         8569,  9932,  1118,  8715, 25596,   175,   113,   193,   114,   119,\n",
      "          851,  1109,  3053,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: − The function we want to minimize or maximize is called the objective func- tion or criterion.\n",
      "content_token: tensor([  101,   851,  1109,  3053,  1195,  1328,  1106, 20220,  1137, 12477,\n",
      "         8745, 19092,  1110,  1270,  1103,  7649,  4106,  1665,   118,   189,\n",
      "         1988,  1137, 26440,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: tion or criterion. When we are minimizing it, we may also call it the cost function, loss function,\n",
      "content_token: tensor([  101,   189,  1988,  1137, 26440,   119,  1332,  1195,  1132,  8715,\n",
      "        25596,  1122,   117,  1195,  1336,  1145,  1840,  1122,  1103,  2616,\n",
      "         3053,   117,  2445,  3053,   117,   102])\n",
      "entity_list: ['objective function', 'loss function']\n",
      "entity_token: [tensor([7649, 3053]), tensor([2445, 3053])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: loss function, or error function. In this book, we use these terms interchangeably, though some\n",
      "content_token: tensor([ 101, 2445, 3053,  117, 1137, 7353, 3053,  119, 1130, 1142, 1520,  117,\n",
      "        1195, 1329, 1292, 2538, 9629, 5382,  117, 1463, 1199,  102])\n",
      "entity_list: ['loss function']\n",
      "entity_token: [tensor([2445, 3053])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: though some machine learning publications assign special meaning to some of these terms. We often\n",
      "content_token: tensor([  101,  1463,  1199,  3395,  3776,  5873, 27430,  1957,  2764,  1106,\n",
      "         1199,  1104,  1292,  2538,   119,  1284,  1510,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: terms. We often denote the value that minimizes or maximizes a function with a superscript . For\n",
      "content_token: tensor([  101,  2538,   119,  1284,  1510, 21185,  1103,  2860,  1115, 20220,\n",
      "         1116,  1137, 12477,  8745, 19092,  1116,   170,  3053,  1114,   170,\n",
      "         7688,  1116, 13590,   119,  1370,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a superscript . For example, we might say x = argminf(x). ∗ ∗ 82\n",
      "content_token: tensor([  101,   170,  7688,  1116, 13590,   119,  1370,  1859,   117,  1195,\n",
      "         1547,  1474,   193,   134,   170, 10805,  7937,  2087,   113,   193,\n",
      "          114,   119,   852,   852,  5787,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS 10.1 Unfolding Computational Graphs A\n",
      "content_token: tensor([  101,  8203,  1275,   119, 12342,  4880, 24846, 15517,  2036,   150,\n",
      "        15609, 21678, 15740,   131,   155,  8231, 19556, 16941, 15681, 16716,\n",
      "          155,  8231, 19556, 13882, 17145, 26546, 11365,  1275,   119,   122,\n",
      "        12118, 10787,  1158,  3291,  8223, 15012, 15937,   144, 14543,  9524,\n",
      "          138,   102])\n",
      "entity_list: ['SEQUENCE MODELING', 'RECURRENT AND RECURSIVE NETS', 'Unfolding Computational Graphs']\n",
      "entity_token: [tensor([12342,  4880, 24846, 15517,  2036,   150, 15609, 21678, 15740]), tensor([  155,  8231, 19556, 16941, 15681, 16716,   155,  8231, 19556, 13882,\n",
      "        17145, 26546, 11365]), tensor([12118, 10787,  1158,  3291,  8223, 15012, 15937,   144, 14543,  9524])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Graphs A computational graph is a way to formalize the structure of a set of computations, such as\n",
      "content_token: tensor([  101,   144, 14543,  9524,   138, 19903, 10873,  1110,   170,  1236,\n",
      "         1106,  4698,  3708,  1103,  2401,  1104,   170,  1383,  1104,  3254,\n",
      "        19675,  1116,   117,  1216,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: such as those involved in mapping inputs and parameters to outputs and loss. Please refer to\n",
      "content_token: tensor([  101,  1216,  1112,  1343,  2017,  1107, 13970, 22743,  1105, 11934,\n",
      "         1106,  5964,  1116,  1105,  2445,   119,  4203,  5991,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Please refer to section 6.5.1 for a general introduction. In this section we explain the idea of\n",
      "content_token: tensor([ 101, 4203, 5991, 1106, 2237,  127,  119,  126,  119,  122, 1111,  170,\n",
      "        1704, 4784,  119, 1130, 1142, 2237, 1195, 4137, 1103, 1911, 1104,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: explain the idea of unfolding a recursive or recurrent computation into a computational graph that\n",
      "content_token: tensor([  101,  4137,  1103,  1911,  1104,  8362, 10787,  1158,   170,  1231,\n",
      "        10182,  1733,  2109,  1137,  1231, 21754,  3254, 19675,  1154,   170,\n",
      "        19903, 10873,  1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: graph that has a repetitive structure, typically corresponding to a chain of events. Unfolding this\n",
      "content_token: tensor([  101, 10873,  1115,  1144,   170, 26976,  2401,   117,  3417,  7671,\n",
      "         1106,   170,  4129,  1104,  1958,   119, 12118, 10787,  1158,  1142,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Unfolding this graph results in the sharing of parameters across a deep network structure. For\n",
      "content_token: tensor([  101, 12118, 10787,  1158,  1142, 10873,  2686,  1107,  1103,  6303,\n",
      "         1104, 11934,  1506,   170,  1996,  2443,  2401,   119,  1370,   102])\n",
      "entity_list: ['deep network structure']\n",
      "entity_token: [tensor([1996, 2443, 2401])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: structure. For example, consider the classical form of a dynamical system: s(t) = f(s(t 1);θ),\n",
      "content_token: tensor([ 101, 2401,  119, 1370, 1859,  117, 4615, 1103, 4521, 1532, 1104,  170,\n",
      "        9652, 1348, 1449,  131,  188,  113,  189,  114,  134,  175,  113,  188,\n",
      "         113,  189,  122,  114,  132,  425,  114,  117,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: s(t) = f(s(t 1);θ), (10.1) − where s(t) is called the state of the system. Equation 10.1 is\n",
      "content_token: tensor([  101,   188,   113,   189,   114,   134,   175,   113,   188,   113,\n",
      "          189,   122,   114,   132,   425,   114,   117,   113,  1275,   119,\n",
      "          122,   114,   851,  1187,   188,   113,   189,   114,  1110,  1270,\n",
      "         1103,  1352,  1104,  1103,  1449,   119,   142, 13284,  2116,  1275,\n",
      "          119,   122,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Equation 10.1 is recurrent because the definition of s at time t refers back to the same definition\n",
      "content_token: tensor([  101,   142, 13284,  2116,  1275,   119,   122,  1110,  1231, 21754,\n",
      "         1272,  1103,  5754,  1104,   188,  1120,  1159,   189,  4431,  1171,\n",
      "         1106,  1103,  1269,  5754,   102])\n",
      "entity_list: ['recurrent']\n",
      "entity_token: [tensor([ 1231, 21754])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the same definition at time t 1. − For a finite number of time steps τ, the graph can be unfolded\n",
      "content_token: tensor([  101,  1103,  1269,  5754,  1120,  1159,   189,   122,   119,   851,\n",
      "         1370,   170, 10996,  1295,  1104,  1159,  3343,   437,   117,  1103,\n",
      "        10873,  1169,  1129, 27118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can be unfolded by applying the definition τ 1 times. For example, if we unfold equation 10.1 for τ\n",
      "content_token: tensor([  101,  1169,  1129, 27118,  1118, 11892,  1103,  5754,   437,   122,\n",
      "         1551,   119,  1370,  1859,   117,  1191,  1195,  8362, 10787,  8381,\n",
      "         1275,   119,   122,  1111,   437,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: equation 10.1 for τ = 3 time − steps, we obtain s(3) =f(s(2) ;θ) (10.2) =f(f(s(1) ;θ);θ) (10.3)\n",
      "content_token: tensor([ 101, 8381, 1275,  119,  122, 1111,  437,  134,  124, 1159,  851, 3343,\n",
      "         117, 1195, 6268,  188,  113,  124,  114,  134,  175,  113,  188,  113,\n",
      "         123,  114,  132,  425,  114,  113, 1275,  119,  123,  114,  134,  175,\n",
      "         113,  175,  113,  188,  113,  122,  114,  132,  425,  114,  132,  425,\n",
      "         114,  113, 1275,  119,  124,  114,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ;θ);θ) (10.3) Unfolding the equation by repeatedly applying the definition in this way has yielded\n",
      "content_token: tensor([  101,   132,   425,   114,   132,   425,   114,   113,  1275,   119,\n",
      "          124,   114, 12118, 10787,  1158,  1103,  8381,  1118,  8038, 11892,\n",
      "         1103,  5754,  1107,  1142,  1236,  1144, 18826,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: way has yielded an expression that does not involve recurrence. Such an expression can now be\n",
      "content_token: tensor([  101,  1236,  1144, 18826,  1126,  2838,  1115,  1674,  1136,  8803,\n",
      "         1231, 10182, 21629,   119,  5723,  1126,  2838,  1169,  1208,  1129,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can now be represented by a traditional directed acyclic computational graph. The unfolded\n",
      "content_token: tensor([  101,  1169,  1208,  1129,  2533,  1118,   170,  2361,  2002,   170,\n",
      "         3457,  1665,  8031, 19903, 10873,   119,  1109, 27118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: graph. The unfolded computational graph of equation 10.1 and equation 10.3 is illustrated in figure\n",
      "content_token: tensor([  101, 10873,   119,  1109, 27118, 19903, 10873,  1104,  8381,  1275,\n",
      "          119,   122,  1105,  8381,  1275,   119,   124,  1110,  8292,  1107,\n",
      "         2482,   102])\n",
      "entity_list: ['computational graph']\n",
      "entity_token: [tensor([19903, 10873])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in figure 10.1. ss((......)) ss((tt 11)) ss((tt)) ss((tt++11)) ss((......)) −− ff ff ff ff Figure\n",
      "content_token: tensor([  101,  1107,  2482,  1275,   119,   122,   119,   188,  1116,   113,\n",
      "          113,   119,   119,   119,   119,   119,   119,   114,   114,   188,\n",
      "         1116,   113,   113,   189,  1204,  1429,   114,   114,   188,  1116,\n",
      "          113,   113,   189,  1204,   114,   114,   188,  1116,   113,   113,\n",
      "          189,  1204,   116,   116,  1429,   114,   114,   188,  1116,   113,\n",
      "          113,   119,   119,   119,   119,   119,   119,   114,   114,   851,\n",
      "        25532,   175,  2087,   175,  2087,   175,  2087,   175,  2087, 15982,\n",
      "          102])\n",
      "entity_list: ['computational graph']\n",
      "entity_token: [tensor([19903, 10873])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ff ff ff ff Figure 10.1: The classical dynamical system described by equation 10.1, illustrated as\n",
      "content_token: tensor([  101,   175,  2087,   175,  2087,   175,  2087,   175,  2087, 15982,\n",
      "         1275,   119,   122,   131,  1109,  4521,  9652,  1348,  1449,  1758,\n",
      "         1118,  8381,  1275,   119,   122,   117,  8292,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: illustrated as an unfolded computational graph. Each node represents the state at some time t and\n",
      "content_token: tensor([  101,  8292,  1112,  1126, 27118, 19903, 10873,   119,  2994, 14372,\n",
      "         5149,  1103,  1352,  1120,  1199,  1159,   189,  1105,   102])\n",
      "entity_list: ['computational graph']\n",
      "entity_token: [tensor([19903, 10873])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: at some time t and the function f maps the state at t to the state at t+1. The same parameters (the\n",
      "content_token: tensor([  101,  1120,  1199,  1159,   189,  1105,  1103,  3053,   175,  7415,\n",
      "         1103,  1352,  1120,   189,  1106,  1103,  1352,  1120,   189,   116,\n",
      "          122,   119,  1109,  1269, 11934,   113,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parameters (the same value of θ used to parametrize f) are used for all time steps. As another\n",
      "content_token: tensor([  101, 11934,   113,  1103,  1269,  2860,  1104,   425,  1215,  1106,\n",
      "        18311, 11006, 28021,  1162,   175,   114,  1132,  1215,  1111,  1155,\n",
      "         1159,  3343,   119,  1249,  1330,   102])\n",
      "entity_list: ['parametrize f']\n",
      "entity_token: [tensor([18311, 11006, 28021,  1162,   175])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: steps. As another example, let us consider a dynamical system driven by an external signal x(t),\n",
      "content_token: tensor([ 101, 3343,  119, 1249, 1330, 1859,  117, 1519, 1366, 4615,  170, 9652,\n",
      "        1348, 1449, 4940, 1118, 1126, 6298, 4344,  193,  113,  189,  114,  117,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: signal x(t), s(t) = f(s(t 1) ,x(t);θ), (10.4) − 375 CHAPTER 10. SEQUENCE MODELING: RECURRENT AND\n",
      "content_token: tensor([  101,  4344,   193,   113,   189,   114,   117,   188,   113,   189,\n",
      "          114,   134,   175,   113,   188,   113,   189,   122,   114,   117,\n",
      "          193,   113,   189,   114,   132,   425,   114,   117,   113,  1275,\n",
      "          119,   125,   114,   851, 19397,  8203,  1275,   119, 12342,  4880,\n",
      "        24846, 15517,  2036,   150, 15609, 21678, 15740,   131,   155,  8231,\n",
      "        19556, 16941, 15681, 16716,   102])\n",
      "entity_list: ['sequence modeling', 'recurrent']\n",
      "entity_token: [tensor([ 4954, 13117]), tensor([ 1231, 21754])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: RECURRENT AND RECURSIVE NETS where we seethat the state now containsinformation about the whole\n",
      "content_token: tensor([  101,   155,  8231, 19556, 16941, 15681, 16716,   155,  8231, 19556,\n",
      "        13882, 17145, 26546, 11365,  1187,  1195,  1267,  7702,  1204,  1103,\n",
      "         1352,  1208,  2515,  1394, 24152,  1164,  1103,  2006,   102])\n",
      "entity_list: ['RECURRENT AND RECURSIVE NETS']\n",
      "entity_token: [tensor([  155,  8231, 19556, 16941, 15681, 16716,   155,  8231, 19556, 13882,\n",
      "        17145, 26546, 11365])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: about the whole pastsequence. Recurrent neural networks can be built in many different ways. Much\n",
      "content_token: tensor([  101,  1164,  1103,  2006,  1763,  2217, 25113,   119, 11336, 21754,\n",
      "        18250,  6379,  1169,  1129,  1434,  1107,  1242,  1472,  3242,   119,\n",
      "         6335,   102])\n",
      "entity_list: ['Recurrent neural networks']\n",
      "entity_token: [tensor([11336, 21754, 18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ways. Much as almost any function can be considered a feedforward neural network, essentially any\n",
      "content_token: tensor([  101,  3242,   119,  6335,  1112,  1593,  1251,  3053,  1169,  1129,\n",
      "         1737,   170,  4877, 14467,  1197,  5984, 18250,  2443,   117,  7588,\n",
      "         1251,   102])\n",
      "entity_list: ['feedforward neural network']\n",
      "entity_token: [tensor([ 4877, 14467,  1197,  5984, 18250,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: essentially any function involving recurrence can be considered a recurrent neural network. Many\n",
      "content_token: tensor([  101,  7588,  1251,  3053,  5336,  1231, 10182, 21629,  1169,  1129,\n",
      "         1737,   170,  1231, 21754, 18250,  2443,   119,  2408,   102])\n",
      "entity_list: ['recurrent neural network']\n",
      "entity_token: [tensor([ 1231, 21754, 18250,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: network. Many recurrent neural networks use equation 10.5 or a similar equation to define the\n",
      "content_token: tensor([  101,  2443,   119,  2408,  1231, 21754, 18250,  6379,  1329,  8381,\n",
      "         1275,   119,   126,  1137,   170,  1861,  8381,  1106,  9410,  1103,\n",
      "          102])\n",
      "entity_list: ['recurrent neural networks']\n",
      "entity_token: [tensor([ 1231, 21754, 18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to define the values of their hidden units. To indicate that the state is the hidden units of the\n",
      "content_token: tensor([ 101, 1106, 9410, 1103, 4718, 1104, 1147, 4610, 2338,  119, 1706, 5057,\n",
      "        1115, 1103, 1352, 1110, 1103, 4610, 2338, 1104, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: hidden units of the network, we now rewrite equation 10.4 using the variable h to represent the\n",
      "content_token: tensor([  101,  4610,  2338,  1104,  1103,  2443,   117,  1195,  1208,  1231,\n",
      "         2246, 10587,  8381,  1275,   119,   125,  1606,  1103,  7898,   177,\n",
      "         1106,  4248,  1103,   102])\n",
      "entity_list: ['hidden units', 'network']\n",
      "entity_token: [tensor([4610, 2338]), tensor([2443])]\n",
      "label: tensor([0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: h to represent the state: h(t) = f(h(t 1) ,x(t) ;θ), (10.5) − illustrated in figure 10.2, typical\n",
      "content_token: tensor([ 101,  177, 1106, 4248, 1103, 1352,  131,  177,  113,  189,  114,  134,\n",
      "         175,  113,  177,  113,  189,  122,  114,  117,  193,  113,  189,  114,\n",
      "         132,  425,  114,  117,  113, 1275,  119,  126,  114,  851, 8292, 1107,\n",
      "        2482, 1275,  119,  123,  117, 4701,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 10.2, typical RNNs will add extra architectural features such as output layers that read\n",
      "content_token: tensor([ 101, 1275,  119,  123,  117, 4701,  155, 2249, 2249, 1116, 1209, 5194,\n",
      "        3908, 6645, 1956, 1216, 1112, 5964, 8798, 1115, 2373,  102])\n",
      "entity_list: ['RNNs', 'output layers']\n",
      "entity_token: [tensor([ 155, 2249, 2249, 1116]), tensor([5964, 8798])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: layers that read information out of the state h to make predictions. When the recurrentnetwork is\n",
      "content_token: tensor([  101,  8798,  1115,  2373,  1869,  1149,  1104,  1103,  1352,   177,\n",
      "         1106,  1294, 23770,   119,  1332,  1103,  1231, 21754,  6097,  5361,\n",
      "         1110,   102])\n",
      "entity_list: ['recurrent network']\n",
      "entity_token: [tensor([ 1231, 21754,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: recurrentnetwork is trainedto perform a task that requirespredicting the future from the past, the\n",
      "content_token: tensor([  101,  1231, 21754,  6097,  5361,  1110,  3972,  2430,  3870,   170,\n",
      "         4579,  1115,  5315,  1643,  4359, 17882,  1158,  1103,  2174,  1121,\n",
      "         1103,  1763,   117,  1103,   102])\n",
      "entity_list: ['recurrent network']\n",
      "entity_token: [tensor([ 1231, 21754,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from the past, the network typically learns to use h(t) as a kind of lossy summary of the\n",
      "content_token: tensor([  101,  1121,  1103,  1763,   117,  1103,  2443,  3417, 10123,  1106,\n",
      "         1329,   177,   113,   189,   114,  1112,   170,  1912,  1104,  2445,\n",
      "         1183, 14940,  1104,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: summary of the task-relevant aspects of the past sequence of inputs up to t. This summary is in\n",
      "content_token: tensor([  101, 14940,  1104,  1103,  4579,   118,  7503,  5402,  1104,  1103,\n",
      "         1763,  4954,  1104, 22743,  1146,  1106,   189,   119,  1188, 14940,\n",
      "         1110,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This summary is in general necessarily lossy, since it maps an arbitrary length sequence (x(t),x(t\n",
      "content_token: tensor([  101,  1188, 14940,  1110,  1107,  1704,  9073,  2445,  1183,   117,\n",
      "         1290,  1122,  7415,  1126, 16439,  2251,  4954,   113,   193,   113,\n",
      "          189,   114,   117,   193,   113,   189,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sequence (x(t),x(t 1),x(t 2),...,x(2),x(1)) to a fixed length vector h(t). Depending on the − −\n",
      "content_token: tensor([  101,  4954,   113,   193,   113,   189,   114,   117,   193,   113,\n",
      "          189,   122,   114,   117,   193,   113,   189,   123,   114,   117,\n",
      "          119,   119,   119,   117,   193,   113,   123,   114,   117,   193,\n",
      "          113,   122,   114,   114,  1106,   170,  4275,  2251,  9479,   177,\n",
      "          113,   189,   114,   119, 19285,  1113,  1103,   851,   851,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on the − − training criterion, this summary might selectively keep some aspects of the past\n",
      "content_token: tensor([  101,  1113,  1103,   851,   851,  2013, 26440,   117,  1142, 14940,\n",
      "         1547, 14930,  1193,  1712,  1199,  5402,  1104,  1103,  1763,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: aspects of the past sequence with more precision than other aspects. For example, if the RNN is\n",
      "content_token: tensor([  101,  5402,  1104,  1103,  1763,  4954,  1114,  1167, 13218,  1190,\n",
      "         1168,  5402,   119,  1370,  1859,   117,  1191,  1103,   155,  2249,\n",
      "         2249,  1110,   102])\n",
      "entity_list: ['RNN']\n",
      "entity_token: [tensor([ 155, 2249, 2249])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: if the RNN is used in statistical language modeling, typically to predict the next word given\n",
      "content_token: tensor([  101,  1191,  1103,   155,  2249,  2249,  1110,  1215,  1107, 11435,\n",
      "         1846, 13117,   117,  3417,  1106, 17163,  1103,  1397,  1937,  1549,\n",
      "          102])\n",
      "entity_list: ['RNN', 'statistical language modeling']\n",
      "entity_token: [tensor([ 155, 2249, 2249]), tensor([11435,  1846, 13117])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the next word given previous words, it may not be necessary to store all of the information in the\n",
      "content_token: tensor([ 101, 1103, 1397, 1937, 1549, 2166, 1734,  117, 1122, 1336, 1136, 1129,\n",
      "        3238, 1106, 2984, 1155, 1104, 1103, 1869, 1107, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: information in the input sequence up to time t, but rather only enough information to predict the\n",
      "content_token: tensor([  101,  1869,  1107,  1103,  7758,  4954,  1146,  1106,  1159,   189,\n",
      "          117,  1133,  1897,  1178,  1536,  1869,  1106, 17163,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to predict the rest of the sentence. The most demanding situation is when we ask h(t) to be rich\n",
      "content_token: tensor([  101,  1106, 17163,  1103,  1832,  1104,  1103,  5650,   119,  1109,\n",
      "         1211,  9504,  2820,  1110,  1165,  1195,  2367,   177,   113,   189,\n",
      "          114,  1106,  1129,  3987,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ask h(t) to be rich enough to allow one to approximately recover the input sequence, as in\n",
      "content_token: tensor([ 101, 2367,  177,  113,  189,  114, 1106, 1129, 3987, 1536, 1106, 2621,\n",
      "        1141, 1106, 2324, 8680, 1103, 7758, 4954,  117, 1112, 1107,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sequence, as in autoencoder frameworks (chapter 14). hh hh((......)) hh((tt 11)) hh((tt))\n",
      "content_token: tensor([  101,  4954,   117,  1112,  1107, 12365,  1424, 13775,  1197,  8297,\n",
      "         1116,   113,  6073,  1489,   114,   119,   177,  1324,   177,  1324,\n",
      "          113,   113,   119,   119,   119,   119,   119,   119,   114,   114,\n",
      "          177,  1324,   113,   113,   189,  1204,  1429,   114,   114,   177,\n",
      "         1324,   113,   113,   189,  1204,   114,   114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 11)) hh((tt)) hh((tt++11)) hh((......)) −− ff ff ff f ff Unfold xx xx((tt 11)) xx((tt))\n",
      "content_token: tensor([  101,  1429,   114,   114,   177,  1324,   113,   113,   189,  1204,\n",
      "          114,   114,   177,  1324,   113,   113,   189,  1204,   116,   116,\n",
      "         1429,   114,   114,   177,  1324,   113,   113,   119,   119,   119,\n",
      "          119,   119,   119,   114,   114,   851, 25532,   175,  2087,   175,\n",
      "         2087,   175,  2087,   175,   175,  2087, 12118, 10787,   193,  1775,\n",
      "          193,  1775,   113,   113,   189,  1204,  1429,   114,   114,   193,\n",
      "         1775,   113,   113,   189,  1204,   114,   114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 11)) xx((tt)) xx((tt++11)) −− Figure 10.2: A recurrent network with no outputs. This recurrent\n",
      "content_token: tensor([  101,  1429,   114,   114,   193,  1775,   113,   113,   189,  1204,\n",
      "          114,   114,   193,  1775,   113,   113,   189,  1204,   116,   116,\n",
      "         1429,   114,   114,   851, 25532, 15982,  1275,   119,   123,   131,\n",
      "          138,  1231, 21754,  2443,  1114,  1185,  5964,  1116,   119,  1188,\n",
      "         1231, 21754,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This recurrent network just processes information from the input x by incorporating it into the\n",
      "content_token: tensor([  101,  1188,  1231, 21754,  2443,  1198,  5669,  1869,  1121,  1103,\n",
      "         7758,   193,  1118, 14239,  1122,  1154,  1103,   102])\n",
      "entity_list: ['recurrent network']\n",
      "entity_token: [tensor([ 1231, 21754,  2443])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it into the state h that is passed forward through time. (Left)Circuit diagram. The black square\n",
      "content_token: tensor([  101,  1122,  1154,  1103,  1352,   177,  1115,  1110,  2085,  1977,\n",
      "         1194,  1159,   119,   113,  8123,   114,  7887, 18217,   119,  1109,\n",
      "         1602,  1961,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The black square indicates a delay of a single time step. (Right)The same network seen as an\n",
      "content_token: tensor([ 101, 1109, 1602, 1961, 6653,  170, 8513, 1104,  170, 1423, 1159, 2585,\n",
      "         119,  113, 4114,  114, 1109, 1269, 2443, 1562, 1112, 1126,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: network seen as an unfolded computational graph, where each node is now associated with one\n",
      "content_token: tensor([  101,  2443,  1562,  1112,  1126, 27118, 19903, 10873,   117,  1187,\n",
      "         1296, 14372,  1110,  1208,  2628,  1114,  1141,   102])\n",
      "entity_list: ['computational graph']\n",
      "entity_token: [tensor([19903, 10873])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: associated with one particular time instance. Equation 10.5 can be drawn in two different ways. One\n",
      "content_token: tensor([  101,  2628,  1114,  1141,  2440,  1159,  5374,   119,   142, 13284,\n",
      "         2116,  1275,   119,   126,  1169,  1129,  3795,  1107,  1160,  1472,\n",
      "         3242,   119,  1448,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: different ways. One way to draw the RNN is with a diagram containing one node for every component\n",
      "content_token: tensor([  101,  1472,  3242,   119,  1448,  1236,  1106,  3282,  1103,   155,\n",
      "         2249,  2249,  1110,  1114,   170, 18217,  4051,  1141, 14372,  1111,\n",
      "         1451,  6552,   102])\n",
      "entity_list: ['RNN']\n",
      "entity_token: [tensor([ 155, 2249, 2249])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for every component that might exist in a 376 CHAPTER 10. SEQUENCE MODELING: RECURRENT AND\n",
      "content_token: tensor([  101,  1111,  1451,  6552,  1115,  1547,  4056,  1107,   170,  3413,\n",
      "         1545,  8203,  1275,   119, 12342,  4880, 24846, 15517,  2036,   150,\n",
      "        15609, 21678, 15740,   131,   155,  8231, 19556, 16941, 15681, 16716,\n",
      "          102])\n",
      "entity_list: ['SEQUENCE MODELING', 'RECURRENT']\n",
      "entity_token: [tensor([12342,  4880, 24846, 15517,  2036,   150, 15609, 21678, 15740]), tensor([  155,  8231, 19556, 16941, 15681])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        2, 1, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: RECURRENT AND RECURSIVE NETS physical implementation of the model, such as a biological neural\n",
      "content_token: tensor([  101,   155,  8231, 19556, 16941, 15681, 16716,   155,  8231, 19556,\n",
      "        13882, 17145, 26546, 11365,  2952,  7249,  1104,  1103,  2235,   117,\n",
      "         1216,  1112,   170,  7269, 18250,   102])\n",
      "entity_list: ['RECURRENT AND RECURSIVE NETS']\n",
      "entity_token: [tensor([  155,  8231, 19556, 16941, 15681, 16716,   155,  8231, 19556, 13882,\n",
      "        17145, 26546, 11365])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a biological neural network. In this view, the network defines a circuit that operates in real\n",
      "content_token: tensor([  101,   170,  7269, 18250,  2443,   119,  1130,  1142,  2458,   117,\n",
      "         1103,  2443, 12028,   170,  6090,  1115,  5049,  1107,  1842,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: operates in real time, with physical parts whose current state can influence their future state, as\n",
      "content_token: tensor([ 101, 5049, 1107, 1842, 1159,  117, 1114, 2952, 2192, 2133, 1954, 1352,\n",
      "        1169, 2933, 1147, 2174, 1352,  117, 1112,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: future state, as in the left of figure 10.2. Throughout this chapter, we use a black square in a\n",
      "content_token: tensor([ 101, 2174, 1352,  117, 1112, 1107, 1103, 1286, 1104, 2482, 1275,  119,\n",
      "         123,  119, 7092, 1142, 6073,  117, 1195, 1329,  170, 1602, 1961, 1107,\n",
      "         170,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a black square in a circuit diagram to indicate that an interaction takes place with a delay of a\n",
      "content_token: tensor([  101,   170,  1602,  1961,  1107,   170,  6090, 18217,  1106,  5057,\n",
      "         1115,  1126,  8234,  2274,  1282,  1114,   170,  8513,  1104,   170,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with a delay of a single time step, from the state at time t to the state at time t+ 1. The other\n",
      "content_token: tensor([ 101, 1114,  170, 8513, 1104,  170, 1423, 1159, 2585,  117, 1121, 1103,\n",
      "        1352, 1120, 1159,  189, 1106, 1103, 1352, 1120, 1159,  189,  116,  122,\n",
      "         119, 1109, 1168,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: t+ 1. The other way to draw the RNN is as an unfolded computational graph, in which each component\n",
      "content_token: tensor([  101,   189,   116,   122,   119,  1109,  1168,  1236,  1106,  3282,\n",
      "         1103,   155,  2249,  2249,  1110,  1112,  1126, 27118, 19903, 10873,\n",
      "          117,  1107,  1134,  1296,  6552,   102])\n",
      "entity_list: ['RNN', 'unfolded computational graph']\n",
      "entity_token: [tensor([ 155, 2249, 2249]), tensor([27118, 19903, 10873])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: each component is represented by many different variables, with one variable per time step,\n",
      "content_token: tensor([  101,  1296,  6552,  1110,  2533,  1118,  1242,  1472, 10986,   117,\n",
      "         1114,  1141,  7898,  1679,  1159,  2585,   117,   102])\n",
      "entity_list: ['RNN']\n",
      "entity_token: [tensor([ 155, 2249, 2249])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: per time step, representing the state of the component at that point in time. Each variable for\n",
      "content_token: tensor([ 101, 1679, 1159, 2585,  117, 4311, 1103, 1352, 1104, 1103, 6552, 1120,\n",
      "        1115, 1553, 1107, 1159,  119, 2994, 7898, 1111,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Each variable for each time step is drawn as a separate node of the computational graph, as in the\n",
      "content_token: tensor([  101,  2994,  7898,  1111,  1296,  1159,  2585,  1110,  3795,  1112,\n",
      "          170,  2767, 14372,  1104,  1103, 19903, 10873,   117,  1112,  1107,\n",
      "         1103,   102])\n",
      "entity_list: [\"'computational graph'\"]\n",
      "entity_token: [tensor([  112, 19903, 10873,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: graph, as in the right of figure 10.2. What we call unfolding is the operation that maps a circuit\n",
      "content_token: tensor([  101, 10873,   117,  1112,  1107,  1103,  1268,  1104,  2482,  1275,\n",
      "          119,   123,   119,  1327,  1195,  1840,  8362, 10787,  1158,  1110,\n",
      "         1103,  2805,  1115,  7415,   170,  6090,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that maps a circuit as in the left side of the figure to a computational graph with repeated pieces\n",
      "content_token: tensor([  101,  1115,  7415,   170,  6090,  1112,  1107,  1103,  1286,  1334,\n",
      "         1104,  1103,  2482,  1106,   170, 19903, 10873,  1114,  4892,  3423,\n",
      "          102])\n",
      "entity_list: [\"'computational graph'\"]\n",
      "entity_token: [tensor([  112, 19903, 10873,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: repeated pieces as in the right side. The unfolded graph now has a size that depends on the\n",
      "content_token: tensor([  101,  4892,  3423,  1112,  1107,  1103,  1268,  1334,   119,  1109,\n",
      "        27118, 10873,  1208,  1144,   170,  2060,  1115,  9113,  1113,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that depends on the sequence length. We can represent the unfolded recurrence after t steps with a\n",
      "content_token: tensor([  101,  1115,  9113,  1113,  1103,  4954,  2251,   119,  1284,  1169,\n",
      "         4248,  1103, 27118,  1231, 10182, 21629,  1170,   189,  3343,  1114,\n",
      "          170,   102])\n",
      "entity_list: [\"'unfolded recurrence'\"]\n",
      "entity_token: [tensor([  112, 27118,  1231, 10182, 21629,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: t steps with a function g(t): h(t) =g(t)(x(t),x(t 1),x(t 2),...,x(2),x(1) ) (10.6) − − =f(h(t\n",
      "content_token: tensor([ 101,  189, 3343, 1114,  170, 3053,  176,  113,  189,  114,  131,  177,\n",
      "         113,  189,  114,  134,  176,  113,  189,  114,  113,  193,  113,  189,\n",
      "         114,  117,  193,  113,  189,  122,  114,  117,  193,  113,  189,  123,\n",
      "         114,  117,  119,  119,  119,  117,  193,  113,  123,  114,  117,  193,\n",
      "         113,  122,  114,  114,  113, 1275,  119,  127,  114,  851,  851,  134,\n",
      "         175,  113,  177,  113,  189,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ) (10.6) − − =f(h(t 1),x(t) ;θ) (10.7) − The function g(t) takes the whole past sequence (x(t),x(t\n",
      "content_token: tensor([ 101,  114,  113, 1275,  119,  127,  114,  851,  851,  134,  175,  113,\n",
      "         177,  113,  189,  122,  114,  117,  193,  113,  189,  114,  132,  425,\n",
      "         114,  113, 1275,  119,  128,  114,  851, 1109, 3053,  176,  113,  189,\n",
      "         114, 2274, 1103, 2006, 1763, 4954,  113,  193,  113,  189,  114,  117,\n",
      "         193,  113,  189,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sequence (x(t),x(t 1),x(t 2),...,x(2),x(1)) − − as input and produces the current state, but the\n",
      "content_token: tensor([ 101, 4954,  113,  193,  113,  189,  114,  117,  193,  113,  189,  122,\n",
      "         114,  117,  193,  113,  189,  123,  114,  117,  119,  119,  119,  117,\n",
      "         193,  113,  123,  114,  117,  193,  113,  122,  114,  114,  851,  851,\n",
      "        1112, 7758, 1105, 6570, 1103, 1954, 1352,  117, 1133, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: state, but the unfolded recurrent structure allows us to factorize g(t) into repeated application\n",
      "content_token: tensor([  101,  1352,   117,  1133,  1103, 27118,  1231, 21754,  2401,  3643,\n",
      "         1366,  1106,  5318,  3708,   176,   113,   189,   114,  1154,  4892,\n",
      "         4048,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: application of a function f. The unfolding process thus introduces two major advantages: 1.\n",
      "content_token: tensor([  101,  4048,  1104,   170,  3053,   175,   119,  1109,  8362, 10787,\n",
      "         1158,  1965,  2456, 14681,  1160,  1558, 13300,   131,   122,   119,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: advantages: 1. Regardless of the sequence length, the learned model always has the same input size,\n",
      "content_token: tensor([  101, 13300,   131,   122,   119, 20498,  1104,  1103,  4954,  2251,\n",
      "          117,  1103,  3560,  2235,  1579,  1144,  1103,  1269,  7758,  2060,\n",
      "          117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: same input size, because it is specified in terms of transition from one state to another state,\n",
      "content_token: tensor([ 101, 1269, 7758, 2060,  117, 1272, 1122, 1110, 9467, 1107, 2538, 1104,\n",
      "        6468, 1121, 1141, 1352, 1106, 1330, 1352,  117,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to another state, rather than specified in terms of a variable-length history of states. 2. It is\n",
      "content_token: tensor([ 101, 1106, 1330, 1352,  117, 1897, 1190, 9467, 1107, 2538, 1104,  170,\n",
      "        7898,  118, 2251, 1607, 1104, 2231,  119,  123,  119, 1135, 1110,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of states. 2. It is possible to use the same transition function f with the same parameters at\n",
      "content_token: tensor([  101,  1104,  2231,   119,   123,   119,  1135,  1110,  1936,  1106,\n",
      "         1329,  1103,  1269,  6468,  3053,   175,  1114,  1103,  1269, 11934,\n",
      "         1120,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: same parameters at every time step. These two factors make it possible to learn a single model f\n",
      "content_token: tensor([  101,  1269, 11934,  1120,  1451,  1159,  2585,   119,  1636,  1160,\n",
      "         5320,  1294,  1122,  1936,  1106,  3858,   170,  1423,  2235,   175,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a single model f that operates on all time steps and all sequence lengths, rather than needing to\n",
      "content_token: tensor([  101,   170,  1423,  2235,   175,  1115,  5049,  1113,  1155,  1159,\n",
      "         3343,  1105,  1155,  4954, 10707,   117,  1897,  1190, 12038,  1106,\n",
      "          102])\n",
      "entity_list: ['model']\n",
      "entity_token: [tensor([2235])]\n",
      "label: tensor([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: than needing to learn a separate model g(t) for all possible time steps. Learning a single, shared\n",
      "content_token: tensor([  101,  1190, 12038,  1106,  3858,   170,  2767,  2235,   176,   113,\n",
      "          189,   114,  1111,  1155,  1936,  1159,  3343,   119,  9681,   170,\n",
      "         1423,   117,  3416,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a single, shared model allows generalization to sequence lengths that did not appear in the\n",
      "content_token: tensor([  101,   170,  1423,   117,  3416,  2235,  3643,  1704,  2734,  1106,\n",
      "         4954, 10707,  1115,  1225,  1136,  2845,  1107,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: not appear in the training set, and allows the model to be estimated with far fewer training\n",
      "content_token: tensor([ 101, 1136, 2845, 1107, 1103, 2013, 1383,  117, 1105, 3643, 1103, 2235,\n",
      "        1106, 1129, 3555, 1114, 1677, 8307, 2013,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: far fewer training examples than would be required without parameter sharing. Both the recurrent\n",
      "content_token: tensor([  101,  1677,  8307,  2013,  5136,  1190,  1156,  1129,  2320,  1443,\n",
      "        17816,  6303,   119,  2695,  1103,  1231, 21754,   102])\n",
      "entity_list: ['parameter sharing', 'recurrent']\n",
      "entity_token: [tensor([17816,  6303]), tensor([ 1231, 21754])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Both the recurrent graph and the unrolled graph have their uses. The recurrent graph is succinct.\n",
      "content_token: tensor([  101,  2695,  1103,  1231, 21754, 10873,  1105,  1103,  8362, 10747,\n",
      "         1174, 10873,  1138,  1147,  2745,   119,  1109,  1231, 21754, 10873,\n",
      "         1110, 28117, 19557, 26405,  1204,   119,   102])\n",
      "entity_list: ['recurrent graph']\n",
      "entity_token: [tensor([ 1231, 21754, 10873])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: graph is succinct. The unfolded graph provides an explicit description of which computations to\n",
      "content_token: tensor([  101, 10873,  1110, 28117, 19557, 26405,  1204,   119,  1109, 27118,\n",
      "        10873,  2790,  1126, 14077,  6136,  1104,  1134,  3254, 19675,  1116,\n",
      "         1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: computations to perform. The unfolded graph also helps to illustrate the idea of 377\n",
      "content_token: tensor([  101,  3254, 19675,  1116,  1106,  3870,   119,  1109, 27118, 10873,\n",
      "         1145,  6618,  1106, 20873,  1103,  1911,  1104,  3413,  1559,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 15. REPRESENTATION LEARNING not yet know how this is possible. Many factors could explain\n",
      "content_token: tensor([  101,  8203,  1405,   119,   155, 16668, 16941, 12649, 15681, 13821,\n",
      "        24805,   149, 12420,  2069, 27451, 11780,  1136,  1870,  1221,  1293,\n",
      "         1142,  1110,  1936,   119,  2408,  5320,  1180,  4137,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: could explain improved human performance—for example, the brain may use very large ensembles of\n",
      "content_token: tensor([  101,  1180,  4137,  4725,  1769,  2099,   783,  1111,  1859,   117,\n",
      "         1103,  3575,  1336,  1329,  1304,  1415, 24957,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: large ensembles of classifiers or Bayesian inference techniques. One popular hypothesis is that the\n",
      "content_token: tensor([  101,  1415, 24957,  1104,  1705, 17792,  1116,  1137,  2410, 18766,\n",
      "         1389,  1107, 16792,  4884,   119,  1448,  1927, 11066,  1110,  1115,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is that the brain is able to leverage unsupervised or semi-supervised learning. There are many ways\n",
      "content_token: tensor([  101,  1110,  1115,  1103,  3575,  1110,  1682,  1106, 24228,  8362,\n",
      "         6385,  3365, 16641,  1181,  1137,  3533,   118, 14199,  3776,   119,\n",
      "         1247,  1132,  1242,  3242,   102])\n",
      "entity_list: ['unsupervised learning', 'semi-supervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776]), tensor([ 3533,   118, 14199,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: There are many ways to leverage unlabeled data. In this chapter, we focus on the hypothesis that\n",
      "content_token: tensor([  101,  1247,  1132,  1242,  3242,  1106, 24228,  8362,  1742,  8511,\n",
      "         1174,  2233,   119,  1130,  1142,  6073,   117,  1195,  2817,  1113,\n",
      "         1103, 11066,  1115,   102])\n",
      "entity_list: ['unlabeled data']\n",
      "entity_token: [tensor([8362, 1742, 8511, 1174, 2233])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the hypothesis that the unlabeled data can be used to learn a good representation. 15.1 Greedy\n",
      "content_token: tensor([  101,  1103, 11066,  1115,  1103,  8362,  1742,  8511,  1174,  2233,\n",
      "         1169,  1129,  1215,  1106,  3858,   170,  1363,  6368,   119,  1405,\n",
      "          119,   122,   144, 15825,  1183,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 15.1 Greedy Layer-Wise Unsupervised Pretraining Unsupervised learning played a key historical role\n",
      "content_token: tensor([  101,  1405,   119,   122,   144, 15825,  1183, 22002,  1200,   118,\n",
      "        16089, 12118,  6385,  3365, 16641,  1181, 11689,  4487, 16534, 12118,\n",
      "         6385,  3365, 16641,  1181,  3776,  1307,   170,  2501,  3009,  1648,\n",
      "          102])\n",
      "entity_list: ['Unsupervised learning']\n",
      "entity_token: [tensor([12118,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: key historical role in the revival of deep neural networks, enabling researchers for the first time\n",
      "content_token: tensor([  101,  2501,  3009,  1648,  1107,  1103,  9408,  1104,  1996, 18250,\n",
      "         6379,   117, 12619,  6962,  1111,  1103,  1148,  1159,   102])\n",
      "entity_list: ['deep neural networks']\n",
      "entity_token: [tensor([ 1996, 18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for the first time to train a deep supervised network without requiring architectural\n",
      "content_token: tensor([  101,  1111,  1103,  1148,  1159,  1106,  2669,   170,  1996, 14199,\n",
      "         2443,  1443,  8753,  6645,   102])\n",
      "entity_list: ['deep supervised network']\n",
      "entity_token: [tensor([ 1996, 14199,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: architectural specializations like convolution or recurrence. We call this procedure unsupervised\n",
      "content_token: tensor([  101,  6645,  1957, 20412,  1176, 14255,  6005, 18404,  1137,  1231,\n",
      "        10182, 21629,   119,  1284,  1840,  1142,  7791,  8362,  6385,  3365,\n",
      "        16641,  1181,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: unsupervised pretraining, or more precisely, greedy layer- wise unsupervised pretraining. This\n",
      "content_token: tensor([  101,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,   117,\n",
      "         1137,  1167, 11228,   117, 24007,  6440,   118, 10228,  8362,  6385,\n",
      "         3365, 16641,  1181,  3073,  4487, 16534,   119,  1188,   102])\n",
      "entity_list: ['unsupervised pretraining', 'greedy layer-wise unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534]), tensor([24007,  6440,   118, 10228,  8362,  6385,  3365, 16641,  1181,  3073,\n",
      "         4487, 16534])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining. This procedure is a canonical example of how a representation learned for one task\n",
      "content_token: tensor([  101,  3073,  4487, 16534,   119,  1188,  7791,  1110,   170, 21768,\n",
      "         1859,  1104,  1293,   170,  6368,  3560,  1111,  1141,  4579,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for one task (unsupervised learning, trying to capture the shape of the input distribution) can\n",
      "content_token: tensor([  101,  1111,  1141,  4579,   113,  8362,  6385,  3365, 16641,  1181,\n",
      "         3776,   117,  1774,  1106,  4821,  1103,  3571,  1104,  1103,  7758,\n",
      "         3735,   114,  1169,   102])\n",
      "entity_list: ['unsupervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distribution) can sometimes be useful for another task (supervised learning with the same input\n",
      "content_token: tensor([  101,  3735,   114,  1169,  2121,  1129,  5616,  1111,  1330,  4579,\n",
      "          113, 14199,  3776,  1114,  1103,  1269,  7758,   102])\n",
      "entity_list: ['supervised learning']\n",
      "entity_token: [tensor([14199,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with the same input domain). Greedy layer-wise unsupervised pretraining relies on a single-layer\n",
      "content_token: tensor([  101,  1114,  1103,  1269,  7758,  5777,   114,   119,   144, 15825,\n",
      "         1183,  6440,   118, 10228,  8362,  6385,  3365, 16641,  1181,  3073,\n",
      "         4487, 16534, 17918,  1113,   170,  1423,   118,  6440,   102])\n",
      "entity_list: ['greedy layer-wise unsupervised pretraining']\n",
      "entity_token: [tensor([24007,  6440,   118, 10228,  8362,  6385,  3365, 16641,  1181,  3073,\n",
      "         4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on a single-layer represen- tation learning algorithm such as an RBM, a single-layer autoencoder, a\n",
      "content_token: tensor([  101,  1113,   170,  1423,   118,  6440,  1231,  1643,  4894,  1424,\n",
      "          118, 27629,  2116,  3776,  9932,  1216,  1112,  1126, 24718,  2107,\n",
      "          117,   170,  1423,   118,  6440, 12365,  1424, 13775,  1197,   117,\n",
      "          170,   102])\n",
      "entity_list: ['RBM', 'autoencoder']\n",
      "entity_token: [tensor([24718,  2107]), tensor([12365,  1424, 13775,  1197])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0,\n",
      "        0, 2, 1, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: autoencoder, a sparse coding model, or another model that learns latent representations. Each layer\n",
      "content_token: tensor([  101, 12365,  1424, 13775,  1197,   117,   170, 22726, 19350,  2235,\n",
      "          117,  1137,  1330,  2235,  1115, 10123,  1523,  2227, 16539,   119,\n",
      "         2994,  6440,   102])\n",
      "entity_list: ['autoencoder', 'sparse coding']\n",
      "entity_token: [tensor([12365,  1424, 13775,  1197]), tensor([22726, 19350])]\n",
      "label: tensor([0, 2, 1, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Each layer is pretrained using unsupervised learning, taking the output of the previous layer and\n",
      "content_token: tensor([  101,  2994,  6440,  1110,  3073,  4487,  9044,  1606,  8362,  6385,\n",
      "         3365, 16641,  1181,  3776,   117,  1781,  1103,  5964,  1104,  1103,\n",
      "         2166,  6440,  1105,   102])\n",
      "entity_list: ['unsupervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: previous layer and producing as output a new representation of the data, whose distribution (or its\n",
      "content_token: tensor([ 101, 2166, 6440, 1105, 4411, 1112, 5964,  170, 1207, 6368, 1104, 1103,\n",
      "        2233,  117, 2133, 3735,  113, 1137, 1157,  102])\n",
      "entity_list: ['previous layer']\n",
      "entity_token: [tensor([2166, 6440])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (or its relation to other variables such as categories to predict) is hopefully simpler. See\n",
      "content_token: tensor([  101,   113,  1137,  1157,  6796,  1106,  1168, 10986,  1216,  1112,\n",
      "         6788,  1106, 17163,   114,  1110, 16121, 17633,   119,  3969,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: simpler. See algorithm 15.1 for a formal description. Greedy layer-wise training procedures based\n",
      "content_token: tensor([  101, 17633,   119,  3969,  9932,  1405,   119,   122,  1111,   170,\n",
      "         4698,  6136,   119,   144, 15825,  1183,  6440,   118, 10228,  2013,\n",
      "         8826,  1359,   102])\n",
      "entity_list: ['Greedy layer-wise training procedures']\n",
      "entity_token: [tensor([  144, 15825,  1183,  6440,   118, 10228,  2013,  8826])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: procedures based on unsupervised criteria have long been used to sidestep the difficulty of jointly\n",
      "content_token: tensor([  101,  8826,  1359,  1113,  8362,  6385,  3365, 16641,  1181,  9173,\n",
      "         1138,  1263,  1151,  1215,  1106,  3091, 21747,  1103,  7262,  1104,\n",
      "        10824,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of jointly training the layers of a deep neural net for a supervised task. This approach dates back\n",
      "content_token: tensor([  101,  1104, 10824,  2013,  1103,  8798,  1104,   170,  1996, 18250,\n",
      "         5795,  1111,   170, 14199,  4579,   119,  1188,  3136,  4595,  1171,\n",
      "          102])\n",
      "entity_list: ['deep neural net']\n",
      "entity_token: [tensor([ 1996, 18250,  5795])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: approach dates back at least as far as the Neocognitron (Fukushima, 1975). The deep learning\n",
      "content_token: tensor([  101,  3136,  4595,  1171,  1120,  1655,  1112,  1677,  1112,  1103,\n",
      "        14521,  2528, 22152, 19138,   113, 14763, 27974,   117,  2429,   114,\n",
      "          119,  1109,  1996,  3776,   102])\n",
      "entity_list: ['Neocognitron', 'deep learning']\n",
      "entity_token: [tensor([14521,  2528, 22152, 19138]), tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The deep learning renaissance of 2006 began with the discovery that this greedy learning procedure\n",
      "content_token: tensor([  101,  1109,  1996,  3776,  1231, 26727, 11655,  3633,  1104,  1386,\n",
      "         1310,  1114,  1103,  6004,  1115,  1142, 24007,  3776,  7791,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning procedure could be used to find a good initialization for a joint learning procedure over\n",
      "content_token: tensor([ 101, 3776, 7791, 1180, 1129, 1215, 1106, 1525,  170, 1363, 3288, 2734,\n",
      "        1111,  170, 4091, 3776, 7791, 1166,  102])\n",
      "entity_list: ['joint learning procedure']\n",
      "entity_token: [tensor([4091, 3776, 7791])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: procedure over all the layers, and that this approach could be used to successfully train even\n",
      "content_token: tensor([ 101, 7791, 1166, 1155, 1103, 8798,  117, 1105, 1115, 1142, 3136, 1180,\n",
      "        1129, 1215, 1106, 4358, 2669, 1256,  102])\n",
      "entity_list: ['layers']\n",
      "entity_token: [tensor([8798])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: train even fully connected architectures (Hinton et al., 2006; Hinton and Salakhutdinov, 2006;\n",
      "content_token: tensor([  101,  2669,  1256,  3106,  3387,  4220,  1116,   113,  8790, 13124,\n",
      "         3084,  2393,   119,   117,  1386,   132,  8790, 13124,  1105, 18613,\n",
      "         3715,  6583,  1204,  7126,  3292,   117,  1386,   132,   102])\n",
      "entity_list: ['fully connected architectures', 'Hinton et al.', 'Hinton and Salakhutdinov']\n",
      "entity_token: [tensor([3106, 3387, 4220, 1116]), tensor([ 8790, 13124,  3084,  2393,   119]), tensor([ 8790, 13124,  1105, 18613,  3715,  6583,  1204,  7126,  3292])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 2006; Hinton, 2006; Bengio et al., 2007; Ranzato et al., 2007a). Prior to this discovery, only\n",
      "content_token: tensor([  101,  1386,   132,  8790, 13124,   117,  1386,   132,  3096, 10712,\n",
      "         3084,  2393,   119,   117,  1384,   132, 16890, 12541,  2430,  3084,\n",
      "         2393,   119,   117,  1384,  1161,   114,   119,  4602,  1106,  1142,\n",
      "         6004,   117,  1178,   102])\n",
      "entity_list: ['Hinton', 'Bengio et al.', 'Ranzato et al.']\n",
      "entity_token: [tensor([ 8790, 13124]), tensor([ 3096, 10712,  3084,  2393,   119]), tensor([16890, 12541,  2430,  3084,  2393,   119])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: discovery, only convolutional deep networks or networks whose depth resulted from recurrence were\n",
      "content_token: tensor([  101,  6004,   117,  1178, 14255,  6005, 18404,  1348,  1996,  6379,\n",
      "         1137,  6379,  2133,  5415,  3657,  1121,  1231, 10182, 21629,  1127,\n",
      "          102])\n",
      "entity_list: ['convolutional deep networks']\n",
      "entity_token: [tensor([14255,  6005, 18404,  1348,  1996,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: recurrence were regarded as feasible to train. Today, we now know that greedy layer-wise\n",
      "content_token: tensor([  101,  1231, 10182, 21629,  1127,  4485,  1112, 25667,  1106,  2669,\n",
      "          119,  3570,   117,  1195,  1208,  1221,  1115, 24007,  6440,   118,\n",
      "        10228,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: greedy layer-wise pretraining is not required to train fully connected deep architectures, but the\n",
      "content_token: tensor([  101, 24007,  6440,   118, 10228,  3073,  4487, 16534,  1110,  1136,\n",
      "         2320,  1106,  2669,  3106,  3387,  1996,  4220,  1116,   117,  1133,\n",
      "         1103,   102])\n",
      "entity_list: ['greedy layer-wise pretraining']\n",
      "entity_token: [tensor([24007,  6440,   118, 10228,  3073,  4487, 16534])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: but the unsupervised pretraining approach was the first method to succeed. Greedy layer-wise\n",
      "content_token: tensor([  101,  1133,  1103,  8362,  6385,  3365, 16641,  1181,  3073,  4487,\n",
      "        16534,  3136,  1108,  1103,  1148,  3442,  1106,  9381,   119,   144,\n",
      "        15825,  1183,  6440,   118, 10228,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Greedy layer-wise pretraining is called greedy because it is a greedy algo- 528 CHAPTER 15.\n",
      "content_token: tensor([  101,   144, 15825,  1183,  6440,   118, 10228,  3073,  4487, 16534,\n",
      "         1110,  1270, 24007,  1272,  1122,  1110,   170, 24007,  2393,  2758,\n",
      "          118,  3882,  1604,  8203,  1405,   119,   102])\n",
      "entity_list: ['Greedy layer-wise pretraining']\n",
      "entity_token: [tensor([  144, 15825,  1183,  6440,   118, 10228,  3073,  4487, 16534])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 528 CHAPTER 15. REPRESENTATION LEARNING rithm, meaning that it optimizes each piece of the solution\n",
      "content_token: tensor([  101,  3882,  1604,  8203,  1405,   119,   155, 16668, 16941, 12649,\n",
      "        15681, 13821, 24805,   149, 12420,  2069, 27451, 11780,   187,  7088,\n",
      "         1306,   117,  2764,  1115,  1122, 11769,  3121, 19092,  1116,  1296,\n",
      "         2727,  1104,  1103,  5072,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the solution independently, one piece at a time, rather than jointly optimizing all pieces. It\n",
      "content_token: tensor([  101,  1104,  1103,  5072,  8942,   117,  1141,  2727,  1120,   170,\n",
      "         1159,   117,  1897,  1190, 10824, 11769,  3121, 25596,  1155,  3423,\n",
      "          119,  1135,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: all pieces. It is called layer-wise because these independent pieces are the layers of the network.\n",
      "content_token: tensor([  101,  1155,  3423,   119,  1135,  1110,  1270,  6440,   118, 10228,\n",
      "         1272,  1292,  2457,  3423,  1132,  1103,  8798,  1104,  1103,  2443,\n",
      "          119,   102])\n",
      "entity_list: ['layer-wise']\n",
      "entity_token: [tensor([ 6440,   118, 10228])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the network. Specifically, greedy layer-wise pretraining proceeds one layer at a time, training\n",
      "content_token: tensor([  101,  1104,  1103,  2443,   119, 21325,   117, 24007,  6440,   118,\n",
      "        10228,  3073,  4487, 16534, 11283,  1141,  6440,  1120,   170,  1159,\n",
      "          117,  2013,   102])\n",
      "entity_list: ['greedy layer-wise pretraining']\n",
      "entity_token: [tensor([24007,  6440,   118, 10228,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: at a time, training the k-th layer while keeping the previous ones fixed. In particular, the lower\n",
      "content_token: tensor([  101,  1120,   170,  1159,   117,  2013,  1103,   180,   118, 24438,\n",
      "         6440,  1229,  3709,  1103,  2166,  3200,  4275,   119,  1130,  2440,\n",
      "          117,  1103,  2211,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the lower layers (which are trained first) are not adapted after the upper layers are introduced.\n",
      "content_token: tensor([ 101, 1103, 2211, 8798,  113, 1134, 1132, 3972, 1148,  114, 1132, 1136,\n",
      "        5546, 1170, 1103, 3105, 8798, 1132, 2234,  119,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are introduced. It is called unsuper- vised because each layer is trained with an unsupervised\n",
      "content_token: tensor([  101,  1132,  2234,   119,  1135,  1110,  1270,  8362,  6385,  3365,\n",
      "          118,   191,  3673,  1272,  1296,  6440,  1110,  3972,  1114,  1126,\n",
      "         8362,  6385,  3365, 16641,  1181,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: an unsupervised representation learning algorithm. However it is also called pretraining, because\n",
      "content_token: tensor([  101,  1126,  8362,  6385,  3365, 16641,  1181,  6368,  3776,  9932,\n",
      "          119,  1438,  1122,  1110,  1145,  1270,  3073,  4487, 16534,   117,\n",
      "         1272,   102])\n",
      "entity_list: ['unsupervised representation learning algorithm', 'pretraining\\n\\n解释：这个句子中包含了两个深度学习领域的实体：unsupervised representation learning algorithm和pretraining。']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  6368,  3776,  9932]), tensor([ 3073,  4487, 16534,   100,   100,  1102,   100,   100,   100,  1014,\n",
      "          980,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "          100,   100,   100,   100,   100,  1102,  8362,  6385,  3365, 16641,\n",
      "         1181,  6368,  3776,  9932,  1002,  3073,  4487, 16534,   886])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: because it is supposed to be only a first step before a joint training algorithm is applied to\n",
      "content_token: tensor([ 101, 1272, 1122, 1110, 3155, 1106, 1129, 1178,  170, 1148, 2585, 1196,\n",
      "         170, 4091, 2013, 9932, 1110, 3666, 1106,  102])\n",
      "entity_list: ['joint training algorithm\\n\\n解释：这个句子中包含了一个深度学习领域的实体：joint training algorithm。']\n",
      "entity_token: [tensor([4091, 2013, 9932,  100,  100, 1102,  100,  100,  100, 1014,  980,  100,\n",
      "         100,  100,  976,  100,  100,  100,  100,  100,  100,  100,  100,  100,\n",
      "         100, 1102, 4091, 2013, 9932,  886])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is applied to fine-tune all the layers together. In the context of a supervised learning task, it\n",
      "content_token: tensor([  101,  1110,  3666,  1106,  2503,   118,  9253,  1155,  1103,  8798,\n",
      "         1487,   119,  1130,  1103,  5618,  1104,   170, 14199,  3776,  4579,\n",
      "          117,  1122,   102])\n",
      "entity_list: ['fine-tune\\n\\n解释：这个句子中包含了一个深度学习领域的实体：fine-tune。']\n",
      "entity_token: [tensor([2503,  118, 9253,  100,  100, 1102,  100,  100,  100, 1014,  980,  100,\n",
      "         100,  100,  976,  100,  100,  100,  100,  100,  100,  100,  100,  100,\n",
      "         100, 1102, 2503,  118, 9253,  886])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning task, it can be viewed as a regularizer (in some experiments, pretraining decreases test\n",
      "content_token: tensor([  101,  3776,  4579,   117,  1122,  1169,  1129,  6497,  1112,   170,\n",
      "         2366, 17260,   113,  1107,  1199,  7857,   117,  3073,  4487, 16534,\n",
      "        19377,  2774,   102])\n",
      "entity_list: ['pretraining\\n\\n解释：这个句子中包含了两个深度学习领域的实体：regularizer和pretraining。']\n",
      "entity_token: [tensor([ 3073,  4487, 16534,   100,   100,  1102,   100,   100,   100,  1014,\n",
      "          980,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "          100,   100,   100,   100,   100,  1102,  2366, 17260,  1002,  3073,\n",
      "         4487, 16534,   886])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: decreases test error without decreasing training error) and a form of parameter initialization. It\n",
      "content_token: tensor([  101, 19377,  2774,  7353,  1443, 18326,  2013,  7353,   114,  1105,\n",
      "          170,  1532,  1104, 17816,  3288,  2734,   119,  1135,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: initialization. It is common to use the word “pretraining” to refer not only to the pretraining\n",
      "content_token: tensor([  101,  3288,  2734,   119,  1135,  1110,  1887,  1106,  1329,  1103,\n",
      "         1937,   789,  3073,  4487, 16534,   790,  1106,  5991,  1136,  1178,\n",
      "         1106,  1103,  3073,  4487, 16534,   102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to the pretraining stage itself but to the entire two phase protocol that combines the pretraining\n",
      "content_token: tensor([  101,  1106,  1103,  3073,  4487, 16534,  2016,  2111,  1133,  1106,\n",
      "         1103,  2072,  1160,  4065, 11309,  1115, 14215,  1103,  3073,  4487,\n",
      "        16534,   102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the pretraining phase and a supervised learning phase. The supervised learning phase may involve\n",
      "content_token: tensor([  101,  1103,  3073,  4487, 16534,  4065,  1105,   170, 14199,  3776,\n",
      "         4065,   119,  1109, 14199,  3776,  4065,  1336,  8803,   102])\n",
      "entity_list: ['supervised learning phase']\n",
      "entity_token: [tensor([14199,  3776,  4065])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: phase may involve training a simple classifier on top of the features learned in the pretraining\n",
      "content_token: tensor([  101,  4065,  1336,  8803,  2013,   170,  3014,  1705, 17792,  1113,\n",
      "         1499,  1104,  1103,  1956,  3560,  1107,  1103,  3073,  4487, 16534,\n",
      "          102])\n",
      "entity_list: ['pretraining', 'supervised learning phase']\n",
      "entity_token: [tensor([ 3073,  4487, 16534]), tensor([14199,  3776,  4065])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in the pretraining phase, or it may involve supervised fine-tuning of the entire network learned in\n",
      "content_token: tensor([  101,  1107,  1103,  3073,  4487, 16534,  4065,   117,  1137,  1122,\n",
      "         1336,  8803, 14199,  2503,   118, 19689,  1104,  1103,  2072,  2443,\n",
      "         3560,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: network learned in the pretraining phase. No matter what kind of unsupervised learning algorithm or\n",
      "content_token: tensor([  101,  2443,  3560,  1107,  1103,  3073,  4487, 16534,  4065,   119,\n",
      "         1302,  2187,  1184,  1912,  1104,  8362,  6385,  3365, 16641,  1181,\n",
      "         3776,  9932,  1137,   102])\n",
      "entity_list: ['unsupervised learning algorithm']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithm or what model type is employed, in the vast majority of cases, the overall training\n",
      "content_token: tensor([ 101, 9932, 1137, 1184, 2235, 2076, 1110, 4071,  117, 1107, 1103, 6047,\n",
      "        2656, 1104, 2740,  117, 1103, 2905, 2013,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: overall training scheme is nearly the same. While the choice of unsupervised learning algorithm\n",
      "content_token: tensor([  101,  2905,  2013,  5471,  1110,  2212,  1103,  1269,   119,  1799,\n",
      "         1103,  3026,  1104,  8362,  6385,  3365, 16641,  1181,  3776,  9932,\n",
      "          102])\n",
      "entity_list: ['unsupervised learning algorithm']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning algorithm will obviously impact the details, most applications of unsupervised pretraining\n",
      "content_token: tensor([  101,  3776,  9932,  1209,  5544,  3772,  1103,  4068,   117,  1211,\n",
      "         4683,  1104,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,\n",
      "          102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining follow this basic protocol. Greedy layer-wise unsupervised pretraining can also be used\n",
      "content_token: tensor([  101,  3073,  4487, 16534,  2812,  1142,  3501, 11309,   119,   144,\n",
      "        15825,  1183,  6440,   118, 10228,  8362,  6385,  3365, 16641,  1181,\n",
      "         3073,  4487, 16534,  1169,  1145,  1129,  1215,   102])\n",
      "entity_list: ['pretraining', 'Greedy layer-wise unsupervised pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534]), tensor([  144, 15825,  1183,  6440,   118, 10228,  8362,  6385,  3365, 16641,\n",
      "         1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can also be used as initialization for other unsupervised learning algorithms, such as deep\n",
      "content_token: tensor([  101,  1169,  1145,  1129,  1215,  1112,  3288,  2734,  1111,  1168,\n",
      "         8362,  6385,  3365, 16641,  1181,  3776, 14975,   117,  1216,  1112,\n",
      "         1996,   102])\n",
      "entity_list: ['pretraining', 'unsupervised learning']\n",
      "entity_token: [tensor([ 3073,  4487, 16534]), tensor([ 8362,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: such as deep autoencoders (Hinton and Salakhutdinov, 2006) and probabilistic models with many\n",
      "content_token: tensor([  101,  1216,  1112,  1996, 12365,  1424, 13775,  1733,   113,  8790,\n",
      "        13124,  1105, 18613,  3715,  6583,  1204,  7126,  3292,   117,  1386,\n",
      "          114,  1105,  5250,  2822, 15197,  5562,  3584,  1114,  1242,   102])\n",
      "entity_list: ['deep autoencoders', 'probabilistic models']\n",
      "entity_token: [tensor([ 1996, 12365,  1424, 13775,  1733]), tensor([ 5250,  2822, 15197,  5562,  3584])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        1, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: models with many layers of latent variables. Such models include deep belief networks (Hinton et\n",
      "content_token: tensor([  101,  3584,  1114,  1242,  8798,  1104,  1523,  2227, 10986,   119,\n",
      "         5723,  3584,  1511,  1996,  6369,  6379,   113,  8790, 13124,  3084,\n",
      "          102])\n",
      "entity_list: ['deep belief networks']\n",
      "entity_token: [tensor([1996, 6369, 6379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: networks (Hinton et al., 2006) and deep Boltzmann machines (Salakhutdinov and Hinton, 2009a). These\n",
      "content_token: tensor([  101,  6379,   113,  8790, 13124,  3084,  2393,   119,   117,  1386,\n",
      "          114,  1105,  1996,  9326, 23501,  4119,  6555,   113, 18613,  3715,\n",
      "         6583,  1204,  7126,  3292,  1105,  8790, 13124,   117,  1371,  1161,\n",
      "          114,   119,  1636,   102])\n",
      "entity_list: ['deep Boltzmann machines']\n",
      "entity_token: [tensor([ 1996,  9326, 23501,  4119,  6555])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 2009a). These deep generative models will be described in chapter 20. As discussed in section\n",
      "content_token: tensor([  101,  1371,  1161,   114,   119,  1636,  1996, 11974,  3946,  3584,\n",
      "         1209,  1129,  1758,  1107,  6073,  1406,   119,  1249,  6352,  1107,\n",
      "         2237,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in section 8.7.4, it is also possible to have greedy layer-wise supervised pretraining. This builds\n",
      "content_token: tensor([  101,  1107,  2237,   129,   119,   128,   119,   125,   117,  1122,\n",
      "         1110,  1145,  1936,  1106,  1138, 24007,  6440,   118, 10228, 14199,\n",
      "         3073,  4487, 16534,   119,  1188, 17850,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This builds on the premise that training a shallow network is easier than training a deep one,\n",
      "content_token: tensor([  101,  1188, 17850,  1113,  1103, 20197,  1115,  2013,   170,  8327,\n",
      "         2443,  1110,  5477,  1190,  2013,   170,  1996,  1141,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a deep one, which seems to have been validated in several contexts (Erhan et al., 2010). 15.1.1\n",
      "content_token: tensor([  101,   170,  1996,  1141,   117,  1134,  3093,  1106,  1138,  1151,\n",
      "         9221,  2913,  1107,  1317, 20011,   113,   142,  1197,  3822,  3084,\n",
      "         2393,   119,   117,  1333,   114,   119,  1405,   119,   122,   119,\n",
      "          122,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: al., 2010). 15.1.1 When and Why Does Unsupervised Pretraining Work? On many tasks, greedy\n",
      "content_token: tensor([  101,  2393,   119,   117,  1333,   114,   119,  1405,   119,   122,\n",
      "          119,   122,  1332,  1105,  2009,  7187, 12118,  6385,  3365, 16641,\n",
      "         1181, 11689,  4487, 16534,  6955,   136,  1212,  1242,  8249,   117,\n",
      "        24007,   102])\n",
      "entity_list: ['Unsupervised Pretraining']\n",
      "entity_token: [tensor([12118,  6385,  3365, 16641,  1181, 11689,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: many tasks, greedy layer-wise unsupervised pretraining can yield substantial improvements in test\n",
      "content_token: tensor([  101,  1242,  8249,   117, 24007,  6440,   118, 10228,  8362,  6385,\n",
      "         3365, 16641,  1181,  3073,  4487, 16534,  1169, 10972,  6432,  8313,\n",
      "         1107,  2774,   102])\n",
      "entity_list: ['greedy layer-wise unsupervised pretraining']\n",
      "entity_token: [tensor([24007,  6440,   118, 10228,  8362,  6385,  3365, 16641,  1181,  3073,\n",
      "         4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in test error for classification tasks. This observation was responsible for the renewed interested\n",
      "content_token: tensor([ 101, 1107, 2774, 7353, 1111, 5393, 8249,  119, 1188, 8310, 1108, 2784,\n",
      "        1111, 1103, 8978, 3888,  102])\n",
      "entity_list: ['classification tasks']\n",
      "entity_token: [tensor([5393, 8249])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: renewed interested in deep neural networks starting in 2006 (Hinton et al., 529 CHAPTER 15.\n",
      "content_token: tensor([  101,  8978,  3888,  1107,  1996, 18250,  6379,  2547,  1107,  1386,\n",
      "          113,  8790, 13124,  3084,  2393,   119,   117,  3882,  1580,  8203,\n",
      "         1405,   119,   102])\n",
      "entity_list: ['deep neural networks']\n",
      "entity_token: [tensor([ 1996, 18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 529 CHAPTER 15. REPRESENTATION LEARNING Algorithm 15.1 Greedy layer-wise unsupervised pretraining\n",
      "content_token: tensor([  101,  3882,  1580,  8203,  1405,   119,   155, 16668, 16941, 12649,\n",
      "        15681, 13821, 24805,   149, 12420,  2069, 27451, 11780,  2586, 18791,\n",
      "         7088,  1306,  1405,   119,   122,   144, 15825,  1183,  6440,   118,\n",
      "        10228,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,   102])\n",
      "entity_list: ['Greedy layer-wise unsupervised pretraining']\n",
      "entity_token: [tensor([  144, 15825,  1183,  6440,   118, 10228,  8362,  6385,  3365, 16641,\n",
      "         1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining protocol. Given the following: Unsupervised feature learning algorithm , which takes a\n",
      "content_token: tensor([  101,  3073,  4487, 16534, 11309,   119, 10470,  1103,  1378,   131,\n",
      "        12118,  6385,  3365, 16641,  1181,  2672,  3776,  9932,   117,  1134,\n",
      "         2274,   170,   102])\n",
      "entity_list: ['Unsupervised feature learning algorithm']\n",
      "entity_token: [tensor([12118,  6385,  3365, 16641,  1181,  2672,  3776,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: , which takes a L training set of examples and returns an encoder or feature function f. The raw\n",
      "content_token: tensor([  101,   117,  1134,  2274,   170,   149,  2013,  1383,  1104,  5136,\n",
      "         1105,  5166,  1126,  4035, 13775,  1197,  1137,  2672,  3053,   175,\n",
      "          119,  1109,  7158,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: function f. The raw input data is X, with one row per example and f(1)(X) is the output of the\n",
      "content_token: tensor([ 101, 3053,  175,  119, 1109, 7158, 7758, 2233, 1110,  161,  117, 1114,\n",
      "        1141, 5105, 1679, 1859, 1105,  175,  113,  122,  114,  113,  161,  114,\n",
      "        1110, 1103, 5964, 1104, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the output of the first stage encoder on X. In the case where fine-tuning is performed, we use a\n",
      "content_token: tensor([  101,  1103,  5964,  1104,  1103,  1148,  2016,  4035, 13775,  1197,\n",
      "         1113,   161,   119,  1130,  1103,  1692,  1187,  2503,   118, 19689,\n",
      "         1110,  1982,   117,  1195,  1329,   170,   102])\n",
      "entity_list: ['fine-tuning']\n",
      "entity_token: [tensor([ 2503,   118, 19689])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: performed, we use a learner which takes an initial function f, input examples X (and in the\n",
      "content_token: tensor([ 101, 1982,  117, 1195, 1329,  170, 3858, 1200, 1134, 2274, 1126, 3288,\n",
      "        3053,  175,  117, 7758, 5136,  161,  113, 1105, 1107, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: X (and in the supervised T fine-tuning case, associated targets Y ), and returns a tuned function.\n",
      "content_token: tensor([  101,   161,   113,  1105,  1107,  1103, 14199,   157,  2503,   118,\n",
      "        19689,  1692,   117,  2628,  7539,   162,   114,   117,  1105,  5166,\n",
      "          170, 17169,  3053,   119,   102])\n",
      "entity_list: ['fine-tuning']\n",
      "entity_token: [tensor([ 2503,   118, 19689])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a tuned function. The number of stages is m. f Identity function ← ˜ X = X for k = 1,...,m do f(k)\n",
      "content_token: tensor([  101,   170, 17169,  3053,   119,  1109,  1295,  1104,  5251,  1110,\n",
      "          182,   119,   175, 22855,  3053,   843,   100,   161,   134,   161,\n",
      "         1111,   180,   134,   122,   117,   119,   119,   119,   117,   182,\n",
      "         1202,   175,   113,   180,   114,   102])\n",
      "entity_list: [\"'Identity function'\"]\n",
      "entity_token: [tensor([  112, 22855,  3053,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: k = 1,...,m do f(k) = (X˜ ) L f f(k) f ← ◦ X˜ f(k)(X˜ ) ← end for if fine-tuning then f (f,X,Y ) ←\n",
      "content_token: tensor([  101,   180,   134,   122,   117,   119,   119,   119,   117,   182,\n",
      "         1202,   175,   113,   180,   114,   134,   113,   100,   114,   149,\n",
      "          175,   175,   113,   180,   114,   175,   843,   100,   100,   175,\n",
      "          113,   180,   114,   113,   100,   114,   843,  1322,  1111,  1191,\n",
      "         2503,   118, 19689,  1173,   175,   113,   175,   117,   161,   117,\n",
      "          162,   114,   843,   102])\n",
      "entity_list: [\"'fine-tuning'\"]\n",
      "entity_token: [tensor([  112,  2503,   118, 19689,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: then f (f,X,Y ) ← T end if Return f 2006; Bengio et al., 2007; Ranzato et al., 2007a). On many\n",
      "content_token: tensor([  101,  1173,   175,   113,   175,   117,   161,   117,   162,   114,\n",
      "          843,   157,  1322,  1191, 11121,   175,  1386,   132,  3096, 10712,\n",
      "         3084,  2393,   119,   117,  1384,   132, 16890, 12541,  2430,  3084,\n",
      "         2393,   119,   117,  1384,  1161,   114,   119,  1212,  1242,   102])\n",
      "entity_list: [\"'Bengio et al.'\", \"'Ranzato et al.'\"]\n",
      "entity_token: [tensor([  112,  3096, 10712,  3084,  2393,   119,   112]), tensor([  112, 16890, 12541,  2430,  3084,  2393,   119,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 2007a). On many other tasks, however, unsupervised pretraining either does not confer a benefit or\n",
      "content_token: tensor([  101,  1384,  1161,   114,   119,  1212,  1242,  1168,  8249,   117,\n",
      "         1649,   117,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,\n",
      "         1719,  1674,  1136, 14255,  6732,   170,  5257,  1137,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: confer a benefit or even causes noticeable harm. Ma et al. (2015) studied the effect of pretraining\n",
      "content_token: tensor([  101, 14255,  6732,   170,  5257,  1137,  1256,  4680, 19178,  7031,\n",
      "          119,  7085,  3084,  2393,   119,   113,  1410,   114,  2376,  1103,\n",
      "         2629,  1104,  3073,  4487, 16534,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of pretraining on machine learning models for chemical activity prediction and found that, on\n",
      "content_token: tensor([  101,  1104,  3073,  4487, 16534,  1113,  3395,  3776,  3584,  1111,\n",
      "         5297,  3246, 20770,  1105,  1276,  1115,   117,  1113,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and found that, on average, pretraining was slightlyharmful, but for manytasks\n",
      "content_token: tensor([  101,  1105,  1276,  1115,   117,  1113,  1903,   117,  3073,  4487,\n",
      "        16534,  1108,  2776,  7111,  1306,  2365,   117,  1133,  1111,  1242,\n",
      "        10401,  4616,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: but for manytasks wassignificantlyhelpful. Becauseunsupervised pretraining is sometimes helpful but\n",
      "content_token: tensor([  101,  1133,  1111,  1242, 10401,  4616,  1108, 19638, 18772,  1193,\n",
      "        18809, 20201,  4654,   119,  2279, 17096, 26939,  1197, 16641,  1181,\n",
      "         3073,  4487, 16534,  1110,  2121, 14739,  1133,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: helpful but often harmful it is important to understand when and why it works in order to determine\n",
      "content_token: tensor([  101, 14739,  1133,  1510, 19403,  1122,  1110,  1696,  1106,  2437,\n",
      "         1165,  1105,  1725,  1122,  1759,  1107,  1546,  1106,  4959,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: order to determine whether it is applicable to a particular task. At the outset, it is important to\n",
      "content_token: tensor([  101,  1546,  1106,  4959,  2480,  1122,  1110, 13036,  1106,   170,\n",
      "         2440,  4579,   119,  1335,  1103, 24046,  2105,   117,  1122,  1110,\n",
      "         1696,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it is important to clarify that most of this discussion is restricted to greedy unsupervised\n",
      "content_token: tensor([  101,  1122,  1110,  1696,  1106,   172,  5815,  6120,  1115,  1211,\n",
      "         1104,  1142,  6145,  1110,  7458,  1106, 24007,  8362,  6385,  3365,\n",
      "        16641,  1181,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: greedy unsupervised pretraining in particular. There are other, completely different paradigms for\n",
      "content_token: tensor([  101, 24007,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,\n",
      "         1107,  2440,   119,  1247,  1132,  1168,   117,  2423,  1472, 26213,\n",
      "         1116,  1111,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: paradigms for performing semi-supervised learning with neural networks, such as virtual adversarial\n",
      "content_token: tensor([  101, 26213,  1116,  1111,  4072,  3533,   118, 14199,  3776,  1114,\n",
      "        18250,  6379,   117,  1216,  1112,  8496,  8050, 10704, 11315,  1233,\n",
      "          102])\n",
      "entity_list: ['semi-supervised learning', 'neural networks', 'virtual adversarial']\n",
      "entity_token: [tensor([ 3533,   118, 14199,  3776]), tensor([18250,  6379]), tensor([ 8496,  8050, 10704, 11315,  1233])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 2, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: virtual adversarial training described in section 7.13. It is also possible to train an autoencoder\n",
      "content_token: tensor([  101,  8496,  8050, 10704, 11315,  1233,  2013,  1758,  1107,  2237,\n",
      "          128,   119,  1492,   119,  1135,  1110,  1145,  1936,  1106,  2669,\n",
      "         1126, 12365,  1424, 13775,  1197,   102])\n",
      "entity_list: ['virtual adversarial training', 'autoencoder']\n",
      "entity_token: [tensor([ 8496,  8050, 10704, 11315,  1233,  2013]), tensor([12365,  1424, 13775,  1197])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1,\n",
      "        1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: an autoencoder or generative model at the same time as the supervised model. Examples of this\n",
      "content_token: tensor([  101,  1126, 12365,  1424, 13775,  1197,  1137, 11974,  3946,  2235,\n",
      "         1120,  1103,  1269,  1159,  1112,  1103, 14199,  2235,   119, 10839,\n",
      "         1104,  1142,   102])\n",
      "entity_list: ['autoencoder', 'generative model', 'supervised model']\n",
      "entity_token: [tensor([12365,  1424, 13775,  1197]), tensor([11974,  3946,  2235]), tensor([14199,  2235])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Examples of this single-stage approach include the discriminative RBM (Larochelle and Bengio, 2008)\n",
      "content_token: tensor([  101, 10839,  1104,  1142,  1423,   118,  2016,  3136,  1511,  1103,\n",
      "         6187, 10205,  2983,  3946, 24718,  2107,   113,  2001,  2180, 20492,\n",
      "         1513,  1105,  3096, 10712,   117,  1369,   114,   102])\n",
      "entity_list: ['discriminative RBM']\n",
      "entity_token: [tensor([ 6187, 10205,  2983,  3946, 24718,  2107])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and Bengio, 2008) and the ladder network (Rasmus et al., 2015), in which the total objective is an\n",
      "content_token: tensor([  101,  1105,  3096, 10712,   117,  1369,   114,  1105,  1103, 11413,\n",
      "         2443,   113, 16890, 23283,  3084,  2393,   119,   117,  1410,   114,\n",
      "          117,  1107,  1134,  1103,  1703,  7649,  1110,  1126,   102])\n",
      "entity_list: ['ladder network']\n",
      "entity_token: [tensor([11413,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: objective is an explicit sum of the two terms (one using the labels and one only using the input).\n",
      "content_token: tensor([  101,  7649,  1110,  1126, 14077,  7584,  1104,  1103,  1160,  2538,\n",
      "          113,  1141,  1606,  1103, 11080,  1105,  1141,  1178,  1606,  1103,\n",
      "         7758,   114,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: using the input). Unsupervised pretraining combines two different ideas. First, it makes use of 530\n",
      "content_token: tensor([  101,  1606,  1103,  7758,   114,   119, 12118,  6385,  3365, 16641,\n",
      "         1181,  3073,  4487, 16534, 14215,  1160,  1472,  4133,   119,  1752,\n",
      "          117,  1122,  2228,  1329,  1104, 26260,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it makes use of 530 CHAPTER 15. REPRESENTATION LEARNING the idea that the choice of initial\n",
      "content_token: tensor([  101,  1122,  2228,  1329,  1104, 26260,  8203,  1405,   119,   155,\n",
      "        16668, 16941, 12649, 15681, 13821, 24805,   149, 12420,  2069, 27451,\n",
      "        11780,  1103,  1911,  1115,  1103,  3026,  1104,  3288,   102])\n",
      "entity_list: ['REPRESENTATION LEARNING']\n",
      "entity_token: [tensor([  155, 16668, 16941, 12649, 15681, 13821, 24805,   149, 12420,  2069,\n",
      "        27451, 11780])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: choice of initial parameters for a deep neural network can have a significant regularizing effect\n",
      "content_token: tensor([  101,  3026,  1104,  3288, 11934,  1111,   170,  1996, 18250,  2443,\n",
      "         1169,  1138,   170,  2418,  2366,  4404,  2629,   102])\n",
      "entity_list: ['deep neural network']\n",
      "entity_token: [tensor([ 1996, 18250,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regularizing effect on the model (and, to a lesser extent, that it can improve optimization).\n",
      "content_token: tensor([  101,  2366,  4404,  2629,  1113,  1103,  2235,   113,  1105,   117,\n",
      "         1106,   170,  9774,  6102,   117,  1115,  1122,  1169,  4607, 25161,\n",
      "          114,   119,   102])\n",
      "entity_list: ['model']\n",
      "entity_token: [tensor([2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: optimization). Second, it makes use of the more general idea that learning about the input\n",
      "content_token: tensor([  101, 25161,   114,   119,  2307,   117,  1122,  2228,  1329,  1104,\n",
      "         1103,  1167,  1704,  1911,  1115,  3776,  1164,  1103,  7758,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: about the input distribution can help to learn about the mapping from inputs to outputs. Both of\n",
      "content_token: tensor([  101,  1164,  1103,  7758,  3735,  1169,  1494,  1106,  3858,  1164,\n",
      "         1103, 13970,  1121, 22743,  1106,  5964,  1116,   119,  2695,  1104,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to outputs. Both of these ideas involve many complicated interactions between several parts of the\n",
      "content_token: tensor([  101,  1106,  5964,  1116,   119,  2695,  1104,  1292,  4133,  8803,\n",
      "         1242,  8277, 10393,  1206,  1317,  2192,  1104,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parts of the machine learning algorithm that are not entirely understood. The first idea, that the\n",
      "content_token: tensor([ 101, 2192, 1104, 1103, 3395, 3776, 9932, 1115, 1132, 1136, 3665, 4628,\n",
      "         119, 1109, 1148, 1911,  117, 1115, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: idea, that the choice of initial parameters for a deep neural network can have astrong regularizing\n",
      "content_token: tensor([  101,  1911,   117,  1115,  1103,  3026,  1104,  3288, 11934,  1111,\n",
      "          170,  1996, 18250,  2443,  1169,  1138,  1112, 19138,  1403,  2366,\n",
      "         4404,   102])\n",
      "entity_list: ['deep neural network']\n",
      "entity_token: [tensor([ 1996, 18250,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regularizing effect on its performance, is the least wellunderstood. At the time that pretraining\n",
      "content_token: tensor([  101,  2366,  4404,  2629,  1113,  1157,  2099,   117,  1110,  1103,\n",
      "         1655,  1218,  6775,  1468,  2430,  5412,   119,  1335,  1103,  1159,\n",
      "         1115,  3073,  4487, 16534,   102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that pretraining became popular, it was understood as initializing the model in a location that\n",
      "content_token: tensor([  101,  1115,  3073,  4487, 16534,  1245,  1927,   117,  1122,  1108,\n",
      "         4628,  1112,  3288,  4404,  1103,  2235,  1107,   170,  2450,  1115,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in a location that would cause it to approach one local minimum rather than another. Today, local\n",
      "content_token: tensor([ 101, 1107,  170, 2450, 1115, 1156, 2612, 1122, 1106, 3136, 1141, 1469,\n",
      "        5867, 1897, 1190, 1330,  119, 3570,  117, 1469,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Today, local minima are no longer considered to be a serious problem for neural network\n",
      "content_token: tensor([  101,  3570,   117,  1469,  8715,  1918,  1132,  1185,  2039,  1737,\n",
      "         1106,  1129,   170,  3021,  2463,  1111, 18250,  2443,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for neural network optimization. We now know that our standard neural network training procedures\n",
      "content_token: tensor([  101,  1111, 18250,  2443, 25161,   119,  1284,  1208,  1221,  1115,\n",
      "         1412,  2530, 18250,  2443,  2013,  8826,   102])\n",
      "entity_list: ['neural network optimization']\n",
      "entity_token: [tensor([18250,  2443, 25161])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: training procedures usually do not arrive at a critical point of any kind. It remains possible that\n",
      "content_token: tensor([ 101, 2013, 8826, 1932, 1202, 1136, 6657, 1120,  170, 3607, 1553, 1104,\n",
      "        1251, 1912,  119, 1135, 2606, 1936, 1115,  102])\n",
      "entity_list: ['training procedures\\n\\n\"Training procedures\" is an entity related to deep learning in this sentence.']\n",
      "entity_token: [tensor([2013, 8826,  107, 5513, 8826,  107, 1110, 1126, 9127, 2272, 1106, 1996,\n",
      "        3776, 1107, 1142, 5650,  119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: possible that pretraining initializes the model in a location that would otherwise be\n",
      "content_token: tensor([  101,  1936,  1115,  3073,  4487, 16534,  3288,  9534,  1103,  2235,\n",
      "         1107,   170,  2450,  1115,  1156,  4303,  1129,   102])\n",
      "entity_list: ['pretraining', 'model\\n\\nBoth \"pretraining\" and \"model\" are entities related to deep learning in this sentence.']\n",
      "entity_token: [tensor([ 3073,  4487, 16534]), tensor([ 2235,  2695,   107,  3073,  4487, 16534,   107,  1105,   107,  2235,\n",
      "          107,  1132, 11659,  2272,  1106,  1996,  3776,  1107,  1142,  5650,\n",
      "          119])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: would otherwise be inaccessible—for example, a region that is surrounded by areas where the cost\n",
      "content_token: tensor([  101,  1156,  4303,  1129,  1107,  7409, 22371,  5225,   783,  1111,\n",
      "         1859,   117,   170,  1805,  1115,  1110,  4405,  1118,  1877,  1187,\n",
      "         1103,  2616,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: where the cost function varies so much from one example to another that minibatches give only a\n",
      "content_token: tensor([  101,  1187,  1103,  2616,  3053,  9544,  1177,  1277,  1121,  1141,\n",
      "         1859,  1106,  1330,  1115,  8715, 14602,  7486,  1660,  1178,   170,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: give only a very noisy estimate of the gradient, or a region surrounded by areas where the Hessian\n",
      "content_token: tensor([  101,  1660,  1178,   170,  1304, 24678, 10301,  1104,  1103, 19848,\n",
      "          117,  1137,   170,  1805,  4405,  1118,  1877,  1187,  1103, 26349,\n",
      "         1811,   102])\n",
      "entity_list: ['minibatches\\n\\nIn the given input sentence', 'the entities \"gradient\" and \"minibatches\" are related to the field of deep learning.']\n",
      "entity_token: [tensor([ 8715, 14602,  7486,  1130,  1103,  1549,  7758,  5650]), tensor([ 1103, 11659,   107, 19848,   107,  1105,   107,  8715, 14602,  7486,\n",
      "          107,  1132,  2272,  1106,  1103,  1768,  1104,  1996,  3776,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: where the Hessian matrix is so poorly conditioned that gradient descent methods must use very small\n",
      "content_token: tensor([  101,  1187,  1103, 26349,  1811,  8952,  1110,  1177,  9874, 25592,\n",
      "         1115, 19848,  6585,  4069,  1538,  1329,  1304,  1353,   102])\n",
      "entity_list: ['gradient descent methods\\n\\nIn the given input sentence', 'the entities \"Hessian matrix\" and \"gradient descent methods\" are related to the field of deep learning.']\n",
      "entity_token: [tensor([19848,  6585,  4069,  1130,  1103,  1549,  7758,  5650]), tensor([ 1103, 11659,   107, 26349,  1811,  8952,   107,  1105,   107, 19848,\n",
      "         6585,  4069,   107,  1132,  2272,  1106,  1103,  1768,  1104,  1996,\n",
      "         3776,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: must use very small steps. However, our ability to characterize exactly what aspects of the\n",
      "content_token: tensor([ 101, 1538, 1329, 1304, 1353, 3343,  119, 1438,  117, 1412, 2912, 1106,\n",
      "        1959, 3708, 2839, 1184, 5402, 1104, 1103,  102])\n",
      "entity_list: ['gradient descent methods\\n\\nIn the given input sentence', 'the entity \"gradient descent methods\" is mentioned', 'which is related to the field of deep learning.']\n",
      "entity_token: [tensor([19848,  6585,  4069,  1130,  1103,  1549,  7758,  5650]), tensor([ 1103,  9127,   107, 19848,  6585,  4069,   107,  1110,  3025]), tensor([1134, 1110, 2272, 1106, 1103, 1768, 1104, 1996, 3776,  119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: what aspects of the pretrained parameters are retained during the supervised training stage is\n",
      "content_token: tensor([  101,  1184,  5402,  1104,  1103,  3073,  4487,  9044, 11934,  1132,\n",
      "         5366,  1219,  1103, 14199,  2013,  2016,  1110,   102])\n",
      "entity_list: ['pretrained parameters', 'supervised training stage\\n\\nIn the given input sentence', 'the entities \"pretrained parameters\" and \"supervised training stage\" are mentioned', 'which are related to the field of deep learning.']\n",
      "entity_token: [tensor([ 3073,  4487,  9044, 11934]), tensor([14199,  2013,  2016,  1130,  1103,  1549,  7758,  5650]), tensor([ 1103, 11659,   107,  3073,  4487,  9044, 11934,   107,  1105,   107,\n",
      "        14199,  2013,  2016,   107,  1132,  3025]), tensor([1134, 1132, 2272, 1106, 1103, 1768, 1104, 1996, 3776,  119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: training stage is limited. This is one reason that modern approaches typically use simultaneous\n",
      "content_token: tensor([  101,  2013,  2016,  1110,  2609,   119,  1188,  1110,  1141,  2255,\n",
      "         1115,  2030,  8015,  3417,  1329, 19648,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: use simultaneous unsupervised learning and supervised learning rather than two sequential stages.\n",
      "content_token: tensor([  101,  1329, 19648,  8362,  6385,  3365, 16641,  1181,  3776,  1105,\n",
      "        14199,  3776,  1897,  1190,  1160, 14516, 21967,  5251,   119,   102])\n",
      "entity_list: ['unsupervised learning', 'supervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776]), tensor([14199,  3776])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sequential stages. One may also avoid struggling with these complicated ideas about how\n",
      "content_token: tensor([  101, 14516, 21967,  5251,   119,  1448,  1336,  1145,  3644,  7851,\n",
      "         1114,  1292,  8277,  4133,  1164,  1293,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ideas about how optimization in the supervised learning stage preserves information from the\n",
      "content_token: tensor([  101,  4133,  1164,  1293, 25161,  1107,  1103, 14199,  3776,  2016,\n",
      "        20421,  1869,  1121,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from the unsupervised learning stage by simply freezing the parameters for the feature extractors\n",
      "content_token: tensor([  101,  1121,  1103,  8362,  6385,  3365, 16641,  1181,  3776,  2016,\n",
      "         1118,  2566, 13543,  1103, 11934,  1111,  1103,  2672, 16143,  3864,\n",
      "          102])\n",
      "entity_list: ['feature extractors']\n",
      "entity_token: [tensor([ 2672, 16143,  3864])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: feature extractors and using supervised learning only to add a classifier on top of the learned\n",
      "content_token: tensor([  101,  2672, 16143,  3864,  1105,  1606, 14199,  3776,  1178,  1106,\n",
      "         5194,   170,  1705, 17792,  1113,  1499,  1104,  1103,  3560,   102])\n",
      "entity_list: ['feature extractors', 'supervised learning']\n",
      "entity_token: [tensor([ 2672, 16143,  3864]), tensor([14199,  3776])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: top of the learned features. The other idea, that a learning algorithm can use information learned\n",
      "content_token: tensor([ 101, 1499, 1104, 1103, 3560, 1956,  119, 1109, 1168, 1911,  117, 1115,\n",
      "         170, 3776, 9932, 1169, 1329, 1869, 3560,  102])\n",
      "entity_list: ['learning algorithm']\n",
      "entity_token: [tensor([3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: information learned in the unsupervised phase to perform better in the supervised learning stage,\n",
      "content_token: tensor([  101,  1869,  3560,  1107,  1103,  8362,  6385,  3365, 16641,  1181,\n",
      "         4065,  1106,  3870,  1618,  1107,  1103, 14199,  3776,  2016,   117,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning stage, is better understood. Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised\n",
      "content_token: tensor([  101,  3776,  2016,   117,  1110,  1618,  4628,   119,  1109, 16531,\n",
      "        24421, 15837,  7702,  2145,  6758,  8124, 17337,  2050, 11220,  8836,\n",
      "         5613,  2365, 15396, 14272,  2316, 26939,  1197, 16641,  1181,   102])\n",
      "entity_list: ['unsupervised']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: task may also be useful for the supervised learning task. For example, if we train a generative\n",
      "content_token: tensor([  101,  4579,  1336,  1145,  1129,  5616,  1111,  1103, 14199,  3776,\n",
      "         4579,   119,  1370,  1859,   117,  1191,  1195,  2669,   170, 11974,\n",
      "         3946,   102])\n",
      "entity_list: ['supervised learning task']\n",
      "entity_token: [tensor([14199,  3776,  4579])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: train a generative model of images of cars and motorcycles, it will need to know about wheels, and\n",
      "content_token: tensor([  101,  2669,   170, 11974,  3946,  2235,  1104,  4351,  1104,  3079,\n",
      "         1105, 21552,   117,  1122,  1209,  1444,  1106,  1221,  1164,  8089,\n",
      "          117,  1105,   102])\n",
      "entity_list: ['generative model']\n",
      "entity_token: [tensor([11974,  3946,  2235])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: about wheels, and about how many wheels should be in an image. If we are fortunate, the\n",
      "content_token: tensor([  101,  1164,  8089,   117,  1105,  1164,  1293,  1242,  8089,  1431,\n",
      "         1129,  1107,  1126,  3077,   119,  1409,  1195,  1132, 22649,   117,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are fortunate, the representation of the wheels will take on a form that is easy for the supervised\n",
      "content_token: tensor([  101,  1132, 22649,   117,  1103,  6368,  1104,  1103,  8089,  1209,\n",
      "         1321,  1113,   170,  1532,  1115,  1110,  3123,  1111,  1103, 14199,\n",
      "          102])\n",
      "entity_list: ['representation', 'supervised']\n",
      "entity_token: [tensor([6368]), tensor([14199])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for the supervised learner to access. This is not yet understood at a mathematical, theoretical\n",
      "content_token: tensor([  101,  1111,  1103, 14199,  3858,  1200,  1106,  2469,   119,  1188,\n",
      "         1110,  1136,  1870,  4628,  1120,   170,  9988,   117, 10093,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: theoretical level, so it is not always possible to predict which tasks will benefit from\n",
      "content_token: tensor([  101, 10093,  1634,   117,  1177,  1122,  1110,  1136,  1579,  1936,\n",
      "         1106, 17163,  1134,  8249,  1209,  5257,  1121,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: will benefit from unsupervised learning in this way. Many aspects of this approach are highly\n",
      "content_token: tensor([  101,  1209,  5257,  1121,  8362,  6385,  3365, 16641,  1181,  3776,\n",
      "         1107,  1142,  1236,   119,  2408,  5402,  1104,  1142,  3136,  1132,\n",
      "         3023,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: approach are highly dependent on the specific models used. For example, if we wish to add a linear\n",
      "content_token: tensor([ 101, 3136, 1132, 3023, 7449, 1113, 1103, 2747, 3584, 1215,  119, 1370,\n",
      "        1859,  117, 1191, 1195, 3683, 1106, 5194,  170, 7378,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to add a linear classifier on 531 CHAPTER 15. REPRESENTATION LEARNING top of pretrained features,\n",
      "content_token: tensor([  101,  1106,  5194,   170,  7378,  1705, 17792,  1113,  4389,  1475,\n",
      "         8203,  1405,   119,   155, 16668, 16941, 12649, 15681, 13821, 24805,\n",
      "          149, 12420,  2069, 27451, 11780,  1499,  1104,  3073,  4487,  9044,\n",
      "         1956,   117,   102])\n",
      "entity_list: ['pretrained features']\n",
      "entity_token: [tensor([3073, 4487, 9044, 1956])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 2, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: features, the features must make the underlying classes linearly separable. These properties often\n",
      "content_token: tensor([  101,  1956,   117,  1103,  1956,  1538,  1294,  1103, 10311,  3553,\n",
      "         7378,  1193, 14516, 17482,  1895,   119,  1636,  4625,  1510,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: properties often occur naturally but do not always do so. This is another reason that simultaneous\n",
      "content_token: tensor([  101,  4625,  1510,  4467,  8534,  1133,  1202,  1136,  1579,  1202,\n",
      "         1177,   119,  1188,  1110,  1330,  2255,  1115, 19648,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that simultaneous supervised and unsupervised learning can be preferable—the constraints imposed by\n",
      "content_token: tensor([  101,  1115, 19648, 14199,  1105,  8362,  6385,  3365, 16641,  1181,\n",
      "         3776,  1169,  1129,  9353,  1895,   783,  1103, 15651,  9520,  1118,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: imposed by the output layer are naturally included from the start. From the point of view of\n",
      "content_token: tensor([ 101, 9520, 1118, 1103, 5964, 6440, 1132, 8534, 1529, 1121, 1103, 1838,\n",
      "         119, 1622, 1103, 1553, 1104, 2458, 1104,  102])\n",
      "entity_list: ['output layer']\n",
      "entity_token: [tensor([5964, 6440])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: point of view of unsupervised pretraining as learning a representation, we can expect unsupervised\n",
      "content_token: tensor([  101,  1553,  1104,  2458,  1104,  8362,  6385,  3365, 16641,  1181,\n",
      "         3073,  4487, 16534,  1112,  3776,   170,  6368,   117,  1195,  1169,\n",
      "         5363,  8362,  6385,  3365, 16641,  1181,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: expect unsupervised pretraining to be more effective when the initial representation is poor. One\n",
      "content_token: tensor([  101,  5363,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,\n",
      "         1106,  1129,  1167,  3903,  1165,  1103,  3288,  6368,  1110,  2869,\n",
      "          119,  1448,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is poor. One key example of this is the use of word embeddings. Words represented by one-hot\n",
      "content_token: tensor([  101,  1110,  2869,   119,  1448,  2501,  1859,  1104,  1142,  1110,\n",
      "         1103,  1329,  1104,  1937,  9712,  4774,  3408,  1116,   119, 13212,\n",
      "         2533,  1118,  1141,   118,  2633,   102])\n",
      "entity_list: ['word embeddings']\n",
      "entity_token: [tensor([1937, 9712, 4774, 3408, 1116])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by one-hot vectors are not very informative because every two distinct one-hot vectors are the same\n",
      "content_token: tensor([  101,  1118,  1141,   118,  2633, 21118,  1132,  1136,  1304, 12862,\n",
      "         5838,  1272,  1451,  1160,  4966,  1141,   118,  2633, 21118,  1132,\n",
      "         1103,  1269,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are the same distance from each other (squared L2 distance of 2). Learned word embeddings naturally\n",
      "content_token: tensor([  101,  1132,  1103,  1269,  2462,  1121,  1296,  1168,   113, 23215,\n",
      "          149,  1477,  2462,  1104,   123,   114,   119, 12958, 23537,  1937,\n",
      "         9712,  4774,  3408,  1116,  8534,   102])\n",
      "entity_list: ['word embeddings']\n",
      "entity_token: [tensor([1937, 9712, 4774, 3408, 1116])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: naturally encode similarity between words by their distance from each other. Because of this,\n",
      "content_token: tensor([  101,  8534,  4035, 13775, 15213,  1206,  1734,  1118,  1147,  2462,\n",
      "         1121,  1296,  1168,   119,  2279,  1104,  1142,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Because of this, unsupervised pretraining is especially useful when processing words. It is less\n",
      "content_token: tensor([  101,  2279,  1104,  1142,   117,  8362,  6385,  3365, 16641,  1181,\n",
      "         3073,  4487, 16534,  1110,  2108,  5616,  1165,  6165,  1734,   119,\n",
      "         1135,  1110,  1750,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: words. It is less useful when processing images, perhaps because images already lie in a rich\n",
      "content_token: tensor([ 101, 1734,  119, 1135, 1110, 1750, 5616, 1165, 6165, 4351,  117, 3229,\n",
      "        1272, 4351, 1640, 4277, 1107,  170, 3987,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: lie in a rich vector space where distances provide a low quality similarity metric. From the point\n",
      "content_token: tensor([  101,  4277,  1107,   170,  3987,  9479,  2000,  1187, 12424,  2194,\n",
      "          170,  1822,  3068, 15213, 12676,   119,  1622,  1103,  1553,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: From the point of view of unsupervised pretraining as a regularizer, we can expect unsupervised\n",
      "content_token: tensor([  101,  1622,  1103,  1553,  1104,  2458,  1104,  8362,  6385,  3365,\n",
      "        16641,  1181,  3073,  4487, 16534,  1112,   170,  2366, 17260,   117,\n",
      "         1195,  1169,  5363,  8362,  6385,  3365, 16641,  1181,   102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: expect unsupervised pretraining to be most helpful when the number of labeled examples is very\n",
      "content_token: tensor([  101,  5363,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,\n",
      "         1106,  1129,  1211, 14739,  1165,  1103,  1295,  1104, 12893,  5136,\n",
      "         1110,  1304,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: examples is very small. Because the source of information added by unsupervised pretraining is the\n",
      "content_token: tensor([  101,  5136,  1110,  1304,  1353,   119,  2279,  1103,  2674,  1104,\n",
      "         1869,  1896,  1118,  8362,  6385,  3365, 16641,  1181,  3073,  4487,\n",
      "        16534,  1110,  1103,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining is the unlabeled data, we may also expect unsupervised pretraining to perform best when\n",
      "content_token: tensor([  101,  3073,  4487, 16534,  1110,  1103,  8362,  1742,  8511,  1174,\n",
      "         2233,   117,  1195,  1336,  1145,  5363,  8362,  6385,  3365, 16641,\n",
      "         1181,  3073,  4487, 16534,  1106,  3870,  1436,  1165,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: perform best when the number of unlabeled examples is very large. The advantage of semi-supervised\n",
      "content_token: tensor([  101,  3870,  1436,  1165,  1103,  1295,  1104,  8362,  1742,  8511,\n",
      "         1174,  5136,  1110,  1304,  1415,   119,  1109,  4316,  1104,  3533,\n",
      "          118, 14199,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of semi-supervised learning via unsupervised pretraining with many unlabeled examples and few\n",
      "content_token: tensor([  101,  1104,  3533,   118, 14199,  3776,  2258,  8362,  6385,  3365,\n",
      "        16641,  1181,  3073,  4487, 16534,  1114,  1242,  8362,  1742,  8511,\n",
      "         1174,  5136,  1105,  1374,   102])\n",
      "entity_list: ['semi-supervised learning', 'unsupervised pretraining']\n",
      "entity_token: [tensor([ 3533,   118, 14199,  3776]), tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: examples and few labeled examples was made particularly clear in 2011 with unsupervised pretraining\n",
      "content_token: tensor([  101,  5136,  1105,  1374, 12893,  5136,  1108,  1189,  2521,  2330,\n",
      "         1107,  1349,  1114,  8362,  6385,  3365, 16641,  1181,  3073,  4487,\n",
      "        16534,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining winning two international transfer learning competitions (Mesnil et al., 2011;\n",
      "content_token: tensor([  101,  3073,  4487, 16534,  2183,  1160,  1835,  4036,  3776,  6025,\n",
      "          113,  2508,  1116,  2605,  1233,  3084,  2393,   119,   117,  1349,\n",
      "          132,   102])\n",
      "entity_list: ['pretraining', 'transfer learning']\n",
      "entity_token: [tensor([ 3073,  4487, 16534]), tensor([4036, 3776])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: et al., 2011; Goodfellow et al., 2011), in settings where the number of labeled examples in the\n",
      "content_token: tensor([  101,  3084,  2393,   119,   117,  1349,   132,  2750, 27610,  4064,\n",
      "         3084,  2393,   119,   117,  1349,   114,   117,  1107, 11106,  1187,\n",
      "         1103,  1295,  1104, 12893,  5136,  1107,  1103,   102])\n",
      "entity_list: ['transfer learning']\n",
      "entity_token: [tensor([4036, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: examples in the target task was small (from a handful to dozens of examples per class). These\n",
      "content_token: tensor([  101,  5136,  1107,  1103,  4010,  4579,  1108,  1353,   113,  1121,\n",
      "          170,  8973,  1106, 10366,  1104,  5136,  1679,  1705,   114,   119,\n",
      "         1636,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: per class). These effects were also documented in carefully controlled experiments by Paine et al.\n",
      "content_token: tensor([  101,  1679,  1705,   114,   119,  1636,  3154,  1127,  1145,  8510,\n",
      "         1107,  4727,  4013,  7857,  1118, 13304,  1162,  3084,  2393,   119,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by Paine et al. (2014). Other factors are likely to be involved. For example, unsupervised\n",
      "content_token: tensor([  101,  1118, 13304,  1162,  3084,  2393,   119,   113,  1387,   114,\n",
      "          119,  2189,  5320,  1132,  2620,  1106,  1129,  2017,   119,  1370,\n",
      "         1859,   117,  8362,  6385,  3365, 16641,  1181,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: unsupervised pretraining is likely to be most useful when the function to be learned is extremely\n",
      "content_token: tensor([  101,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,  1110,\n",
      "         2620,  1106,  1129,  1211,  5616,  1165,  1103,  3053,  1106,  1129,\n",
      "         3560,  1110,  4450,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is extremely complicated. Unsupervised learningdiffers from regularizers like weight decaybecause\n",
      "content_token: tensor([  101,  1110,  4450,  8277,   119, 12118,  6385,  3365, 16641,  1181,\n",
      "         3776,  3309, 12929,  1116,  1121,  2366, 17260,  1116,  1176,  2841,\n",
      "        14352,  3962,  2599,  5613,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: weight decaybecause it does not bias the learner toward discovering a simple function but rather\n",
      "content_token: tensor([  101,  2841, 14352,  3962,  2599,  5613,  1122,  1674,  1136, 15069,\n",
      "         1103,  3858,  1200,  1755, 15137,   170,  3014,  3053,  1133,  1897,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: function but rather toward discovering feature functions that are useful for the unsupervised\n",
      "content_token: tensor([  101,  3053,  1133,  1897,  1755, 15137,  2672,  4226,  1115,  1132,\n",
      "         5616,  1111,  1103,  8362,  6385,  3365, 16641,  1181,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the unsupervised learning task. If the true underlying functions are complicated and shaped by\n",
      "content_token: tensor([  101,  1103,  8362,  6385,  3365, 16641,  1181,  3776,  4579,   119,\n",
      "         1409,  1103,  2276, 10311,  4226,  1132,  8277,  1105,  4283,  1118,\n",
      "          102])\n",
      "entity_list: ['unsupervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and shaped by regularities of the input distribution, unsupervised learning can be a more\n",
      "content_token: tensor([  101,  1105,  4283,  1118,  2366,  4233,  1104,  1103,  7758,  3735,\n",
      "          117,  8362,  6385,  3365, 16641,  1181,  3776,  1169,  1129,   170,\n",
      "         1167,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can be a more appropriate regularizer. These caveats aside, we now analyze some success cases where\n",
      "content_token: tensor([  101,  1169,  1129,   170,  1167,  5806,  2366, 17260,   119,  1636,\n",
      "         5812,  9971,  4783,   117,  1195,  1208, 19774,  1199,  2244,  2740,\n",
      "         1187,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: success cases where unsupervised pretraining is known to cause an improvement, and explain what is\n",
      "content_token: tensor([  101,  2244,  2740,  1187,  8362,  6385,  3365, 16641,  1181,  3073,\n",
      "         4487, 16534,  1110,  1227,  1106,  2612,  1126,  8331,   117,  1105,\n",
      "         4137,  1184,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and explain what is known about why this improvement occurs. Unsupervised pretraining has usually\n",
      "content_token: tensor([  101,  1105,  4137,  1184,  1110,  1227,  1164,  1725,  1142,  8331,\n",
      "         4365,   119, 12118,  6385,  3365, 16641,  1181,  3073,  4487, 16534,\n",
      "         1144,  1932,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: has usually been used to improve classifiers, and is usually most interesting from the point of\n",
      "content_token: tensor([  101,  1144,  1932,  1151,  1215,  1106,  4607,  1705, 17792,  1116,\n",
      "          117,  1105,  1110,  1932,  1211,  5426,  1121,  1103,  1553,  1104,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from the point of view of 532 CHAPTER 15. REPRESENTATION LEARNING   \n",
      "content_token: tensor([  101,  1121,  1103,  1553,  1104,  2458,  1104,  4389,  1477,  8203,\n",
      "         1405,   119,   155, 16668, 16941, 12649, 15681, 13821, 24805,   149,\n",
      "        12420,  2069, 27451, 11780,   102])\n",
      "entity_list: ['REPRESENTATION LEARNING']\n",
      "entity_token: [tensor([  155, 16668, 16941, 12649, 15681, 13821, 24805,   149, 12420,  2069,\n",
      "        27451, 11780])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content:     \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000     \n",
      "content_token: tensor([101, 102])\n",
      "entity_list: ['REPRESENTATION LEARNING']\n",
      "entity_token: [tensor([  155, 16668, 16941, 12649, 15681, 13821, 24805,   149, 12420,  2069,\n",
      "        27451, 11780])]\n",
      "label: tensor([0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content:     Figure 15.1: Visualization via nonlinear projection of the learning\n",
      "content_token: tensor([  101, 15982,  1405,   119,   122,   131, 12071,  2734,  2258,  1664,\n",
      "        24984, 15178,  1104,  1103,  3776,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the learning trajectories of different neural networks in function space (not parameter space,\n",
      "content_token: tensor([  101,  1104,  1103,  3776,   189, 18663, 20302,  1905,  1104,  1472,\n",
      "        18250,  6379,  1107,  3053,  2000,   113,  1136, 17816,  2000,   117,\n",
      "          102])\n",
      "entity_list: ['neural networks']\n",
      "entity_token: [tensor([18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parameter space, to avoid the issue of many-to-one mappings from parameter vectors to functions),\n",
      "content_token: tensor([  101, 17816,  2000,   117,  1106,  3644,  1103,  2486,  1104,  1242,\n",
      "          118,  1106,   118,  1141, 13970,  1116,  1121, 17816, 21118,  1106,\n",
      "         4226,   114,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to functions), with different random initializations and with or without unsupervised pretraining.\n",
      "content_token: tensor([  101,  1106,  4226,   114,   117,  1114,  1472,  7091,  3288, 20412,\n",
      "         1105,  1114,  1137,  1443,  8362,  6385,  3365, 16641,  1181,  3073,\n",
      "         4487, 16534,   119,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining. Each point corresponds to a different neural network, at a particular time during its\n",
      "content_token: tensor([  101,  3073,  4487, 16534,   119,  2994,  1553, 15497,  1106,   170,\n",
      "         1472, 18250,  2443,   117,  1120,   170,  2440,  1159,  1219,  1157,\n",
      "          102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: time during its training process. This figure is adapted with permission from Erhan et al. (2010).\n",
      "content_token: tensor([ 101, 1159, 1219, 1157, 2013, 1965,  119, 1188, 2482, 1110, 5546, 1114,\n",
      "        6156, 1121,  142, 1197, 3822, 3084, 2393,  119,  113, 1333,  114,  119,\n",
      "         102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: et al. (2010). A coordinate in function space is an infinite- dimensional vector associating every\n",
      "content_token: tensor([  101,  3084,  2393,   119,   113,  1333,   114,   119,   138, 14137,\n",
      "         1107,  3053,  2000,  1110,  1126, 13157,   118,  8611,  9479,  3919,\n",
      "        13335, 25148,  1451,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: associating every input x with an output y. Erhan et al. (2010) made a linear projection to\n",
      "content_token: tensor([  101,  3919, 13335, 25148,  1451,  7758,   193,  1114,  1126,  5964,\n",
      "          194,   119,   142,  1197,  3822,  3084,  2393,   119,   113,  1333,\n",
      "          114,  1189,   170,  7378, 15178,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: projection to high-dimensional space by concatenating the y for many specific x points. They then\n",
      "content_token: tensor([  101, 15178,  1106,  1344,   118,  8611,  2000,  1118, 14255, 20127,\n",
      "        26434,  1103,   194,  1111,  1242,  2747,   193,  1827,   119,  1220,\n",
      "         1173,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: x points. They then made a further nonlinear projection to 2-D by Isomap (Tenenbaum et al., 2000).\n",
      "content_token: tensor([  101,   193,  1827,   119,  1220,  1173,  1189,   170,  1748,  1664,\n",
      "        24984, 15178,  1106,   123,   118,   141,  1118,  2181,  7903,  1643,\n",
      "          113,  5157,  1424, 14318,  3084,  2393,   119,   117,  1539,   114,\n",
      "          119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: et al., 2000). Color indicates time. All networks are initialized near the center of the plot\n",
      "content_token: tensor([  101,  3084,  2393,   119,   117,  1539,   114,   119, 13066,  6653,\n",
      "         1159,   119,  1398,  6379,  1132,  3288,  2200,  1485,  1103,  2057,\n",
      "         1104,  1103,  4928,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: center of the plot (correspondingto theregion offunctions thatproduce approximatelyuniform\n",
      "content_token: tensor([  101,  2057,  1104,  1103,  4928,   113,  7671,  2430,  1175, 16680,\n",
      "         1228,  3488, 13945,  1115,  1643, 13225, 15776,  2324, 19782, 13199,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distributions over the class y for most inputs). Over time, learning moves the function outward, to\n",
      "content_token: tensor([  101, 23190,  1166,  1103,  1705,   194,  1111,  1211, 22743,   114,\n",
      "          119,  3278,  1159,   117,  3776,  5279,  1103,  3053, 16923,   117,\n",
      "         1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: outward, to points that make strong predictions. Training consistently terminates in one region\n",
      "content_token: tensor([  101, 16923,   117,  1106,  1827,  1115,  1294,  2012, 23770,   119,\n",
      "         5513, 10887, 22516,  1116,  1107,  1141,  1805,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in one region when using pretraining and in another, non-overlapping region when not using\n",
      "content_token: tensor([  101,  1107,  1141,  1805,  1165,  1606,  3073,  4487, 16534,  1105,\n",
      "         1107,  1330,   117,  1664,   118, 23003,  1805,  1165,  1136,  1606,\n",
      "          102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: when not using pretraining. Isomap tries to preserve global relative distances (and hence volumes)\n",
      "content_token: tensor([  101,  1165,  1136,  1606,  3073,  4487, 16534,   119,  2181,  7903,\n",
      "         1643,  4642,  1106,  8333,  4265,  5236, 12424,   113,  1105,  7544,\n",
      "         6357,   114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (and hence volumes) so the small region corresponding to pretrained models may indicate that the\n",
      "content_token: tensor([ 101,  113, 1105, 7544, 6357,  114, 1177, 1103, 1353, 1805, 7671, 1106,\n",
      "        3073, 4487, 9044, 3584, 1336, 5057, 1115, 1103,  102])\n",
      "entity_list: ['pretrained models']\n",
      "entity_token: [tensor([3073, 4487, 9044, 3584])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: indicate that the pretraining-based estimator has reduced variance. 533 CHAPTER 15. REPRESENTATION\n",
      "content_token: tensor([  101,  5057,  1115,  1103,  3073,  4487, 16534,   118,  1359, 12890,\n",
      "        23021,  1144,  3549, 26717,   119,  4389,  1495,  8203,  1405,   119,\n",
      "          155, 16668, 16941, 12649, 15681, 13821, 24805,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 15. REPRESENTATION LEARNING reducing test set error. However, unsupervised pretraining can help\n",
      "content_token: tensor([  101,  1405,   119,   155, 16668, 16941, 12649, 15681, 13821, 24805,\n",
      "          149, 12420,  2069, 27451, 11780,  7914,  2774,  1383,  7353,   119,\n",
      "         1438,   117,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,\n",
      "         1169,  1494,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can help tasks other than classification, and can act to improve optimization rather than being\n",
      "content_token: tensor([  101,  1169,  1494,  8249,  1168,  1190,  5393,   117,  1105,  1169,\n",
      "         2496,  1106,  4607, 25161,  1897,  1190,  1217,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: rather than being merely a regularizer. For example, it can improve both train and test\n",
      "content_token: tensor([  101,  1897,  1190,  1217,  5804,   170,  2366, 17260,   119,  1370,\n",
      "         1859,   117,  1122,  1169,  4607,  1241,  2669,  1105,  2774,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: both train and test reconstruction error for deep autoencoders (Hinton and Salakhutdinov, 2006).\n",
      "content_token: tensor([  101,  1241,  2669,  1105,  2774, 10442,  7353,  1111,  1996, 12365,\n",
      "         1424, 13775,  1733,   113,  8790, 13124,  1105, 18613,  3715,  6583,\n",
      "         1204,  7126,  3292,   117,  1386,   114,   119,   102])\n",
      "entity_list: ['deep autoencoders']\n",
      "entity_token: [tensor([ 1996, 12365,  1424, 13775,  1733])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 2006). Erhan et al. (2010) performed many experiments to explain several successes of unsupervised\n",
      "content_token: tensor([  101,  1386,   114,   119,   142,  1197,  3822,  3084,  2393,   119,\n",
      "          113,  1333,   114,  1982,  1242,  7857,  1106,  4137,  1317, 14792,\n",
      "         1104,  8362,  6385,  3365, 16641,  1181,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of unsupervised pretraining. Both improvements to training error and improvements to test error may\n",
      "content_token: tensor([  101,  1104,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,\n",
      "          119,  2695,  8313,  1106,  2013,  7353,  1105,  8313,  1106,  2774,\n",
      "         7353,  1336,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to test error may be explained in terms of unsupervised pretraining taking the parameters into a\n",
      "content_token: tensor([  101,  1106,  2774,  7353,  1336,  1129,  3716,  1107,  2538,  1104,\n",
      "         8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,  1781,  1103,\n",
      "        11934,  1154,   170,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parameters into a region that would otherwise be inaccessible. Neural network training is\n",
      "content_token: tensor([  101, 11934,  1154,   170,  1805,  1115,  1156,  4303,  1129,  1107,\n",
      "         7409, 22371,  5225,   119,   151,  8816,  1348,  2443,  2013,  1110,\n",
      "          102])\n",
      "entity_list: ['neural network training']\n",
      "entity_token: [tensor([18250,  2443,  2013])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: network training is non-deterministic, and converges to a different function every time it is run.\n",
      "content_token: tensor([  101,  2443,  2013,  1110,  1664,   118,  1260,  2083, 25685,  5668,\n",
      "          117,  1105, 14255,  4121,  7562,  1106,   170,  1472,  3053,  1451,\n",
      "         1159,  1122,  1110,  1576,   119,   102])\n",
      "entity_list: ['network training']\n",
      "entity_token: [tensor([2443, 2013])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: time it is run. Training may halt at a point where the gradient becomes small, a point where early\n",
      "content_token: tensor([  101,  1159,  1122,  1110,  1576,   119,  5513,  1336,  9700,  1120,\n",
      "          170,  1553,  1187,  1103, 19848,  3316,  1353,   117,   170,  1553,\n",
      "         1187,  1346,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a point where early stopping ends training to prevent overfitting, or at a point where the gradient\n",
      "content_token: tensor([  101,   170,  1553,  1187,  1346,  7202,  3769,  2013,  1106,  3843,\n",
      "         1166, 14067,  1916,   117,  1137,  1120,   170,  1553,  1187,  1103,\n",
      "        19848,   102])\n",
      "entity_list: ['early stopping', 'overfitting']\n",
      "entity_token: [tensor([1346, 7202]), tensor([ 1166, 14067,  1916])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: where the gradient is large but it is difficult to find a downhill step due to problems such as\n",
      "content_token: tensor([  101,  1187,  1103, 19848,  1110,  1415,  1133,  1122,  1110,  2846,\n",
      "         1106,  1525,   170, 23895,  2585,  1496,  1106,  2645,  1216,  1112,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to problems such as stochasticity or poor conditioning of the Hessian. Neural networks that receive\n",
      "content_token: tensor([  101,  1106,  2645,  1216,  1112,   188,  2430,  7147,  5668,  1785,\n",
      "         1137,  2869, 15851,  1104,  1103, 26349,  1811,   119,   151,  8816,\n",
      "         1348,  6379,  1115,  3531,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that receive unsupervised pretraining consistently halt in the same region of function space, while\n",
      "content_token: tensor([  101,  1115,  3531,  8362,  6385,  3365, 16641,  1181,  3073,  4487,\n",
      "        16534, 10887,  9700,  1107,  1103,  1269,  1805,  1104,  3053,  2000,\n",
      "          117,  1229,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: space, while neural networks without pretraining consistently halt in another region. See figure\n",
      "content_token: tensor([  101,  2000,   117,  1229, 18250,  6379,  1443,  3073,  4487, 16534,\n",
      "        10887,  9700,  1107,  1330,  1805,   119,  3969,  2482,   102])\n",
      "entity_list: ['neural networks', 'pretraining']\n",
      "entity_token: [tensor([18250,  6379]), tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: region. See figure 15.1 for a visualization of this phenomenon. The region where pretrained\n",
      "content_token: tensor([ 101, 1805,  119, 3969, 2482, 1405,  119,  122, 1111,  170, 5173, 2734,\n",
      "        1104, 1142, 9501,  119, 1109, 1805, 1187, 3073, 4487, 9044,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: where pretrained networks arrive is smaller, suggesting that pretraining reduces the variance of\n",
      "content_token: tensor([  101,  1187,  3073,  4487,  9044,  6379,  6657,  1110,  2964,   117,\n",
      "         8783,  1115,  3073,  4487, 16534, 13822,  1103, 26717,  1104,   102])\n",
      "entity_list: ['pretrained networks']\n",
      "entity_token: [tensor([3073, 4487, 9044, 6379])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the variance of the estimation process, which can in turn reduce the risk of severe over-fitting.\n",
      "content_token: tensor([  101,  1103, 26717,  1104,  1103, 12890, 21517,  1965,   117,  1134,\n",
      "         1169,  1107,  1885,  4851,  1103,  3187,  1104,  5199,  1166,   118,\n",
      "        11732,   119,   102])\n",
      "entity_list: ['over-fitting']\n",
      "entity_token: [tensor([ 1166,   118, 11732])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: over-fitting. In other words, unsupervised pretraining initializes neural network parameters into a\n",
      "content_token: tensor([  101,  1166,   118, 11732,   119,  1130,  1168,  1734,   117,  8362,\n",
      "         6385,  3365, 16641,  1181,  3073,  4487, 16534,  3288,  9534, 18250,\n",
      "         2443, 11934,  1154,   170,   102])\n",
      "entity_list: ['over-fitting', 'unsupervised pretraining', 'neural network']\n",
      "entity_token: [tensor([ 1166,   118, 11732]), tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534]), tensor([18250,  2443])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parameters into a region that they do not escape, and the results following this initialization are\n",
      "content_token: tensor([  101, 11934,  1154,   170,  1805,  1115,  1152,  1202,  1136,  3359,\n",
      "          117,  1105,  1103,  2686,  1378,  1142,  3288,  2734,  1132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: initialization are more consistent and less likely to be very bad than without this initialization.\n",
      "content_token: tensor([ 101, 3288, 2734, 1132, 1167, 8080, 1105, 1750, 2620, 1106, 1129, 1304,\n",
      "        2213, 1190, 1443, 1142, 3288, 2734,  119,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: initialization. Erhan et al. (2010) also provide some answers as to when pretraining works best—the\n",
      "content_token: tensor([  101,  3288,  2734,   119,   142,  1197,  3822,  3084,  2393,   119,\n",
      "          113,  1333,   114,  1145,  2194,  1199,  6615,  1112,  1106,  1165,\n",
      "         3073,  4487, 16534,  1759,  1436,   783,  1103,   102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: works best—the mean and variance of the test error were most reduced by pretraining for deeper\n",
      "content_token: tensor([  101,  1759,  1436,   783,  1103,  1928,  1105, 26717,  1104,  1103,\n",
      "         2774,  7353,  1127,  1211,  3549,  1118,  3073,  4487, 16534,  1111,\n",
      "         6353,   102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for deeper networks. Keep in mind that these experiments were performed before the invention and\n",
      "content_token: tensor([  101,  1111,  6353,  6379,   119,  7947,  1107,  1713,  1115,  1292,\n",
      "         7857,  1127,  1982,  1196,  1103, 11918,  1105,   102])\n",
      "entity_list: ['deeper networks']\n",
      "entity_token: [tensor([6353, 6379])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the invention and popularization of modern techniques for training very deep networks (rectified\n",
      "content_token: tensor([  101,  1103, 11918,  1105,  1927,  2734,  1104,  2030,  4884,  1111,\n",
      "         2013,  1304,  1996,  6379,   113,  1231,  5822,  6202,   102])\n",
      "entity_list: ['deep networks']\n",
      "entity_token: [tensor([1996, 6379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: networks (rectified linear units, dropout and batch normalization) so less is known about the\n",
      "content_token: tensor([  101,  6379,   113,  1231,  5822,  6202,  7378,  2338,   117,  3968,\n",
      "         3554,  1105, 15817,  2999,  2734,   114,  1177,  1750,  1110,  1227,\n",
      "         1164,  1103,   102])\n",
      "entity_list: ['rectified linear units', 'dropout', 'batch normalization']\n",
      "entity_token: [tensor([1231, 5822, 6202, 7378, 2338]), tensor([3968, 3554]), tensor([15817,  2999,  2734])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 1, 0, 2, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is known about the effect of unsupervised pretraining in conjunction with contemporary approaches.\n",
      "content_token: tensor([  101,  1110,  1227,  1164,  1103,  2629,  1104,  8362,  6385,  3365,\n",
      "        16641,  1181,  3073,  4487, 16534,  1107,  9342,  1114,  3793,  8015,\n",
      "          119,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: approaches. An important question is how unsupervised pretraining can act as a regularizer. One\n",
      "content_token: tensor([  101,  8015,   119,  1760,  1696,  2304,  1110,  1293,  8362,  6385,\n",
      "         3365, 16641,  1181,  3073,  4487, 16534,  1169,  2496,  1112,   170,\n",
      "         2366, 17260,   119,  1448,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a regularizer. One hypothesis is that pretraining encourages the learning algorithm to discover\n",
      "content_token: tensor([  101,   170,  2366, 17260,   119,  1448, 11066,  1110,  1115,  3073,\n",
      "         4487, 16534, 17233,  1103,  3776,  9932,  1106,  7290,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to discover features that relate to the underlying causes that generate the observed data. This is\n",
      "content_token: tensor([  101,  1106,  7290,  1956,  1115, 15123,  1106,  1103, 10311,  4680,\n",
      "         1115,  9509,  1103,  4379,  2233,   119,  1188,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: data. This is an important idea motivating many other algorithms besides unsupervised pretraining,\n",
      "content_token: tensor([  101,  2233,   119,  1188,  1110,  1126,  1696,  1911,   182,  3329,\n",
      "        19012,  1242,  1168, 14975,  8655,  8362,  6385,  3365, 16641,  1181,\n",
      "         3073,  4487, 16534,   117,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining, and is described further in section 15.3. Compared to other forms of unsupervised\n",
      "content_token: tensor([  101,  3073,  4487, 16534,   117,  1105,  1110,  1758,  1748,  1107,\n",
      "         2237,  1405,   119,   124,   119, 22439,  1106,  1168,  2769,  1104,\n",
      "         8362,  6385,  3365, 16641,  1181,   102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of unsupervised learning, unsupervised pretraining has the disadvantage that it operates with two\n",
      "content_token: tensor([  101,  1104,  8362,  6385,  3365, 16641,  1181,  3776,   117,  8362,\n",
      "         6385,  3365, 16641,  1181,  3073,  4487, 16534,  1144,  1103, 22611,\n",
      "         1115,  1122,  5049,  1114,  1160,   102])\n",
      "entity_list: ['unsupervised learning', 'unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776]), tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: operates with two separate training phases. Many regularization strategies have the advantage of\n",
      "content_token: tensor([  101,  5049,  1114,  1160,  2767,  2013, 12877,   119,  2408,  2366,\n",
      "         2734, 10700,  1138,  1103,  4316,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the advantage of allowing the user to control the strength of the regularization by adjusting the\n",
      "content_token: tensor([  101,  1103,  4316,  1104,  3525,  1103,  4795,  1106,  1654,  1103,\n",
      "         3220,  1104,  1103,  2366,  2734,  1118, 21763,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by adjusting the value of a single hyperparameter. Unsupervised pretraining does not offer a clear\n",
      "content_token: tensor([  101,  1118, 21763,  1103,  2860,  1104,   170,  1423,   177, 24312,\n",
      "        17482, 16470,  2083,   119, 12118,  6385,  3365, 16641,  1181,  3073,\n",
      "         4487, 16534,  1674,  1136,  2906,   170,  2330,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: not offer a clear way to adjust the the strength of the regularization arising from the\n",
      "content_token: tensor([  101,  1136,  2906,   170,  2330,  1236,  1106, 14878,  1103,  1103,\n",
      "         3220,  1104,  1103,  2366,  2734, 19528,  1121,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: arising from the unsupervised stage. Instead, there are 534 CHAPTER 15. REPRESENTATION LEARNING\n",
      "content_token: tensor([  101, 19528,  1121,  1103,  8362,  6385,  3365, 16641,  1181,  2016,\n",
      "          119,  3743,   117,  1175,  1132,  4389,  1527,  8203,  1405,   119,\n",
      "          155, 16668, 16941, 12649, 15681, 13821, 24805,   149, 12420,  2069,\n",
      "        27451, 11780,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: LEARNING very many hyperparameters, whose effect may be measured after the fact but is often\n",
      "content_token: tensor([  101,   149, 12420,  2069, 27451, 11780,  1304,  1242,   177, 24312,\n",
      "        17482, 16470,  5759,   117,  2133,  2629,  1336,  1129,  7140,  1170,\n",
      "         1103,  1864,  1133,  1110,  1510,   102])\n",
      "entity_list: ['hyperparameters']\n",
      "entity_token: [tensor([  177, 24312, 17482, 16470,  5759])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: fact but is often difficult to predict ahead of time. When we perform unsupervised and supervised\n",
      "content_token: tensor([  101,  1864,  1133,  1110,  1510,  2846,  1106, 17163,  3075,  1104,\n",
      "         1159,   119,  1332,  1195,  3870,  8362,  6385,  3365, 16641,  1181,\n",
      "         1105, 14199,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and supervised learning simultaneously, instead of using the pretraining strategy, there is a\n",
      "content_token: tensor([  101,  1105, 14199,  3776,  7344,   117,  1939,  1104,  1606,  1103,\n",
      "         3073,  4487, 16534,  5564,   117,  1175,  1110,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: there is a single hyperparameter, usually a coefficient attached to the unsupervised cost, that\n",
      "content_token: tensor([  101,  1175,  1110,   170,  1423,   177, 24312, 17482, 16470,  2083,\n",
      "          117,  1932,   170, 21130,  4309,  1106,  1103,  8362,  6385,  3365,\n",
      "        16641,  1181,  2616,   117,  1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: cost, that determines how strongly the unsupervised objective will regularize the supervised model.\n",
      "content_token: tensor([  101,  2616,   117,  1115, 17579,  1293,  5473,  1103,  8362,  6385,\n",
      "         3365, 16641,  1181,  7649,  1209,  2366,  3708,  1103, 14199,  2235,\n",
      "          119,   102])\n",
      "entity_list: ['unsupervised objective', 'supervised model']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  7649]), tensor([14199,  2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: supervised model. One can always predictably obtain less regularization by decreasing this\n",
      "content_token: tensor([  101, 14199,  2235,   119,  1448,  1169,  1579, 17163,  5382,  6268,\n",
      "         1750,  2366,  2734,  1118, 18326,  1142,   102])\n",
      "entity_list: ['supervised model']\n",
      "entity_token: [tensor([14199,  2235])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by decreasing this coefficient. In the case of unsupervised pretraining, there is not a way of\n",
      "content_token: tensor([  101,  1118, 18326,  1142, 21130,   119,  1130,  1103,  1692,  1104,\n",
      "         8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,   117,  1175,\n",
      "         1110,  1136,   170,  1236,  1104,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is not a way of flexibly adapting the strength of the regularization—either the supervised model is\n",
      "content_token: tensor([  101,  1110,  1136,   170,  1236,  1104, 22593, 11708, 15298, 16677,\n",
      "         1158,  1103,  3220,  1104,  1103,  2366,  2734,   783,  1719,  1103,\n",
      "        14199,  2235,  1110,   102])\n",
      "entity_list: ['regularization']\n",
      "entity_token: [tensor([2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: supervised model is initialized to pretrained parameters, or it is not. Another disadvantage of\n",
      "content_token: tensor([  101, 14199,  2235,  1110,  3288,  2200,  1106,  3073,  4487,  9044,\n",
      "        11934,   117,  1137,  1122,  1110,  1136,   119,  2543, 22611,  1104,\n",
      "          102])\n",
      "entity_list: ['supervised model', 'pretrained parameters']\n",
      "entity_token: [tensor([14199,  2235]), tensor([ 3073,  4487,  9044, 11934])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: disadvantage of having two separate training phases is that each phase has its own hyperparameters.\n",
      "content_token: tensor([  101, 22611,  1104,  1515,  1160,  2767,  2013, 12877,  1110,  1115,\n",
      "         1296,  4065,  1144,  1157,  1319,   177, 24312, 17482, 16470,  5759,\n",
      "          119,   102])\n",
      "entity_list: ['hyperparameters']\n",
      "entity_token: [tensor([  177, 24312, 17482, 16470,  5759])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: hyperparameters. The performance of the second phase usually cannot be predicted during the first\n",
      "content_token: tensor([  101,   177, 24312, 17482, 16470,  5759,   119,  1109,  2099,  1104,\n",
      "         1103,  1248,  4065,  1932,  2834,  1129, 10035,  1219,  1103,  1148,\n",
      "          102])\n",
      "entity_list: ['hyperparameters']\n",
      "entity_token: [tensor([  177, 24312, 17482, 16470,  5759])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: during the first phase, so there is a long delay between proposing hyperparameters for the first\n",
      "content_token: tensor([  101,  1219,  1103,  1148,  4065,   117,  1177,  1175,  1110,   170,\n",
      "         1263,  8513,  1206, 24637,   177, 24312, 17482, 16470,  5759,  1111,\n",
      "         1103,  1148,   102])\n",
      "entity_list: ['hyperparameters']\n",
      "entity_token: [tensor([  177, 24312, 17482, 16470,  5759])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for the first phase and being able to update them using feedback from the second phase. The most\n",
      "content_token: tensor([  101,  1111,  1103,  1148,  4065,  1105,  1217,  1682,  1106, 11984,\n",
      "         1172,  1606, 13032,  1121,  1103,  1248,  4065,   119,  1109,  1211,\n",
      "          102])\n",
      "entity_list: ['hyperparameters']\n",
      "entity_token: [tensor([  177, 24312, 17482, 16470,  5759])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: phase. The most principled approach is to use validation set error in the supervised phase in order\n",
      "content_token: tensor([  101,  4065,   119,  1109,  1211,  6708,  1181,  3136,  1110,  1106,\n",
      "         1329,  9221,  1891,  1383,  7353,  1107,  1103, 14199,  4065,  1107,\n",
      "         1546,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: phase in order to select the hyperparameters of the pretraining phase, as discussed in Larochelle\n",
      "content_token: tensor([  101,  4065,  1107,  1546,  1106,  8247,  1103,   177, 24312, 17482,\n",
      "        16470,  5759,  1104,  1103,  3073,  4487, 16534,  4065,   117,  1112,\n",
      "         6352,  1107,  2001,  2180, 20492,  1513,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in Larochelle et al. (2009). In practice, some hyperparameters, like the number of pretraining\n",
      "content_token: tensor([  101,  1107,  2001,  2180, 20492,  1513,  3084,  2393,   119,   113,\n",
      "         1371,   114,   119,  1130,  2415,   117,  1199,   177, 24312, 17482,\n",
      "        16470,  5759,   117,  1176,  1103,  1295,  1104,  3073,  4487, 16534,\n",
      "          102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 2, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of pretraining iterations, are more conveniently set during the pretraining phase, using early\n",
      "content_token: tensor([  101,  1104,  3073,  4487, 16534,  1122, 17166,  1116,   117,  1132,\n",
      "         1167, 14785,  1193,  1383,  1219,  1103,  3073,  4487, 16534,  4065,\n",
      "          117,  1606,  1346,   102])\n",
      "entity_list: ['pretraining phase']\n",
      "entity_token: [tensor([ 3073,  4487, 16534,  4065])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: phase, using early stopping on the unsupervised objective, which is not ideal but computationally\n",
      "content_token: tensor([  101,  4065,   117,  1606,  1346,  7202,  1113,  1103,  8362,  6385,\n",
      "         3365, 16641,  1181,  7649,   117,  1134,  1110,  1136,  7891,  1133,\n",
      "        19903,  1193,   102])\n",
      "entity_list: ['early stopping', 'unsupervised objective']\n",
      "entity_token: [tensor([1346, 7202]), tensor([ 8362,  6385,  3365, 16641,  1181,  7649])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: but computationally much cheaper than using the supervised objective. Today, unsupervised\n",
      "content_token: tensor([  101,  1133, 19903,  1193,  1277, 17780,  1190,  1606,  1103, 14199,\n",
      "         7649,   119,  3570,   117,  8362,  6385,  3365, 16641,  1181,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Today, unsupervised pretraining has been largely abandoned, except in the field of natural language\n",
      "content_token: tensor([  101,  3570,   117,  8362,  6385,  3365, 16641,  1181,  3073,  4487,\n",
      "        16534,  1144,  1151,  3494,  3928,   117,  2589,  1107,  1103,  1768,\n",
      "         1104,  2379,  1846,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of natural language processing, where the natural representation of words as one-hot vectors\n",
      "content_token: tensor([  101,  1104,  2379,  1846,  6165,   117,  1187,  1103,  2379,  6368,\n",
      "         1104,  1734,  1112,  1141,   118,  2633, 21118,   102])\n",
      "entity_list: ['natural language processing']\n",
      "entity_token: [tensor([2379, 1846, 6165])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as one-hot vectors conveys no similarity information and where very large unlabeled sets are\n",
      "content_token: tensor([  101,  1112,  1141,   118,  2633, 21118, 17863,  1116,  1185, 15213,\n",
      "         1869,  1105,  1187,  1304,  1415,  8362,  1742,  8511,  1174,  3741,\n",
      "         1132,   102])\n",
      "entity_list: ['one-hot vectors']\n",
      "entity_token: [tensor([ 1141,   118,  2633, 21118])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: unlabeled sets are available. Inthat case, the advantageof pretraining is that onecan pretrain once\n",
      "content_token: tensor([  101,  8362,  1742,  8511,  1174,  3741,  1132,  1907,   119,  1130,\n",
      "         7702,  1204,  1692,   117,  1103,  4316, 10008,  3073,  4487, 16534,\n",
      "         1110,  1115,  1141,  7804,  3073,  4487,  1394,  1517,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretrain once on a huge unlabeled set (for example with a corpus containing billions of words),\n",
      "content_token: tensor([  101,  3073,  4487,  1394,  1517,  1113,   170,  3321,  8362,  1742,\n",
      "         8511,  1174,  1383,   113,  1111,  1859,  1114,   170, 26661,  4051,\n",
      "         3775,  1116,  1104,  1734,   114,   117,   102])\n",
      "entity_list: ['pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: billions of words), learn a good representation (typically of words, but also of sentences), and\n",
      "content_token: tensor([  101,  3775,  1116,  1104,  1734,   114,   117,  3858,   170,  1363,\n",
      "         6368,   113,  3417,  1104,  1734,   117,  1133,  1145,  1104, 12043,\n",
      "          114,   117,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of sentences), and then use this representation or fine-tune it for a supervised task for which the\n",
      "content_token: tensor([  101,  1104, 12043,   114,   117,  1105,  1173,  1329,  1142,  6368,\n",
      "         1137,  2503,   118,  9253,  1122,  1111,   170, 14199,  4579,  1111,\n",
      "         1134,  1103,   102])\n",
      "entity_list: ['supervised task']\n",
      "entity_token: [tensor([14199,  4579])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: task for which the training set contains substantially fewer examples. This approach was pioneered\n",
      "content_token: tensor([  101,  4579,  1111,  1134,  1103,  2013,  1383,  2515, 12613,  8307,\n",
      "         5136,   119,  1188,  3136,  1108, 17320,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: was pioneered by by Collobert and Weston (2008b), Turian et al. (2010), and Collobert et al.\n",
      "content_token: tensor([  101,  1108, 17320,  1118,  1118,  9518,  2858,  7488,  1105, 12946,\n",
      "          113,  1369,  1830,   114,   117, 17037,  5476,  3084,  2393,   119,\n",
      "          113,  1333,   114,   117,  1105,  9518,  2858,  7488,  3084,  2393,\n",
      "          119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Collobert et al. (2011a) and remains in common use today. Deeplearning techniquesbasedon\n",
      "content_token: tensor([  101,  9518,  2858,  7488,  3084,  2393,   119,   113,  1349,  1161,\n",
      "          114,  1105,  2606,  1107,  1887,  1329,  2052,   119,  7786, 19094,\n",
      "         4558,  1158,  4884, 14017,  3842,   102])\n",
      "entity_list: ['deep learning techniques']\n",
      "entity_token: [tensor([1996, 3776, 4884])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: techniquesbasedon supervisedlearning, regularizedwith dropout or batch normalization, are able to\n",
      "content_token: tensor([  101,  4884, 14017,  3842, 14199, 19094,  4558,  1158,   117,  2366,\n",
      "         2200, 22922,  3968,  3554,  1137, 15817,  2999,  2734,   117,  1132,\n",
      "         1682,  1106,   102])\n",
      "entity_list: ['supervised learning', 'dropout', 'batch normalization']\n",
      "entity_token: [tensor([14199,  3776]), tensor([3968, 3554]), tensor([15817,  2999,  2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are able to achieve human-level performance on very many tasks, but only with extremely large\n",
      "content_token: tensor([ 101, 1132, 1682, 1106, 5515, 1769,  118, 1634, 2099, 1113, 1304, 1242,\n",
      "        8249,  117, 1133, 1178, 1114, 4450, 1415,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: extremely large labeled datasets. These same techniques out- perform unsupervised pretraining on\n",
      "content_token: tensor([  101,  4450,  1415, 12893,  2233, 27948,   119,  1636,  1269,  4884,\n",
      "         1149,   118,  3870,  8362,  6385,  3365, 16641,  1181,  3073,  4487,\n",
      "        16534,  1113,   102])\n",
      "entity_list: ['labeled datasets']\n",
      "entity_token: [tensor([12893,  2233, 27948])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining on medium-sized datasets such as CIFAR-10 and MNIST, which have roughly 5,000 labeled\n",
      "content_token: tensor([  101,  3073,  4487, 16534,  1113,  5143,   118,  6956,  2233, 27948,\n",
      "         1216,  1112,   140, 15499, 12426,   118,  1275,  1105,   150, 27451,\n",
      "         9272,   117,  1134,  1138,  4986,   126,   117,  1288, 12893,   102])\n",
      "entity_list: ['CIFAR-10', 'MNIST']\n",
      "entity_token: [tensor([  140, 15499, 12426,   118,  1275]), tensor([  150, 27451,  9272])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 5,000 labeled examples per class. On extremely small datasets, such as the alternative splicing\n",
      "content_token: tensor([  101,   126,   117,  1288, 12893,  5136,  1679,  1705,   119,  1212,\n",
      "         4450,  1353,  2233, 27948,   117,  1216,  1112,  1103,  4174,   188,\n",
      "         1643, 22548,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: splicing dataset, Bayesian methods outperform methods based on unsupervised pretraining\n",
      "content_token: tensor([  101,   188,  1643, 22548,  2233,  9388,   117,  2410, 18766,  1389,\n",
      "         4069,  1149,  3365, 13199,  4069,  1359,  1113,  8362,  6385,  3365,\n",
      "        16641,  1181,  3073,  4487, 16534,   102])\n",
      "entity_list: ['Bayesian methods']\n",
      "entity_token: [tensor([ 2410, 18766,  1389,  4069])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining (Srivastava, 2013). For these reasons, the popularityof unsupervised pretraining\n",
      "content_token: tensor([  101,  3073,  4487, 16534,   113,  4471, 11509,  1777,  2497,   117,\n",
      "         1381,   114,   119,  1370,  1292,  3672,   117,  1103,  5587, 10008,\n",
      "         8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,   102])\n",
      "entity_list: ['unsupervised pretraining']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pretraining hasdeclined. Nevertheless, unsupervised pretraining remains an important milestone in\n",
      "content_token: tensor([  101,  3073,  4487, 16534,  1144,  2007, 24230,  1181,   119,  8094,\n",
      "          117,  8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534,  2606,\n",
      "         1126,  1696, 24697,  1107,   102])\n",
      "entity_list: ['pretraining', 'unsupervised pretraining']\n",
      "entity_token: [tensor([ 3073,  4487, 16534]), tensor([ 8362,  6385,  3365, 16641,  1181,  3073,  4487, 16534])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: milestone in the history of deep learning research 535\n",
      "content_token: tensor([  101, 24697,  1107,  1103,  1607,  1104,  1996,  3776,  1844,  4389,\n",
      "         1571,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Chapter 20 Deep Generative Models In this chapter, we present several of the specific kinds of\n",
      "content_token: tensor([  101,  2943,  1406,  7786,  9066, 15306, 24025,  1130,  1142,  6073,\n",
      "          117,  1195,  1675,  1317,  1104,  1103,  2747,  7553,  1104,   102])\n",
      "entity_list: [\"'Deep Generative Models'\"]\n",
      "entity_token: [tensor([  112,  7786,  9066, 15306, 24025,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: specific kinds of generative models that can be built and trained using the techniques presented in\n",
      "content_token: tensor([  101,  2747,  7553,  1104, 11974,  3946,  3584,  1115,  1169,  1129,\n",
      "         1434,  1105,  3972,  1606,  1103,  4884,  2756,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: presented in chapters16–19. All of these models represent probability distributions over multiple\n",
      "content_token: tensor([  101,  2756,  1107,  9611, 16229,   782,  1627,   119,  1398,  1104,\n",
      "         1292,  3584,  4248,  9750, 23190,  1166,  2967,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: over multiple variables in some way. Some allow the probability distribution function to be\n",
      "content_token: tensor([  101,  1166,  2967, 10986,  1107,  1199,  1236,   119,  1789,  2621,\n",
      "         1103,  9750,  3735,  3053,  1106,  1129,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: function to be evaluated explicitly. Others do not allow the evaluation of the probability\n",
      "content_token: tensor([  101,  3053,  1106,  1129, 17428, 12252,   119,  8452,  1202,  1136,\n",
      "         2621,  1103, 10540,  1104,  1103,  9750,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the probability distribution function, but support operations that implicitly require knowledge\n",
      "content_token: tensor([  101,  1104,  1103,  9750,  3735,  3053,   117,  1133,  1619,  2500,\n",
      "         1115, 24034, 18726,  1193,  4752,  3044,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: require knowledge of it, such as drawing samples from the distribution. Some of these models are\n",
      "content_token: tensor([ 101, 4752, 3044, 1104, 1122,  117, 1216, 1112, 4619, 8025, 1121, 1103,\n",
      "        3735,  119, 1789, 1104, 1292, 3584, 1132,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of these models are structured probabilistic models described in terms of graphs and factors, using\n",
      "content_token: tensor([  101,  1104,  1292,  3584,  1132, 15695,  5250,  2822, 15197,  5562,\n",
      "         3584,  1758,  1107,  2538,  1104, 21562,  1105,  5320,   117,  1606,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and factors, using the language of graphical models presented in chapter 16. Others can not easily\n",
      "content_token: tensor([  101,  1105,  5320,   117,  1606,  1103,  1846,  1104, 23885,  3584,\n",
      "         2756,  1107,  6073,  1479,   119,  8452,  1169,  1136,  3253,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can not easily be described in terms of factors, but represent probability distributions\n",
      "content_token: tensor([  101,  1169,  1136,  3253,  1129,  1758,  1107,  2538,  1104,  5320,\n",
      "          117,  1133,  4248,  9750, 23190,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distributions nonetheless. 20.1 Boltzmann Machines Boltzmann machines were originally introduced as\n",
      "content_token: tensor([  101, 23190, 14642,   119,  1406,   119,   122,  9326, 23501,  4119,\n",
      "         7792,  1116,  9326, 23501,  4119,  6555,  1127,  2034,  2234,  1112,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: introduced as a general “connectionist” ap- proach to learning arbitrary probability distributions\n",
      "content_token: tensor([  101,  2234,  1112,   170,  1704,   789,  3797,  1776,   790,   170,\n",
      "         1643,   118,  5250,  7291,  1106,  3776, 16439,  9750, 23190,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distributions over binary vectors (Fahlman et al., 1983; Ackley et al., 1985; Hinton et al., 1984;\n",
      "content_token: tensor([  101, 23190,  1166, 13480, 21118,   113,   143, 24166,  1399,  3084,\n",
      "         2393,   119,   117,  2278,   132,   138, 19053,  1183,  3084,  2393,\n",
      "          119,   117,  2210,   132,  8790, 13124,  3084,  2393,   119,   117,\n",
      "         2219,   132,   102])\n",
      "entity_list: ['binary vectors', 'Hinton et al.']\n",
      "entity_token: [tensor([13480, 21118]), tensor([ 8790, 13124,  3084,  2393,   119])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        2, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: et al., 1984; Hinton and Sejnowski, 1986). Variants of the Boltzmann machine that include other\n",
      "content_token: tensor([  101,  3084,  2393,   119,   117,  2219,   132,  8790, 13124,  1105,\n",
      "        22087, 22923, 13379,   117,  2177,   114,   119,   159,  7968,  2145,\n",
      "         1104,  1103,  9326, 23501,  4119,  3395,  1115,  1511,  1168,   102])\n",
      "entity_list: ['Boltzmann machine', 'Hinton and Sejnowski']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  3395]), tensor([ 8790, 13124,  1105, 22087, 22923, 13379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that include other kinds of variables have long ago surpassed the popularity of the original. In\n",
      "content_token: tensor([  101,  1115,  1511,  1168,  7553,  1104, 10986,  1138,  1263,  2403,\n",
      "        16509,  1103,  5587,  1104,  1103,  1560,   119,  1130,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the original. In this section we briefly introduce the binary Boltzmann machine and discuss the\n",
      "content_token: tensor([  101,  1104,  1103,  1560,   119,  1130,  1142,  2237,  1195,  4016,\n",
      "         8698,  1103, 13480,  9326, 23501,  4119,  3395,  1105,  6265,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and discuss the issues that come up when trying to train and perform inference in the model. We\n",
      "content_token: tensor([  101,  1105,  6265,  1103,  2492,  1115,  1435,  1146,  1165,  1774,\n",
      "         1106,  2669,  1105,  3870,  1107, 16792,  1107,  1103,  2235,   119,\n",
      "         1284,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in the model. We define the Boltzmann machine over a d-dimensional binary random vector x 0,1 d.\n",
      "content_token: tensor([  101,  1107,  1103,  2235,   119,  1284,  9410,  1103,  9326, 23501,\n",
      "         4119,  3395,  1166,   170,   173,   118,  8611, 13480,  7091,  9479,\n",
      "          193,   121,   117,   122,   173,   119,   102])\n",
      "entity_list: ['Boltzmann machine']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  3395])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: vector x 0,1 d. The Boltzmann machine is an energy-based model (section 16.2.4), ∈ { } 654 CHAPTER\n",
      "content_token: tensor([  101,  9479,   193,   121,   117,   122,   173,   119,  1109,  9326,\n",
      "        23501,  4119,  3395,  1110,  1126,  2308,   118,  1359,  2235,   113,\n",
      "         2237,  1479,   119,   123,   119,   125,   114,   117,   850,   196,\n",
      "          198,  2625,  1527,  8203,   102])\n",
      "entity_list: ['Boltzmann machine', 'energy-based model']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  3395]), tensor([2308,  118, 1359, 2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∈ { } 654 CHAPTER 20. DEEP GENERATIVE MODELS meaning we define the joint probability distribution\n",
      "content_token: tensor([  101,   850,   196,   198,  2625,  1527,  8203,  1406,   119, 18581,\n",
      "        16668, 25075, 22680,  9664, 21669, 17145,   150, 15609, 21678,  1708,\n",
      "         2764,  1195,  9410,  1103,  4091,  9750,  3735,   102])\n",
      "entity_list: ['DEEP GENERATIVE MODELS']\n",
      "entity_token: [tensor([18581, 16668, 25075, 22680,  9664, 21669, 17145,   150, 15609, 21678,\n",
      "         1708])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distribution using an energy function: exp( E(x)) P(x) = − , (20.1) Z where E(x) is the energy\n",
      "content_token: tensor([ 101, 3735, 1606, 1126, 2308, 3053,  131, 4252, 1643,  113,  142,  113,\n",
      "         193,  114,  114,  153,  113,  193,  114,  134,  851,  117,  113, 1406,\n",
      "         119,  122,  114,  163, 1187,  142,  113,  193,  114, 1110, 1103, 2308,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: E(x) is the energy function and Z is the partition function that ensures that P(x) = 1. The energy\n",
      "content_token: tensor([  101,   142,   113,   193,   114,  1110,  1103,  2308,  3053,  1105,\n",
      "          163,  1110,  1103, 16416,  3053,  1115, 23613,  1115,   153,   113,\n",
      "          193,   114,   134,   122,   119,  1109,  2308,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: = 1. The energy function of the Boltzmann machine is given by x  E(x) = x Ux b x, (20.2)   − −\n",
      "content_token: tensor([  101,   134,   122,   119,  1109,  2308,  3053,  1104,  1103,  9326,\n",
      "        23501,  4119,  3395,  1110,  1549,  1118,   193,   142,   113,   193,\n",
      "          114,   134,   193,   158,  1775,   171,   193,   117,   113,  1406,\n",
      "          119,   123,   114,   851,   851,   102])\n",
      "entity_list: ['Boltzmann machine']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  3395])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: b x, (20.2)   − − where U is the “weight” matrix of model parameters and b is the vector of bias\n",
      "content_token: tensor([  101,   171,   193,   117,   113,  1406,   119,   123,   114,   851,\n",
      "          851,  1187,   158,  1110,  1103,   789,  2841,   790,  8952,  1104,\n",
      "         2235, 11934,  1105,   171,  1110,  1103,  9479,  1104, 15069,   102])\n",
      "entity_list: ['weight matrix', 'model parameters', 'bias']\n",
      "entity_token: [tensor([2841, 8952]), tensor([ 2235, 11934]), tensor([15069])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0,\n",
      "        0, 0, 0, 0, 2, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the vector of bias parameters. In the general setting of the Boltzmann machine, we are given a set\n",
      "content_token: tensor([  101,  1103,  9479,  1104, 15069, 11934,   119,  1130,  1103,  1704,\n",
      "         3545,  1104,  1103,  9326, 23501,  4119,  3395,   117,  1195,  1132,\n",
      "         1549,   170,  1383,   102])\n",
      "entity_list: ['bias parameters', 'Boltzmann machine']\n",
      "entity_token: [tensor([15069, 11934]), tensor([ 9326, 23501,  4119,  3395])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we are given a set of training examples, each of which are n-dimensional. Equation 20.1 describes\n",
      "content_token: tensor([  101,  1195,  1132,  1549,   170,  1383,  1104,  2013,  5136,   117,\n",
      "         1296,  1104,  1134,  1132,   183,   118,  8611,   119,   142, 13284,\n",
      "         2116,  1406,   119,   122,  4856,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 20.1 describes the joint probability distribution over the observed variables. While this scenario\n",
      "content_token: tensor([  101,  1406,   119,   122,  4856,  1103,  4091,  9750,  3735,  1166,\n",
      "         1103,  4379, 10986,   119,  1799,  1142, 12671,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: While this scenario is certainly viable, it does limit the kinds of interactions between the\n",
      "content_token: tensor([  101,  1799,  1142, 12671,  1110,  4664, 15668,   117,  1122,  1674,\n",
      "         5310,  1103,  7553,  1104, 10393,  1206,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: between the observed variables to those described by the weight matrix. Specifically, it means that\n",
      "content_token: tensor([  101,  1206,  1103,  4379, 10986,  1106,  1343,  1758,  1118,  1103,\n",
      "         2841,  8952,   119, 21325,   117,  1122,  2086,  1115,   102])\n",
      "entity_list: ['weight matrix']\n",
      "entity_token: [tensor([2841, 8952])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it means that the probability of one unit being on is given by a linear model (logistic regression)\n",
      "content_token: tensor([  101,  1122,  2086,  1115,  1103,  9750,  1104,  1141,  2587,  1217,\n",
      "         1113,  1110,  1549,  1118,   170,  7378,  2235,   113,  9366,  5562,\n",
      "         1231, 24032,   114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regression) from the values of the other units. The Boltzmann machine becomes more powerful when\n",
      "content_token: tensor([  101,  1231, 24032,   114,  1121,  1103,  4718,  1104,  1103,  1168,\n",
      "         2338,   119,  1109,  9326, 23501,  4119,  3395,  3316,  1167,  3110,\n",
      "         1165,   102])\n",
      "entity_list: ['Boltzmann machine']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  3395])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: more powerful when not all the variables are observed. In this case, the latent variables, can act\n",
      "content_token: tensor([  101,  1167,  3110,  1165,  1136,  1155,  1103, 10986,  1132,  4379,\n",
      "          119,  1130,  1142,  1692,   117,  1103,  1523,  2227, 10986,   117,\n",
      "         1169,  2496,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: variables, can act similarly to hidden units in a multi-layer perceptron and model higher-order\n",
      "content_token: tensor([  101, 10986,   117,  1169,  2496,  9279,  1106,  4610,  2338,  1107,\n",
      "          170,  4321,   118,  6440,  1679,  2093,  6451,  3484,  1105,  2235,\n",
      "         2299,   118,  1546,   102])\n",
      "entity_list: ['multi-layer perceptron']\n",
      "entity_token: [tensor([4321,  118, 6440, 1679, 2093, 6451, 3484])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: model higher-order interactions among the visible units. Justasthe additionofhidden\n",
      "content_token: tensor([  101,  2235,  2299,   118,  1546, 10393,  1621,  1103,  5085,  2338,\n",
      "          119,  2066, 12788,  4638,  1901, 10008,  3031, 19221,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: additionofhidden unitstoconvertlogisticregressionintoanMLPresults in the MLP being a universal\n",
      "content_token: tensor([  101,  1901, 10008,  3031, 19221,  2338,  2430,  7235, 12986,  8844,\n",
      "         1596,  1874, 24032, 10879, 23516, 19332,  2101,  4894,  7067,  1116,\n",
      "         1107,  1103,   150, 20009,  1217,   170,  8462,   102])\n",
      "entity_list: ['MLP']\n",
      "entity_token: [tensor([  150, 20009])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: being a universal approximator of functions, a Boltzmann machine with hidden units is no longer\n",
      "content_token: tensor([  101,  1217,   170,  8462, 26403, 23021,  1104,  4226,   117,   170,\n",
      "         9326, 23501,  4119,  3395,  1114,  4610,  2338,  1110,  1185,  2039,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: units is no longer limited to modeling linear relationships between variables. Instead, the\n",
      "content_token: tensor([  101,  2338,  1110,  1185,  2039,  2609,  1106, 13117,  7378,  6085,\n",
      "         1206, 10986,   119,  3743,   117,  1103,   102])\n",
      "entity_list: ['Boltzmann machine\\n\\n发现两个深度学习领域的实体：units和Boltzmann machine。']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  3395,   100,   100,   100,   100,   100,   100,\n",
      "          100,   100,   100,   100,   100,   100,   100,  1102,  2338,  1002,\n",
      "         9326, 23501,  4119,  3395,   886])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Instead, the Boltzmann machine becomes a universal approximator of probability mass functions over\n",
      "content_token: tensor([  101,  3743,   117,  1103,  9326, 23501,  4119,  3395,  3316,   170,\n",
      "         8462, 26403, 23021,  1104,  9750,  3367,  4226,  1166,   102])\n",
      "entity_list: ['Boltzmann machine\\n\\n发现一个深度学习领域的实体：Boltzmann machine。']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  3395,   100,   100,   976,   100,   100,   100,\n",
      "          100,   100,   100,   100,   100,   100,   100,  1102,  9326, 23501,\n",
      "         4119,  3395,   886])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: mass functions over discrete variables (Le Roux and Bengio, 2008). Formally, we decompose the units\n",
      "content_token: tensor([  101,  3367,  4226,  1166, 18535, 10986,   113,  3180,   155, 24060,\n",
      "         1105,  3096, 10712,   117,  1369,   114,   119, 15075,  2716,   117,\n",
      "         1195,  1260,  8178, 14811,  1103,  2338,   102])\n",
      "entity_list: ['discrete variables\\n\\n发现三个深度学习领域的实体：units、mass functions和discrete variables。']\n",
      "entity_token: [tensor([18535, 10986,   100,   100,   977,   100,   100,   100,   100,   100,\n",
      "          100,   100,   100,   100,   100,  1102,  2338,   885,  3367,  4226,\n",
      "         1002, 18535, 10986,   886])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: decompose the units x into two subsets: the visible units v and the latent (or hidden) units h. The\n",
      "content_token: tensor([  101,  1260,  8178, 14811,  1103,  2338,   193,  1154,  1160, 18005,\n",
      "         1116,   131,  1103,  5085,  2338,   191,  1105,  1103,  1523,  2227,\n",
      "          113,  1137,  4610,   114,  2338,   177,   119,  1109,   102])\n",
      "entity_list: ['visible units', 'latent units', 'hidden units\\n\\n发现四个深度学习领域的实体：units、visible units、latent units和hidden units。']\n",
      "entity_token: [tensor([5085, 2338]), tensor([1523, 2227, 2338]), tensor([4610, 2338,  100,  100,  100,  100,  100,  100,  100,  100,  100,  100,\n",
      "         100,  100,  100, 1102, 2338,  885, 5085, 2338,  885, 1523, 2227, 2338,\n",
      "        1002, 4610, 2338,  886])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: units h. The energy function becomes E(v,h) = v Rv v Wh h Sh b v c h. (20.3)      − − − − −\n",
      "content_token: tensor([ 101, 2338,  177,  119, 1109, 2308, 3053, 3316,  142,  113,  191,  117,\n",
      "         177,  114,  134,  191,  155, 1964,  191,  160, 1324,  177,  156, 1324,\n",
      "         171,  191,  172,  177,  119,  113, 1406,  119,  124,  114,  851,  851,\n",
      "         851,  851,  851,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content:      − − − − − Boltzmann Machine Learning Learning algorithms for Boltzmann machines are\n",
      "content_token: tensor([  101,   851,   851,   851,   851,   851,  9326, 23501,  4119,  7792,\n",
      "         9681,  9681, 14975,  1111,  9326, 23501,  4119,  6555,  1132,   102])\n",
      "entity_list: ['Boltzmann machines']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  6555])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: machines are usually based on maximum likelihood. All Boltzmann machines have an intractable\n",
      "content_token: tensor([  101,  6555,  1132,  1932,  1359,  1113,  4177, 17843,   119,  1398,\n",
      "         9326, 23501,  4119,  6555,  1138,  1126,  1107, 15017,  1895,   102])\n",
      "entity_list: ['Boltzmann machines']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  6555])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: have an intractable partition function, so the maximum likelihood gradient must be ap- proximated\n",
      "content_token: tensor([  101,  1138,  1126,  1107, 15017,  1895, 16416,  3053,   117,  1177,\n",
      "         1103,  4177, 17843, 19848,  1538,  1129,   170,  1643,   118,  5250,\n",
      "         8745, 22733,   102])\n",
      "entity_list: ['maximum likelihood gradient']\n",
      "entity_token: [tensor([ 4177, 17843, 19848])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: be ap- proximated using the techniques described in chapter 18. One interesting property of\n",
      "content_token: tensor([  101,  1129,   170,  1643,   118,  5250,  8745, 22733,  1606,  1103,\n",
      "         4884,  1758,  1107,  6073,  1407,   119,  1448,  5426,  2400,  1104,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: property of Boltzmann machines when trained with learning rules based on maximum likelihood is that\n",
      "content_token: tensor([  101,  2400,  1104,  9326, 23501,  4119,  6555,  1165,  3972,  1114,\n",
      "         3776,  2995,  1359,  1113,  4177, 17843,  1110,  1115,   102])\n",
      "entity_list: ['Boltzmann machines', 'learning rules', 'maximum likelihood']\n",
      "entity_token: [tensor([ 9326, 23501,  4119,  6555]), tensor([3776, 2995]), tensor([ 4177, 17843])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: likelihood is that the update for a particular weight connecting two units depends only the\n",
      "content_token: tensor([  101, 17843,  1110,  1115,  1103, 11984,  1111,   170,  2440,  2841,\n",
      "         6755,  1160,  2338,  9113,  1178,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: depends only the statistics of those two units, collected under different distributions: P (v) and\n",
      "content_token: tensor([  101,  9113,  1178,  1103,  9161,  1104,  1343,  1160,  2338,   117,\n",
      "         4465,  1223,  1472, 23190,   131,   153,   113,   191,   114,  1105,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: P (v) and Pˆ (v)P (h v). The rest of the model data model | 655\n",
      "content_token: tensor([ 101,  153,  113,  191,  114, 1105,  100,  113,  191,  114,  153,  113,\n",
      "         177,  191,  114,  119, 1109, 1832, 1104, 1103, 2235, 2233, 2235,  197,\n",
      "        2625, 1571,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_content(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "35ee6242-c675-42e5-b346-241bbbc10db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:30:24.565791Z",
     "iopub.status.busy": "2024-02-21T10:30:24.565791Z",
     "iopub.status.idle": "2024-02-21T10:30:24.570139Z",
     "shell.execute_reply": "2024-02-21T10:30:24.570139Z",
     "shell.execute_reply.started": "2024-02-21T10:30:24.565791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    dataDir + \"relations/\" + \"sample.csv\",\n",
    "    index_col=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


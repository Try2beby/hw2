{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c427152e-fb64-4fd2-a1d5-e14f848019aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:57:48.815737Z",
     "iopub.status.busy": "2024-02-21T07:57:48.815737Z",
     "iopub.status.idle": "2024-02-21T07:57:48.819684Z",
     "shell.execute_reply": "2024-02-21T07:57:48.819684Z",
     "shell.execute_reply.started": "2024-02-21T07:57:48.815737Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GitHub\\kg\\.conda\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import torch\n",
    "from fuzzywuzzy import fuzz\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "dataDir = \"../data/\"\n",
    "dataName = \"Deep Learning.pdf\"\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# openai.api_base = \"https://api.chatanywhere.com.cn/\"\n",
    "openai.api_base = \"https://api.chatanywhere.tech\"\n",
    "openai.api_key = \"sk-LzwgVgu5xvNPpwoqCdeeVcAt7Tu7ZoZICXzzkheldIbXA60h\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f4fef-8b2d-41ae-8af6-552e79070234",
   "metadata": {},
   "source": [
    "# 1. 获取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9e3a41-3514-4a05-ba63-0459ef06b1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:33:37.177648Z",
     "iopub.status.busy": "2024-02-21T07:33:37.176648Z",
     "iopub.status.idle": "2024-02-21T07:35:17.816953Z",
     "shell.execute_reply": "2024-02-21T07:35:17.816953Z",
     "shell.execute_reply.started": "2024-02-21T07:33:37.176648Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-large-cased\", cache_dir=\"../../../BERT/large\"\n",
    ")\n",
    "model = BertModel.from_pretrained(\"bert-large-cased\", cache_dir=\"../../../BERT/large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364fd41-4285-4f7d-bdee-b743a822c9de",
   "metadata": {},
   "source": [
    "# 2. 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6304d6-f72e-4c38-b3a2-5548ca3a6459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:06:59.601657Z",
     "iopub.status.busy": "2024-02-21T07:06:59.601657Z",
     "iopub.status.idle": "2024-02-21T07:08:29.596720Z",
     "shell.execute_reply": "2024-02-21T07:08:29.596720Z",
     "shell.execute_reply.started": "2024-02-21T07:06:59.601657Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4845fecdf4c4473596d38f46b5715b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pdfplumber.open(dataDir + dataName) as f:\n",
    "    # 目录架构生成\n",
    "    c, p, n = [], [], []\n",
    "    for i in range(7):\n",
    "        page = f.pages[i]\n",
    "        text = page.extract_text()\n",
    "        text_split = text.split(\"\\n\")\n",
    "        for i in text_split:\n",
    "            if bool(re.match(\"[0-9]+\\.[0-9]+\", i.split(\" \")[0])):\n",
    "                c.append(i.split(\" \")[0])\n",
    "                p.append(int(i.split(\" \")[-1]) + 15)\n",
    "            if bool(re.match(\"[0-9]+\", i.split(\" \")[0])):\n",
    "                for j in i.split(\" \"):\n",
    "                    if bool(re.match(\"[A-Za-z]+\", j)):\n",
    "                        n.append((i.split(\" \")[0], j))\n",
    "\n",
    "p_range = list(zip(p, p[1:]))\n",
    "p_range.append((735, 800))\n",
    "c_p_range = list(zip(c, p_range))\n",
    "index_dict = collections.defaultdict(list)\n",
    "for k, v in c_p_range:\n",
    "    index_dict[k.split(\".\")[0]].append((k, v))\n",
    "\n",
    "with pdfplumber.open(dataDir + dataName) as f:\n",
    "    content_dict = collections.defaultdict(list)\n",
    "\n",
    "    for k, v in tqdm(index_dict.items(), total=len(index_dict)):\n",
    "        for i in v:\n",
    "            page_range = i[-1]\n",
    "            if page_range[0] == page_range[1]:\n",
    "                page_range = (page_range[0], page_range[1] + 1)\n",
    "            for j in range(int(page_range[0]) - 1, int(page_range[1]) - 1):\n",
    "                page = f.pages[j]\n",
    "\n",
    "                text = page.extract_text().replace(\"\\n\", \" \")\n",
    "\n",
    "                content_dict[i[0]].append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb403b",
   "metadata": {},
   "source": [
    "# 3. 定义ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f8717b-9aa9-42ad-9c14-290092ff25b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:33:34.102084Z",
     "iopub.status.busy": "2024-02-21T09:33:34.102084Z",
     "iopub.status.idle": "2024-02-21T09:33:34.105930Z",
     "shell.execute_reply": "2024-02-21T09:33:34.105930Z",
     "shell.execute_reply.started": "2024-02-21T09:33:34.102084Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, conversation_list=[]):\n",
    "        self.conversation_list = conversation_list\n",
    "        self.costs_list = []\n",
    "\n",
    "    def show_conversation(self, msg_list):\n",
    "        for msg in msg_list[-2:]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                pass\n",
    "            else:\n",
    "                message = msg[\"content\"]\n",
    "                pass\n",
    "                # print(f\"\\U0001f47D: {message}\\n\")\n",
    "\n",
    "    def ask(self, prompt):\n",
    "        self.conversation_list.append({\"role\": \"user\", \"content\": prompt})\n",
    "        openai.api_key = \"sk-LzwgVgu5xvNPpwoqCdeeVcAt7Tu7ZoZICXzzkheldIbXA60h\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-16k\", messages=self.conversation_list\n",
    "        )\n",
    "        answer = response.choices[0].message[\"content\"]\n",
    "\n",
    "        self.conversation_list.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        self.show_conversation(self.conversation_list)\n",
    "\n",
    "        # cost = total_counts(response)\n",
    "        # self.costs_list.append(cost)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d68d24",
   "metadata": {},
   "source": [
    "# 4. 设计Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3640d992-d4e0-4de4-8209-7ecabd55e5da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:36:08.229147Z",
     "iopub.status.busy": "2024-02-21T09:36:08.228147Z",
     "iopub.status.idle": "2024-02-21T09:36:08.231752Z",
     "shell.execute_reply": "2024-02-21T09:36:08.231752Z",
     "shell.execute_reply.started": "2024-02-21T09:36:08.229147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NER_prompt = f\"\"\"\n",
    "角色：\n",
    "你是一个深度学习领域的实体标注专员\n",
    "\n",
    "任务：\n",
    "给定字符串，请找出全部深度学习领域的实体\n",
    "\n",
    "步骤：\n",
    "请以以下步骤执行：\n",
    "1. 找出句子中的所有深度学习领域的实体\n",
    "2. 依次检查实体是否属于深度学习领域\n",
    "3. 将属于深度学习领域的实体返回\n",
    "4. 若没有深度学习领域的实体，则返回()\n",
    "\n",
    "格式：\n",
    "请以以下格式返回：\n",
    "('entity1', 'entity2', ...)\n",
    "\n",
    "举例如下：\n",
    "input: An illustration of how the gradient descent algorithm uses the derivatives of a function can be used to follow the function downhill to a minimum.\n",
    "output: (gradient descent algorithm)\n",
    "\n",
    "input: an encoder or reader or input RNN processes the input sequence. The encoder emits the context C, usually as a simple function of its final hidden state.\n",
    "output: (encoder, RNN, hidden state)\n",
    "\n",
    "input: There is no constraint that the encoder must have the same size of hidden layer as the decoder\n",
    "output: (hidden layer, decoder)\n",
    "\n",
    "input: Computer vision has traditionally been one of the most active research areas for deep learning applications, because vision is a task that is effortless for humans and many animals but challenging for computers (Ballard et al., 1983)\n",
    "output: (Computer vision, deep learning)\n",
    "\n",
    "input: Dataset augmentation may be seen as a way of preprocessing the training set only.\n",
    "output: (Dataset augmentation)\n",
    "\n",
    "input: CHAPTER 1. INTRODUCTION of the flowchart of the computations needed to compute the representation of each concept may be much deeper than the graph of the concepts themselves.\n",
    "output: ()\n",
    "\n",
    "注意事项：\n",
    "1. 请严格遵循字符串中的原本表述\n",
    "2. 除返回结果外，不要返回任何其他内容\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6277d2bc-5c2b-4aa1-a604-fb4b825ae6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:48:04.087938Z",
     "iopub.status.busy": "2024-02-21T09:48:04.087938Z",
     "iopub.status.idle": "2024-02-21T09:48:04.090724Z",
     "shell.execute_reply": "2024-02-21T09:48:04.090724Z",
     "shell.execute_reply.started": "2024-02-21T09:48:04.087938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_prompt = f\"\"\"\n",
    "任务：\n",
    "请检查所给实体是否属于深度学习领域\n",
    "\n",
    "格式：\n",
    "请以以下格式返回：\n",
    "如果该实体是深度学习领域的实体，返回True，否则返回False\n",
    "\n",
    "举例如下：\n",
    "input: deep learning\n",
    "output: True\n",
    "\n",
    "input: AI system\n",
    "output: True\n",
    "\n",
    "input: image\n",
    "output: False\n",
    "\n",
    "input: Image Net\n",
    "output: True\n",
    "\n",
    "input: face\n",
    "output: False\n",
    "\n",
    "注意事项：\n",
    "1. 请仅返回True或者False，不要返回任何其他内容\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ab818",
   "metadata": {},
   "source": [
    "# 5. 处理数据，获取sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d20474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_position(content_token, entity_token):\n",
    "    \"\"\"\n",
    "    Finds the position of an entity token within a content token.\n",
    "\n",
    "    Args:\n",
    "        content_token (torch.Tensor): A tensor representing the content token.\n",
    "        entity_token (torch.Tensor): A tensor representing the entity token.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the position of the entity token within the content token.\n",
    "                      Each element in the tensor represents the position of a token in the content token:\n",
    "                      - 0: Token does not match the entity token.\n",
    "                      - 1: Token matches the entity token, but is not the first token.\n",
    "                      - 2: Token matches the entity token and is the first token.\n",
    "    \"\"\"\n",
    "    position = torch.zeros_like(content_token)\n",
    "    for entity in entity_token:\n",
    "        for i in range(len(content_token) - len(entity) + 1):\n",
    "            if torch.all(content_token[i : i + len(entity)] == entity):\n",
    "                position[i] = 2\n",
    "                position[i + 1 : i + len(entity)] = 1\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caabba2f-9476-43b1-8e7f-61c96d38080d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:45:57.615752Z",
     "iopub.status.busy": "2024-02-21T10:45:57.615752Z",
     "iopub.status.idle": "2024-02-21T10:49:32.176168Z",
     "shell.execute_reply": "2024-02-21T10:49:32.176168Z",
     "shell.execute_reply.started": "2024-02-21T10:45:57.615752Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_content(content_dict, content_list):\n",
    "    \"\"\"\n",
    "    Process the content dictionary to extract named entities and save them to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        content_dict (dict): A dictionary containing the content to process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for k, v in content_dict.items():\n",
    "        if k in content_list:\n",
    "            if total >= 1500:\n",
    "                break\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=100, chunk_overlap=20\n",
    "            )\n",
    "            docs = text_splitter.split_text(\" \".join(i for i in v))\n",
    "\n",
    "            # Process each document\n",
    "            for index, content in enumerate(docs):\n",
    "                total += 1\n",
    "                if total == 1500:\n",
    "                    break\n",
    "\n",
    "                # Initialize NER chatbot\n",
    "                if index % 5 == 0:\n",
    "                    conversation_list = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": NER_prompt,\n",
    "                        }\n",
    "                    ]\n",
    "                    bot_ner = Chat(conversation_list)\n",
    "\n",
    "                # Extract named entities using NER chatbot\n",
    "                answer_ner = bot_ner.ask(\"input: \" + content)\n",
    "                entity_list_temp = re.sub(\"\\(|\\)|\", \"\", answer_ner).split(\", \")\n",
    "\n",
    "                # Initialize check chatbot\n",
    "                conversation_list = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": check_prompt,\n",
    "                    }\n",
    "                ]\n",
    "                bot_check = Chat(conversation_list)\n",
    "\n",
    "                entity_list = []\n",
    "                # Check if each entity is valid using check chatbot\n",
    "                for e in entity_list_temp:\n",
    "                    answer_check = bot_check.ask(\"input: \" + e)\n",
    "                    if answer_check == \"True\":\n",
    "                        entity_list.append(e)\n",
    "\n",
    "                # Tokenize content and entities\n",
    "                content_token = tokenizer(content, return_tensors=\"pt\")[\n",
    "                    \"input_ids\"\n",
    "                ].squeeze(0)\n",
    "                entity_token = []\n",
    "                for e in entity_list:\n",
    "                    entity_token.append(\n",
    "                        tokenizer(e, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)[1:-1]\n",
    "                    )\n",
    "\n",
    "                # Find position of entities in content\n",
    "                label = find_position(content_token, entity_token)\n",
    "\n",
    "                # Save the results to a CSV file\n",
    "                df = pd.DataFrame(\n",
    "                    [\n",
    "                        [\n",
    "                            tokenizer.batch_decode(content_token),\n",
    "                            tokenizer.batch_decode(entity_token),\n",
    "                            label,\n",
    "                        ]\n",
    "                    ],\n",
    "                    columns=[\"text\", \"entity\", \"label\"],\n",
    "                )\n",
    "                df.to_csv(\n",
    "                    os.path.join(dataDir + \"/relations\", f\"sample.csv\"),\n",
    "                    mode=\"a\",\n",
    "                    header=not os.path.exists(\n",
    "                        os.path.join(dataDir + \"/relations\", f\"sample.csv\")\n",
    "                    ),\n",
    "                    index=False,\n",
    "                )\n",
    "                # Print the content, tokens, entity list, and label\n",
    "                print(\n",
    "                    \"content: \" + str(content),\n",
    "                    \"content_token: \" + str(content_token),\n",
    "                    \"entity_list: \" + str(entity_list),\n",
    "                    \"entity_token: \" + str(entity_token),\n",
    "                    sep=\"\\n\",\n",
    "                )\n",
    "                print(\"label: \" + str(label))\n",
    "                print(\n",
    "                    \"----------------------------------------------------------------------------------------------------------\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cb9aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: CHAPTER 5. MACHINE LEARNING BASICS an optimization algorithm, a cost function, a model, and a\n",
      "content_token: tensor([  101,  8203,   126,   119, 25424,  3048, 11607,  2036,   149, 12420,\n",
      "         2069, 27451, 11780, 12465, 13882, 12122,  1126, 25161,  9932,   117,\n",
      "          170,  2616,  3053,   117,   170,  2235,   117,  1105,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a model, and a dataset to build a machine learning algorithm. Finally, in section 5.11, we describe\n",
      "content_token: tensor([ 101,  170, 2235,  117, 1105,  170, 2233, 9388, 1106, 3076,  170, 3395,\n",
      "        3776, 9932,  119, 4428,  117, 1107, 2237,  126,  119, 1429,  117, 1195,\n",
      "        5594,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 5.11, we describe some of the factors that have limited the ability of traditional machine learning\n",
      "content_token: tensor([ 101,  126,  119, 1429,  117, 1195, 5594, 1199, 1104, 1103, 5320, 1115,\n",
      "        1138, 2609, 1103, 2912, 1104, 2361, 3395, 3776,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: machine learning to generalize. These challenges have motivated the development of deep learning\n",
      "content_token: tensor([  101,  3395,  3776,  1106,  1704,  3708,   119,  1636,  7806,  1138,\n",
      "        13241,  1103,  1718,  1104,  1996,  3776,   102])\n",
      "entity_list: [\"'deep learning'\"]\n",
      "entity_token: [tensor([ 112, 1996, 3776,  112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of deep learning algorithms that overcome these obstacles. 5.1 Learning Algorithms A machine\n",
      "content_token: tensor([  101,  1104,  1996,  3776, 14975,  1115,  9414,  1292, 16503,   119,\n",
      "          126,   119,   122,  9681,  2586, 18791,  7088,  4206,   138,  3395,\n",
      "          102])\n",
      "entity_list: [\"'deep learning algorithms'\"]\n",
      "entity_token: [tensor([  112,  1996,  3776, 14975,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: A machine learning algorithm is an algorithm that is able to learn from data. But what do we mean\n",
      "content_token: tensor([ 101,  138, 3395, 3776, 9932, 1110, 1126, 9932, 1115, 1110, 1682, 1106,\n",
      "        3858, 1121, 2233,  119, 1252, 1184, 1202, 1195, 1928,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: But what do we mean by learning? Mitchell (1997) provides the definition “A computer program is\n",
      "content_token: tensor([ 101, 1252, 1184, 1202, 1195, 1928, 1118, 3776,  136, 5741,  113, 1816,\n",
      "         114, 2790, 1103, 5754,  789,  138, 2775, 1788, 1110,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: computer program is said to learn from experience E with respect to some class of tasks T and\n",
      "content_token: tensor([ 101, 2775, 1788, 1110, 1163, 1106, 3858, 1121, 2541,  142, 1114, 4161,\n",
      "        1106, 1199, 1705, 1104, 8249,  157, 1105,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves\n",
      "content_token: tensor([ 101, 1104, 8249,  157, 1105, 2099, 4929,  153,  117, 1191, 1157, 2099,\n",
      "        1120, 8249, 1107,  157,  117, 1112, 7140, 1118,  153,  117, 4607, 1116,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by P, improves with experience E.” One can imagine a very wide variety of experiences E, tasks T,\n",
      "content_token: tensor([ 101, 1118,  153,  117, 4607, 1116, 1114, 2541,  142,  119,  790, 1448,\n",
      "        1169, 5403,  170, 1304, 2043, 2783, 1104, 5758,  142,  117, 8249,  157,\n",
      "         117,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: E, tasks T, and performance measures P, and we do not make any attempt in this book to provide a\n",
      "content_token: tensor([ 101,  142,  117, 8249,  157,  117, 1105, 2099, 5252,  153,  117, 1105,\n",
      "        1195, 1202, 1136, 1294, 1251, 2661, 1107, 1142, 1520, 1106, 2194,  170,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: book to provide a formal definition of what may be used for each of these entities. Instead, the\n",
      "content_token: tensor([  101,  1520,  1106,  2194,   170,  4698,  5754,  1104,  1184,  1336,\n",
      "         1129,  1215,  1111,  1296,  1104,  1292, 11659,   119,  3743,   117,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Instead, the following sections provide intuitive descriptions and examples of the different kinds\n",
      "content_token: tensor([  101,  3743,   117,  1103,  1378,  4886,  2194,  1107,  7926,  8588,\n",
      "        14256,  1105,  5136,  1104,  1103,  1472,  7553,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the different kinds of tasks, performance measures and experiences that can be used to construct\n",
      "content_token: tensor([ 101, 1103, 1472, 7553, 1104, 8249,  117, 2099, 5252, 1105, 5758, 1115,\n",
      "        1169, 1129, 1215, 1106, 9417,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: used to construct machine learning algorithms. T 5.1.1 The Task, Machine learning allows us to\n",
      "content_token: tensor([  101,  1215,  1106,  9417,  3395,  3776, 14975,   119,   157,   126,\n",
      "          119,   122,   119,   122,  1109, 11513,   117,  7792,  3776,  3643,\n",
      "         1366,  1106,   102])\n",
      "entity_list: ['machine learning algorithms']\n",
      "entity_token: [tensor([ 3395,  3776, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: allows us to tackle tasks that are too difficult to solve with fixed programs written and designed\n",
      "content_token: tensor([  101,  3643,  1366,  1106, 11407,  8249,  1115,  1132,  1315,  2846,\n",
      "         1106,  9474,  1114,  4275,  2648,  1637,  1105,  2011,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and designed by human beings. From a scientific and philosophical point of view, machine learning\n",
      "content_token: tensor([  101,  1105,  2011,  1118,  1769,  9476,   119,  1622,   170,  3812,\n",
      "         1105, 11388,  1553,  1104,  2458,   117,  3395,  3776,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: machine learning is interesting because developing our understanding of machine learning entails\n",
      "content_token: tensor([  101,  3395,  3776,  1110,  5426,  1272,  4297,  1412,  4287,  1104,\n",
      "         3395,  3776,  4035, 23871,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning entails developing our understanding of the principles that underlie intelligence. In this\n",
      "content_token: tensor([  101,  3776,  4035, 23871,  4297,  1412,  4287,  1104,  1103,  6551,\n",
      "         1115,  1223,  7174,  4810,   119,  1130,  1142,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In this relatively formal definition of the word “task,” the process of learning itself is not the\n",
      "content_token: tensor([ 101, 1130, 1142, 3860, 4698, 5754, 1104, 1103, 1937,  789, 4579,  117,\n",
      "         790, 1103, 1965, 1104, 3776, 2111, 1110, 1136, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: itself is not the task. Learning is our means of attaining the ability to perform the task. For\n",
      "content_token: tensor([  101,  2111,  1110,  1136,  1103,  4579,   119,  9681,  1110,  1412,\n",
      "         2086,  1104, 20386,  1158,  1103,  2912,  1106,  3870,  1103,  4579,\n",
      "          119,  1370,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the task. For example, if we want a robot to be able to walk, then walking is the task. We could\n",
      "content_token: tensor([  101,  1103,  4579,   119,  1370,  1859,   117,  1191,  1195,  1328,\n",
      "          170, 10975,  1106,  1129,  1682,  1106,  2647,   117,  1173,  3179,\n",
      "         1110,  1103,  4579,   119,  1284,  1180,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the task. We could program the robot to learn to walk, or we could attempt to directly write a\n",
      "content_token: tensor([  101,  1103,  4579,   119,  1284,  1180,  1788,  1103, 10975,  1106,\n",
      "         3858,  1106,  2647,   117,  1137,  1195,  1180,  2661,  1106,  2626,\n",
      "         3593,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to directly write a program that specifies how to walk manually. Machine learning tasks are usually\n",
      "content_token: tensor([  101,  1106,  2626,  3593,   170,  1788,  1115,   188, 25392,  9387,\n",
      "         1293,  1106,  2647, 23465,   119,  7792,  3776,  8249,  1132,  1932,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: tasks are usually described in terms of how the machine learningsystem shouldprocess an example.\n",
      "content_token: tensor([  101,  8249,  1132,  1932,  1758,  1107,  2538,  1104,  1293,  1103,\n",
      "         3395,  3776,  5821, 13894,  1306,  1431,  1643,  2180, 22371,  1126,\n",
      "         1859,   119,   102])\n",
      "entity_list: ['machine learning system']\n",
      "entity_token: [tensor([3395, 3776, 1449])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: an example. Anexample isa collectionof features that have been quantitatively measured from some\n",
      "content_token: tensor([  101,  1126,  1859,   119,  1760, 11708, 26671,  1110,  1161,  2436,\n",
      "        10008,  1956,  1115,  1138,  1151, 25220,  1193,  7140,  1121,  1199,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: measured from some object or event that we want the machine learning system to process. We\n",
      "content_token: tensor([ 101, 7140, 1121, 1199, 4231, 1137, 1856, 1115, 1195, 1328, 1103, 3395,\n",
      "        3776, 1449, 1106, 1965,  119, 1284,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to process. We typically represent an example as a vector x Rn where each entry x of the vector is\n",
      "content_token: tensor([ 101, 1106, 1965,  119, 1284, 3417, 4248, 1126, 1859, 1112,  170, 9479,\n",
      "         193,  155, 1179, 1187, 1296, 3990,  193, 1104, 1103, 9479, 1110,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: x of the vector is another feature. For example, i ∈ the features of an image are usually the\n",
      "content_token: tensor([ 101,  193, 1104, 1103, 9479, 1110, 1330, 2672,  119, 1370, 1859,  117,\n",
      "         178,  850, 1103, 1956, 1104, 1126, 3077, 1132, 1932, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are usually the values of the pixels in the image. 99 CHAPTER 5. MACHINE LEARNING BASICS Many kinds\n",
      "content_token: tensor([  101,  1132,  1932,  1103,  4718,  1104,  1103,   185, 28076,  1116,\n",
      "         1107,  1103,  3077,   119,  4850,  8203,   126,   119, 25424,  3048,\n",
      "        11607,  2036,   149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,\n",
      "         2408,  7553,   102])\n",
      "entity_list: ['MACHINE LEARNING BASICS']\n",
      "entity_token: [tensor([25424,  3048, 11607,  2036,   149, 12420,  2069, 27451, 11780, 12465,\n",
      "        13882, 12122])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: BASICS Many kinds of tasks can be solved with machine learning. Some of the most common machine\n",
      "content_token: tensor([  101, 12465, 13882, 12122,  2408,  7553,  1104,  8249,  1169,  1129,\n",
      "        13785,  1114,  3395,  3776,   119,  1789,  1104,  1103,  1211,  1887,\n",
      "         3395,   102])\n",
      "entity_list: ['learning tasks include classification', 'and reinforcement learning. Other tasks ']\n",
      "entity_token: [tensor([3776, 8249, 1511, 5393]), tensor([ 1105, 21293,  1880,  3776,   119,  2189,  8249])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: most common machine learning tasks include the following: Classification: In this typeof task, the\n",
      "content_token: tensor([  101,  1211,  1887,  3395,  3776,  8249,  1511,  1103,  1378,   131,\n",
      "        19295,   131,  1130,  1142,  2076, 10008,  4579,   117,  1103,   102])\n",
      "entity_list: ['the model is trained to learn by interacting with an environment and receiving feedback in the form of rewards or penalties.']\n",
      "entity_token: [tensor([ 1103,  2235,  1110,  3972,  1106,  3858,  1118, 24775,  1114,  1126,\n",
      "         3750,  1105,  4172, 13032,  1107,  1103,  1532,  1104, 22278,  1137,\n",
      "        13095,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: typeof task, the computerprogram is askedto specify • which of k categories some input belongs to.\n",
      "content_token: tensor([  101,  2076, 10008,  4579,   117,  1103,  2775,  1643, 24081,  4515,\n",
      "         1110,  1455,  2430, 22829,   794,  1134,  1104,   180,  6788,  1199,\n",
      "         7758,  7017,  1106,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: input belongs to. To solve this task, the learning algorithm is usually asked to produce a function\n",
      "content_token: tensor([ 101, 7758, 7017, 1106,  119, 1706, 9474, 1142, 4579,  117, 1103, 3776,\n",
      "        9932, 1110, 1932, 1455, 1106, 3133,  170, 3053,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: produce a function f : Rn 1,...,k . When → { } y = f(x), the model assigns an input described by\n",
      "content_token: tensor([  101,  3133,   170,  3053,   175,   131,   155,  1179,   122,   117,\n",
      "          119,   119,   119,   117,   180,   119,  1332,   845,   196,   198,\n",
      "          194,   134,   175,   113,   193,   114,   117,  1103,  2235, 27043,\n",
      "         1126,  7758,  1758,  1118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: input described by vector x to a category identified by numeric code y. There are other variants of\n",
      "content_token: tensor([  101,  7758,  1758,  1118,  9479,   193,  1106,   170,  4370,  3626,\n",
      "         1118,   183, 15447,  4907,  3463,   194,   119,  1247,  1132,  1168,\n",
      "        10317,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: other variants of the classification task, for example, where f outputs a probability distribution\n",
      "content_token: tensor([  101,  1168, 10317,  1104,  1103,  5393,  4579,   117,  1111,  1859,\n",
      "          117,  1187,   175,  5964,  1116,   170,  9750,  3735,   102])\n",
      "entity_list: ['classification task', 'probability distribution']\n",
      "entity_token: [tensor([5393, 4579]), tensor([9750, 3735])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distribution over classes. An example of a classification task is object recognition, where the\n",
      "content_token: tensor([ 101, 3735, 1166, 3553,  119, 1760, 1859, 1104,  170, 5393, 4579, 1110,\n",
      "        4231, 4453,  117, 1187, 1103,  102])\n",
      "entity_list: ['classification task', 'object recognition']\n",
      "entity_token: [tensor([5393, 4579]), tensor([4231, 4453])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: where the input is an image (usually described as a set of pixel brightness values), and the output\n",
      "content_token: tensor([  101,  1187,  1103,  7758,  1110,  1126,  3077,   113,  1932,  1758,\n",
      "         1112,   170,  1383,  1104,   185, 28076, 20828,  4718,   114,   117,\n",
      "         1105,  1103,  5964,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and the output is a numeric code identifying the object in the image. For example, the Willow\n",
      "content_token: tensor([  101,  1105,  1103,  5964,  1110,   170,   183, 15447,  4907,  3463,\n",
      "        12760,  1103,  4231,  1107,  1103,  3077,   119,  1370,  1859,   117,\n",
      "         1103, 15133,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: example, the Willow Garage PR2 robot is able to act as a waiter that can recognize different kinds\n",
      "content_token: tensor([  101,  1859,   117,  1103, 15133,   144,  4626,  2176, 11629,  1477,\n",
      "        10975,  1110,  1682,  1106,  2496,  1112,   170, 17989,  1115,  1169,\n",
      "         6239,  1472,  7553,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: different kinds of drinks and deliver them to people on command (Good- fellow et al., 2010). Modern\n",
      "content_token: tensor([ 101, 1472, 7553, 1104, 8898, 1105, 7852, 1172, 1106, 1234, 1113, 2663,\n",
      "         113, 2750,  118, 3235, 3084, 2393,  119,  117, 1333,  114,  119, 4825,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: al., 2010). Modern object recognition is best accomplished with deep learning (Krizhevsky et al.,\n",
      "content_token: tensor([  101,  2393,   119,   117,  1333,   114,   119,  4825,  4231,  4453,\n",
      "         1110,  1436,  8587,  1114,  1996,  3776,   113,   148, 28021,  4638,\n",
      "        13510,  3084,  2393,   119,   117,   102])\n",
      "entity_list: [\"'object recognition'\", \"'deep learning'\"]\n",
      "entity_token: [tensor([ 112, 4231, 4453,  112]), tensor([ 112, 1996, 3776,  112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (Krizhevsky et al., 2012; Ioffe and Szegedy, 2015). Object recognition is the same basic technology\n",
      "content_token: tensor([  101,   113,   148, 28021,  4638, 13510,  3084,  2393,   119,   117,\n",
      "         1368,   132,   146,  5792,  1162,  1105,   156,  3171,  3660,  1183,\n",
      "          117,  1410,   114,   119,   152, 24380,  4453,  1110,  1103,  1269,\n",
      "         3501,  2815,   102])\n",
      "entity_list: [\"'deep learning'\"]\n",
      "entity_token: [tensor([ 112, 1996, 3776,  112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: basic technology that allows computers to recognize faces (Taigman et al., 2014), which can be used\n",
      "content_token: tensor([  101,  3501,  2815,  1115,  3643,  7565,  1106,  6239,  4876,   113,\n",
      "        16191, 24540,  3084,  2393,   119,   117,  1387,   114,   117,  1134,\n",
      "         1169,  1129,  1215,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: which can be used to automatically tag people in photo collections and allow computers to interact\n",
      "content_token: tensor([  101,  1134,  1169,  1129,  1215,  1106,  7743,  9235,  1234,  1107,\n",
      "         6307,  6286,  1105,  2621,  7565,  1106, 12254,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to interact more naturally with their users. Classification with missing inputs: Classification\n",
      "content_token: tensor([  101,  1106, 12254,  1167,  8534,  1114,  1147,  4713,   119, 19295,\n",
      "         1114,  3764, 22743,   131, 19295,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Classification becomes more chal- • lenging if the computer program is not guaranteed that every\n",
      "content_token: tensor([  101, 19295,  3316,  1167, 22572,  1348,   118,   794,  5837,  2118,\n",
      "         1158,  1191,  1103,  2775,  1788,  1110,  1136, 13008,  1115,  1451,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that every measurement in its input vector will always be provided. In order to solve the\n",
      "content_token: tensor([  101,  1115,  1451, 11842,  1107,  1157,  7758,  9479,  1209,  1579,\n",
      "         1129,  2136,   119,  1130,  1546,  1106,  9474,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: order to solve the classification task, the learning algorithm only has to define a single function\n",
      "content_token: tensor([ 101, 1546, 1106, 9474, 1103, 5393, 4579,  117, 1103, 3776, 9932, 1178,\n",
      "        1144, 1106, 9410,  170, 1423, 3053,  102])\n",
      "entity_list: ['classification task', 'learning algorithm']\n",
      "entity_token: [tensor([5393, 4579]), tensor([3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a single function mapping from a vector input to a categorical output. When some of the inputs may\n",
      "content_token: tensor([  101,   170,  1423,  3053, 13970,  1121,   170,  9479,  7758,  1106,\n",
      "          170,  5855, 23820, 17211,  5964,   119,  1332,  1199,  1104,  1103,\n",
      "        22743,  1336,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the inputs may be missing, rather than providing a single classification function, the learning\n",
      "content_token: tensor([  101,  1104,  1103, 22743,  1336,  1129,  3764,   117,  1897,  1190,\n",
      "         3558,   170,  1423,  5393,  3053,   117,  1103,  3776,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the learning algorithm must learn a set of functions. Each function corresponds to classi- fying x\n",
      "content_token: tensor([  101,  1103,  3776,  9932,  1538,  3858,   170,  1383,  1104,  4226,\n",
      "          119,  2994,  3053, 15497,  1106,  1705,  1182,   118,   175,  9257,\n",
      "          193,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to classi- fying x with a different subset of its inputs missing. This kind of situation arises\n",
      "content_token: tensor([  101,  1106,  1705,  1182,   118,   175,  9257,   193,  1114,   170,\n",
      "         1472, 18005,  1104,  1157, 22743,  3764,   119,  1188,  1912,  1104,\n",
      "         2820, 20251,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of situation arises frequently in medical diagnosis, because many kinds of medical tests are\n",
      "content_token: tensor([  101,  1104,  2820, 20251,  3933,  1107,  2657, 12645,   117,  1272,\n",
      "         1242,  7553,  1104,  2657,  5715,  1132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: medical tests are expensive or invasive. One way to efficiently define such a large set of\n",
      "content_token: tensor([  101,  2657,  5715,  1132,  5865,  1137, 19849,   119,  1448,  1236,\n",
      "         1106, 19723,  9410,  1216,   170,  1415,  1383,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: such a large set of functions is to learn a probability distribution over all of the relevant\n",
      "content_token: tensor([ 101, 1216,  170, 1415, 1383, 1104, 4226, 1110, 1106, 3858,  170, 9750,\n",
      "        3735, 1166, 1155, 1104, 1103, 7503,  102])\n",
      "entity_list: ['probability distribution']\n",
      "entity_token: [tensor([9750, 3735])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: all of the relevant variables, then solve the classification task by marginalizing out the missing\n",
      "content_token: tensor([  101,  1155,  1104,  1103,  7503, 10986,   117,  1173,  9474,  1103,\n",
      "         5393,  4579,  1118, 16404,  4404,  1149,  1103,  3764,   102])\n",
      "entity_list: ['classification task']\n",
      "entity_token: [tensor([5393, 4579])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: out the missing variables. With n input variables, we can now obtain all 2n different classifi-\n",
      "content_token: tensor([  101,  1149,  1103,  3764, 10986,   119,  1556,   183,  7758, 10986,\n",
      "          117,  1195,  1169,  1208,  6268,  1155,   123,  1179,  1472,  1705,\n",
      "         8914,  1182,   118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: different classifi- cation functions needed for each possible set of missing inputs, but we only\n",
      "content_token: tensor([  101,  1472,  1705,  8914,  1182,   118,  5855,  1988,  4226,  1834,\n",
      "         1111,  1296,  1936,  1383,  1104,  3764, 22743,   117,  1133,  1195,\n",
      "         1178,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: inputs, but we only need to learn a single function describing the joint probability distribution.\n",
      "content_token: tensor([  101, 22743,   117,  1133,  1195,  1178,  1444,  1106,  3858,   170,\n",
      "         1423,  3053,  7645,  1103,  4091,  9750,  3735,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distribution. See Goodfellow et al. (2013b) for an example of a deep probabilistic model applied to\n",
      "content_token: tensor([  101,  3735,   119,  3969,  2750, 27610,  4064,  3084,  2393,   119,\n",
      "          113,  1381,  1830,   114,  1111,  1126,  1859,  1104,   170,  1996,\n",
      "         5250,  2822, 15197,  5562,  2235,  3666,  1106,   102])\n",
      "entity_list: ['deep probabilistic model']\n",
      "entity_token: [tensor([ 1996,  5250,  2822, 15197,  5562,  2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: model applied to such a task in this way. Many of the other tasks described in this section can\n",
      "content_token: tensor([ 101, 2235, 3666, 1106, 1216,  170, 4579, 1107, 1142, 1236,  119, 2408,\n",
      "        1104, 1103, 1168, 8249, 1758, 1107, 1142, 2237, 1169,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in this section can also be generalized to work with missing inputs; classification with missing\n",
      "content_token: tensor([  101,  1107,  1142,  2237,  1169,  1145,  1129, 22214,  1106,  1250,\n",
      "         1114,  3764, 22743,   132,  5393,  1114,  3764,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with missing inputs is just one example of what machine learning can do. 100 CHAPTER 5. MACHINE\n",
      "content_token: tensor([  101,  1114,  3764, 22743,  1110,  1198,  1141,  1859,  1104,  1184,\n",
      "         3395,  3776,  1169,  1202,   119,  1620,  8203,   126,   119, 25424,\n",
      "         3048, 11607,  2036,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 5. MACHINE LEARNING BASICS Regression: In this type of task, the computer program is asked\n",
      "content_token: tensor([  101,  8203,   126,   119, 25424,  3048, 11607,  2036,   149, 12420,\n",
      "         2069, 27451, 11780, 12465, 13882, 12122, 23287, 26779,   131,  1130,\n",
      "         1142,  2076,  1104,  4579,   117,  1103,  2775,  1788,  1110,  1455,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: program is asked to predict a • numerical value given some input. To solve this task, the learning\n",
      "content_token: tensor([  101,  1788,  1110,  1455,  1106, 17163,   170,   794, 18294,  2860,\n",
      "         1549,  1199,  7758,   119,  1706,  9474,  1142,  4579,   117,  1103,\n",
      "         3776,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: task, the learning algorithm is asked to output a function f : Rn R . This type of task is similar\n",
      "content_token: tensor([ 101, 4579,  117, 1103, 3776, 9932, 1110, 1455, 1106, 5964,  170, 3053,\n",
      "         175,  131,  155, 1179,  155,  119, 1188, 2076, 1104, 4579, 1110, 1861,\n",
      "         102])\n",
      "entity_list: ['learning algorithm']\n",
      "entity_token: [tensor([3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of task is similar to → classification, except that the format of output is different. An example\n",
      "content_token: tensor([ 101, 1104, 4579, 1110, 1861, 1106,  845, 5393,  117, 2589, 1115, 1103,\n",
      "        3536, 1104, 5964, 1110, 1472,  119, 1760, 1859,  102])\n",
      "entity_list: ['classification']\n",
      "entity_token: [tensor([5393])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: An example of a regression task is the prediction of the expected claim amount that an insured\n",
      "content_token: tensor([  101,  1760,  1859,  1104,   170,  1231, 24032,  4579,  1110,  1103,\n",
      "        20770,  1104,  1103,  2637,  3548,  2971,  1115,  1126, 22233, 10105,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that an insured person will make (used to set insurance premiums), or the prediction of future\n",
      "content_token: tensor([  101,  1115,  1126, 22233, 10105,  1825,  1209,  1294,   113,  1215,\n",
      "         1106,  1383,  5986, 16865,  1116,   114,   117,  1137,  1103, 20770,\n",
      "         1104,  2174,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of future prices of securities. These kinds of predictions are also used for algorithmic trading.\n",
      "content_token: tensor([  101,  1104,  2174,  7352,  1104, 19313,   119,  1636,  7553,  1104,\n",
      "        23770,  1132,  1145,  1215,  1111,  9932,  1596,  6157,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: trading. Transcription: In this type of task, the machine learning system is asked • to observe a\n",
      "content_token: tensor([  101,  6157,   119, 13809, 27530,   131,  1130,  1142,  2076,  1104,\n",
      "         4579,   117,  1103,  3395,  3776,  1449,  1110,  1455,   794,  1106,\n",
      "        12326,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: • to observe a relatively unstructured representation of some kind of data and transcribe it into\n",
      "content_token: tensor([  101,   794,  1106, 12326,   170,  3860,  8362, 25198,  1181,  6368,\n",
      "         1104,  1199,  1912,  1104,  2233,  1105, 14715, 17770,  1122,  1154,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: transcribe it into discrete, textual form. For example, in optical character recognition, the\n",
      "content_token: tensor([  101, 14715, 17770,  1122,  1154, 18535,   117,  3087,  4746,  1532,\n",
      "          119,  1370,  1859,   117,  1107, 10312,  1959,  4453,   117,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: recognition, the computer program is shown a photograph containing an image of text and is asked to\n",
      "content_token: tensor([  101,  4453,   117,  1103,  2775,  1788,  1110,  2602,   170, 10110,\n",
      "         4051,  1126,  3077,  1104,  3087,  1105,  1110,  1455,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and is asked to return this text in the form of a sequence of characters (e.g., in ASCII or Unicode\n",
      "content_token: tensor([  101,  1105,  1110,  1455,  1106,  1862,  1142,  3087,  1107,  1103,\n",
      "         1532,  1104,   170,  4954,  1104,  2650,   113,   174,   119,   176,\n",
      "          119,   117,  1107, 15278, 19747,  2240,  1137, 12118, 10658,  2007,\n",
      "          102])\n",
      "entity_list: ['recognition']\n",
      "entity_token: [tensor([4453])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in ASCII or Unicode format). Google Street View uses deep learning to process address numbers in\n",
      "content_token: tensor([  101,  1107, 15278, 19747,  2240,  1137, 12118, 10658,  2007,  3536,\n",
      "          114,   119,  7986,  1715, 10344,  2745,  1996,  3776,  1106,  1965,\n",
      "         4134,  2849,  1107,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: address numbers in this way (Goodfellow et al., 2014d). Another example is speech recognition,\n",
      "content_token: tensor([  101,  4134,  2849,  1107,  1142,  1236,   113,  2750, 27610,  4064,\n",
      "         3084,  2393,   119,   117,  1387,  1181,   114,   119,  2543,  1859,\n",
      "         1110,  4055,  4453,   117,   102])\n",
      "entity_list: ['deep learning', 'speech recognition']\n",
      "entity_token: [tensor([1996, 3776]), tensor([4055, 4453])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: speech recognition, where the computer program is provided an audio waveform and emits a sequence\n",
      "content_token: tensor([  101,  4055,  4453,   117,  1187,  1103,  2775,  1788,  1110,  2136,\n",
      "         1126,  6056,  4003, 13199,  1105,  9712,  6439,   170,  4954,   102])\n",
      "entity_list: ['deep learning', 'speech recognition']\n",
      "entity_token: [tensor([1996, 3776]), tensor([4055, 4453])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: emits a sequence of characters or word ID codes describing the words that were spoken in the audio\n",
      "content_token: tensor([  101,  9712,  6439,   170,  4954,  1104,  2650,  1137,  1937, 10999,\n",
      "         9812,  7645,  1103,  1734,  1115,  1127,  4606,  1107,  1103,  6056,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: spoken in the audio recording. Deep learning is a crucial component of modern speech recognition\n",
      "content_token: tensor([  101,  4606,  1107,  1103,  6056,  2730,   119,  7786,  3776,  1110,\n",
      "          170, 10268,  6552,  1104,  2030,  4055,  4453,   102])\n",
      "entity_list: ['Deep learning', 'speech recognition']\n",
      "entity_token: [tensor([7786, 3776]), tensor([4055, 4453])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: speech recognition systems used at major companies including Microsoft, IBM and Google (Hinton et\n",
      "content_token: tensor([  101,  4055,  4453,  2344,  1215,  1120,  1558,  2557,  1259,  6998,\n",
      "          117,  9768,  1105,  7986,   113,  8790, 13124,  3084,   102])\n",
      "entity_list: ['output: speech recognition systems']\n",
      "entity_token: [tensor([5964,  131, 4055, 4453, 2344])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Google (Hinton et al., 2012b). Machine translation: In a machine translation task, the input\n",
      "content_token: tensor([  101,  7986,   113,  8790, 13124,  3084,  2393,   119,   117,  1368,\n",
      "         1830,   114,   119,  7792,  5179,   131,  1130,   170,  3395,  5179,\n",
      "         4579,   117,  1103,  7758,   102])\n",
      "entity_list: ['Hinton et al.', 'machine translation']\n",
      "entity_token: [tensor([ 8790, 13124,  3084,  2393,   119]), tensor([3395, 5179])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: task, the input already • consistsof asequence ofsymbols insome language, andthe computerprogram\n",
      "content_token: tensor([  101,  4579,   117,  1103,  7758,  1640,   794,  2923, 10008,  1112,\n",
      "         1162, 25113,  1104,  5821, 13685,  3447, 22233,  6758,  1846,   117,\n",
      "         1105, 10681,  2775,  1643, 24081,  4515,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: computerprogram must convert this into a sequence of symbols in another language. This is commonly\n",
      "content_token: tensor([  101,  2775,  1643, 24081,  4515,  1538, 10454,  1142,  1154,   170,\n",
      "         4954,  1104,  9282,  1107,  1330,  1846,   119,  1188,  1110,  3337,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This is commonly applied to natural languages, such as translating from English to French. Deep\n",
      "content_token: tensor([  101,  1188,  1110,  3337,  3666,  1106,  2379,  3483,   117,  1216,\n",
      "         1112, 26894,  1121,  1483,  1106,  1497,   119,  7786,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to French. Deep learning has recently begun to have an important impact on this kind of task\n",
      "content_token: tensor([ 101, 1106, 1497,  119, 7786, 3776, 1144, 3055, 4972, 1106, 1138, 1126,\n",
      "        1696, 3772, 1113, 1142, 1912, 1104, 4579,  102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this kind of task (Sutskever et al., 2014; Bahdanau et al., 2015). Structured output: Structured\n",
      "content_token: tensor([  101,  1142,  1912,  1104,  4579,   113, 15463, 23450, 17791,  3084,\n",
      "         2393,   119,   117,  1387,   132, 18757,  1324,  6778,  3984,  3084,\n",
      "         2393,   119,   117,  1410,   114,   119, 25341,  1181,  5964,   131,\n",
      "        25341,  1181,   102])\n",
      "entity_list: ['Sutskever et al.', 'Bahdanau et al.']\n",
      "entity_token: [tensor([15463, 23450, 17791,  3084,  2393,   119]), tensor([18757,  1324,  6778,  3984,  3084,  2393,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: output: Structured output tasks involve any task where the • output is a vector (or other data\n",
      "content_token: tensor([  101,  5964,   131, 25341,  1181,  5964,  8249,  8803,  1251,  4579,\n",
      "         1187,  1103,   794,  5964,  1110,   170,  9479,   113,  1137,  1168,\n",
      "         2233,   102])\n",
      "entity_list: ['Structured output']\n",
      "entity_token: [tensor([25341,  1181,  5964])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (or other data structure containing multiple values) with important relationships between the\n",
      "content_token: tensor([ 101,  113, 1137, 1168, 2233, 2401, 4051, 2967, 4718,  114, 1114, 1696,\n",
      "        6085, 1206, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: between the different elements. This is a broad category, and subsumes the transcription and\n",
      "content_token: tensor([  101,  1206,  1103,  1472,  3050,   119,  1188,  1110,   170,  4728,\n",
      "         4370,   117,  1105,  4841, 22369,  1116,  1103, 15416,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: transcription and translation tasks described above, but also many other tasks. One example is\n",
      "content_token: tensor([  101, 15416,  1105,  5179,  8249,  1758,  1807,   117,  1133,  1145,\n",
      "         1242,  1168,  8249,   119,  1448,  1859,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: One example is parsing—mapping a natural language sentence into a tree that describes its\n",
      "content_token: tensor([  101,  1448,  1859,  1110, 14247,  4253,   783, 13970,   170,  2379,\n",
      "         1846,  5650,  1154,   170,  2780,  1115,  4856,  1157,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that describes its grammatical structure and tagging nodes of the trees as being verbs, nouns, or\n",
      "content_token: tensor([  101,  1115,  4856,  1157, 25968, 26128,  2401,  1105,  9235,  3375,\n",
      "        15029,  1104,  1103,  2863,  1112,  1217, 17992,   117, 22270,   117,\n",
      "         1137,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: verbs, nouns, or adverbs, and so on. See Collobert (2011) for an example of deep learning applied\n",
      "content_token: tensor([  101, 17992,   117, 22270,   117,  1137,  8050,  4121,  4832,   117,\n",
      "         1105,  1177,  1113,   119,  3969,  9518,  2858,  7488,   113,  1349,\n",
      "          114,  1111,  1126,  1859,  1104,  1996,  3776,  3666,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning applied to a parsing task. Another example is pixel-wise segmentation of images, where the\n",
      "content_token: tensor([  101,  3776,  3666,  1106,   170, 14247,  4253,  4579,   119,  2543,\n",
      "         1859,  1110,   185, 28076,   118, 10228,  6441,  1891,  1104,  4351,\n",
      "          117,  1187,  1103,   102])\n",
      "entity_list: ['pixel-wise segmentation']\n",
      "entity_token: [tensor([  185, 28076,   118, 10228,  6441,  1891])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: images, where the computer program assigns every pixel in an image to a specific category. For 101\n",
      "content_token: tensor([  101,  4351,   117,  1187,  1103,  2775,  1788, 27043,  1451,   185,\n",
      "        28076,  1107,  1126,  3077,  1106,   170,  2747,  4370,   119,  1370,\n",
      "         7393,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: category. For 101 CHAPTER 5. MACHINE LEARNING BASICS example, deep learning can be used to annotate\n",
      "content_token: tensor([  101,  4370,   119,  1370,  7393,  8203,   126,   119, 25424,  3048,\n",
      "        11607,  2036,   149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,\n",
      "         1859,   117,  1996,  3776,  1169,  1129,  1215,  1106,  1126, 12512,\n",
      "         2193,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: be used to annotate the locations of roads in aerial photographs (Mnih and Hinton, 2010). The\n",
      "content_token: tensor([  101,  1129,  1215,  1106,  1126, 12512,  2193,  1103,  4541,  1104,\n",
      "         4744,  1107, 10485,  6810,   113,   150,  2605,  1324,  1105,  8790,\n",
      "        13124,   117,  1333,   114,   119,  1109,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Hinton, 2010). The output need not have its form mirror the structure of the input as closely as in\n",
      "content_token: tensor([  101,  8790, 13124,   117,  1333,   114,   119,  1109,  5964,  1444,\n",
      "         1136,  1138,  1157,  1532,  5220,  1103,  2401,  1104,  1103,  7758,\n",
      "         1112,  4099,  1112,  1107,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as closely as in these annotation-style tasks. For example, in image captioning, the computer\n",
      "content_token: tensor([  101,  1112,  4099,  1112,  1107,  1292,  1126, 12512,  1891,   118,\n",
      "         1947,  8249,   119,  1370,  1859,   117,  1107,  3077,  6707,  2116,\n",
      "         1158,   117,  1103,  2775,   102])\n",
      "entity_list: ['image captioning']\n",
      "entity_token: [tensor([3077, 6707, 2116, 1158])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the computer program observes an image and outputs a natural language sentence describing the image\n",
      "content_token: tensor([  101,  1103,  2775,  1788, 27641,  1126,  3077,  1105,  5964,  1116,\n",
      "          170,  2379,  1846,  5650,  7645,  1103,  3077,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the image (Kiros et al., 2014a,b; Mao et al., 2015; Vinyals et al., 2015b; Donahue et al., 2014;\n",
      "content_token: tensor([  101,  1103,  3077,   113, 14477,  5864,  3084,  2393,   119,   117,\n",
      "         1387,  1161,   117,   171,   132, 16922,  3084,  2393,   119,   117,\n",
      "         1410,   132, 25354, 18543,  1116,  3084,  2393,   119,   117,  1410,\n",
      "         1830,   132,  1790, 23222,  1162,  3084,  2393,   119,   117,  1387,\n",
      "          132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: et al., 2014; Karpathy and Li, 2015; Fang et al., 2015; Xu et al., 2015). These tasks are called\n",
      "content_token: tensor([  101,  3084,  2393,   119,   117,  1387,   132, 14812, 15615, 23610,\n",
      "         1105,  5255,   117,  1410,   132, 18602,  3084,  2393,   119,   117,\n",
      "         1410,   132, 17584,  3084,  2393,   119,   117,  1410,   114,   119,\n",
      "         1636,  8249,  1132,  1270,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: tasks are called structured output tasks because the program must output several values that are\n",
      "content_token: tensor([  101,  8249,  1132,  1270, 15695,  5964,  8249,  1272,  1103,  1788,\n",
      "         1538,  5964,  1317,  4718,  1115,  1132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: values that are all tightly inter-related. For example, the words produced by an image captioning\n",
      "content_token: tensor([ 101, 4718, 1115, 1132, 1155, 6852, 9455,  118, 2272,  119, 1370, 1859,\n",
      "         117, 1103, 1734, 1666, 1118, 1126, 3077, 6707, 2116, 1158,  102])\n",
      "entity_list: ['image captioning']\n",
      "entity_token: [tensor([3077, 6707, 2116, 1158])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: an image captioning program must form a valid sentence. Anomaly detection: In this type of task,\n",
      "content_token: tensor([  101,  1126,  3077,  6707,  2116,  1158,  1788,  1538,  1532,   170,\n",
      "         9221,  5650,   119,  1760,  7903,  1193, 11432,   131,  1130,  1142,\n",
      "         2076,  1104,  4579,   117,   102])\n",
      "entity_list: ['image captioning', 'anomaly detection']\n",
      "entity_token: [tensor([3077, 6707, 2116, 1158]), tensor([ 1126,  7903,  1193, 11432])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this type of task, the computer program sifts • through a set of events or objects, and flags some\n",
      "content_token: tensor([  101,  1142,  2076,  1104,  4579,   117,  1103,  2775,  1788, 27466,\n",
      "        22863,   794,  1194,   170,  1383,  1104,  1958,  1137,  4546,   117,\n",
      "         1105, 14870,  1199,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and flags some of them as being unusual or atypical. An example of an anomaly detection task is\n",
      "content_token: tensor([  101,  1105, 14870,  1199,  1104,  1172,  1112,  1217,  5283,  1137,\n",
      "         1120,  1183, 15328,   119,  1760,  1859,  1104,  1126,  1126,  7903,\n",
      "         1193, 11432,  4579,  1110,   102])\n",
      "entity_list: ['anomaly detection']\n",
      "entity_token: [tensor([ 1126,  7903,  1193, 11432])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: detection task is credit card fraud detection. By modeling your purchasing habits, a credit card\n",
      "content_token: tensor([  101, 11432,  4579,  1110,  4755,  3621, 10258, 11432,   119,  1650,\n",
      "        13117,  1240, 13980, 15640,   117,   170,  4755,  3621,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a credit card company can detect misuse of your cards. If a thief steals your credit card or credit\n",
      "content_token: tensor([  101,   170,  4755,  3621,  1419,  1169, 11552,  1940, 24784,  1104,\n",
      "         1240,  4802,   119,  1409,   170, 16529, 16867,  1240,  4755,  3621,\n",
      "         1137,  4755,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: card or credit card information, the thief’s purchases will often come from a different probability\n",
      "content_token: tensor([  101,  3621,  1137,  4755,  3621,  1869,   117,  1103, 16529,   787,\n",
      "          188, 18908,  1209,  1510,  1435,  1121,   170,  1472,  9750,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: probability distribution over purchase types than your own. The credit card company can prevent\n",
      "content_token: tensor([ 101, 9750, 3735, 1166, 4779, 3322, 1190, 1240, 1319,  119, 1109, 4755,\n",
      "        3621, 1419, 1169, 3843,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: company can prevent fraud by placing a hold on an account as soon as that card has been used for an\n",
      "content_token: tensor([  101,  1419,  1169,  3843, 10258,  1118,  6544,   170,  2080,  1113,\n",
      "         1126,  3300,  1112,  1770,  1112,  1115,  3621,  1144,  1151,  1215,\n",
      "         1111,  1126,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: been used for an uncharacteristic purchase. See Chandola et al. (2009) for a survey of anomaly\n",
      "content_token: tensor([  101,  1151,  1215,  1111,  1126,  8362,  7147, 19366,  2083,  5562,\n",
      "         4779,   119,  3969, 10185,  2572,  1742,  3084,  2393,   119,   113,\n",
      "         1371,   114,  1111,   170,  5980,  1104,  1126,  7903,  1193,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a survey of anomaly detection methods. Synthesis and sampling: In this type of task, the machine\n",
      "content_token: tensor([  101,   170,  5980,  1104,  1126,  7903,  1193, 11432,  4069,   119,\n",
      "          156, 26588, 16317,  1105, 18200,   131,  1130,  1142,  2076,  1104,\n",
      "         4579,   117,  1103,  3395,   102])\n",
      "entity_list: ['anomaly detection methods']\n",
      "entity_token: [tensor([ 1126,  7903,  1193, 11432,  4069])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: task, the machine learning al- • gorithm is asked to generate new examples that are similar to\n",
      "content_token: tensor([  101,  4579,   117,  1103,  3395,  3776,  2393,   118,   794,  1301,\n",
      "        24969,  1306,  1110,  1455,  1106,  9509,  1207,  5136,  1115,  1132,\n",
      "         1861,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that are similar to those in the training data. Synthesis and sampling via machine learning can be\n",
      "content_token: tensor([  101,  1115,  1132,  1861,  1106,  1343,  1107,  1103,  2013,  2233,\n",
      "          119,   156, 26588, 16317,  1105, 18200,  2258,  3395,  3776,  1169,\n",
      "         1129,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning can be useful for media applications where it can be expensive or boring for an artist to\n",
      "content_token: tensor([  101,  3776,  1169,  1129,  5616,  1111,  2394,  4683,  1187,  1122,\n",
      "         1169,  1129,  5865,  1137, 12533,  1111,  1126,  2360,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for an artist to generate large volumes of content by hand. For example, video games can\n",
      "content_token: tensor([ 101, 1111, 1126, 2360, 1106, 9509, 1415, 6357, 1104, 3438, 1118, 1289,\n",
      "         119, 1370, 1859,  117, 1888, 1638, 1169,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: video games can automatically generate textures for large objects or landscapes, rather than\n",
      "content_token: tensor([  101,  1888,  1638,  1169,  7743,  9509, 16117,  1116,  1111,  1415,\n",
      "         4546,  1137, 14026,   117,  1897,  1190,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: rather than requiring an artist to manually label each pixel (Luo et al., 2013). In some cases, we\n",
      "content_token: tensor([  101,  1897,  1190,  8753,  1126,  2360,  1106, 23465,  3107,  1296,\n",
      "          185, 28076,   113, 14557,  1186,  3084,  2393,   119,   117,  1381,\n",
      "          114,   119,  1130,  1199,  2740,   117,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In some cases, we want the sampling or synthesis procedure to generate some specific kind of output\n",
      "content_token: tensor([  101,  1130,  1199,  2740,   117,  1195,  1328,  1103, 18200,  1137,\n",
      "        11362,  7791,  1106,  9509,  1199,  2747,  1912,  1104,  5964,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: kind of output given the input. For example, in a speech synthesis task, we provide a written\n",
      "content_token: tensor([  101,  1912,  1104,  5964,  1549,  1103,  7758,   119,  1370,  1859,\n",
      "          117,  1107,   170,  4055, 11362,  4579,   117,  1195,  2194,   170,\n",
      "         1637,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: provide a written sentence and ask the program to emit an audio waveform containing a spoken\n",
      "content_token: tensor([  101,  2194,   170,  1637,  5650,  1105,  2367,  1103,  1788,  1106,\n",
      "         9712,  2875,  1126,  6056,  4003, 13199,  4051,   170,  4606,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: containing a spoken version of that sentence. This is a kind of structured output task, but with\n",
      "content_token: tensor([  101,  4051,   170,  4606,  1683,  1104,  1115,  5650,   119,  1188,\n",
      "         1110,   170,  1912,  1104, 15695,  5964,  4579,   117,  1133,  1114,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: task, but with the added qualification that there is no single correct output for each input, and\n",
      "content_token: tensor([ 101, 4579,  117, 1133, 1114, 1103, 1896, 8969, 1115, 1175, 1110, 1185,\n",
      "        1423, 5663, 5964, 1111, 1296, 7758,  117, 1105,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for each input, and we explicitly desire a large amount of variation in the output, in order for\n",
      "content_token: tensor([  101,  1111,  1296,  7758,   117,  1105,  1195, 12252,  4232,   170,\n",
      "         1415,  2971,  1104,  8516,  1107,  1103,  5964,   117,  1107,  1546,\n",
      "         1111,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in order for the output to seem more natural and realistic. Imputation of missing values: In this\n",
      "content_token: tensor([  101,  1107,  1546,  1111,  1103,  5964,  1106,  3166,  1167,  2379,\n",
      "         1105, 13142,   119,   146,  8223, 15012,  2116,  1104,  3764,  4718,\n",
      "          131,  1130,  1142,   102])\n",
      "entity_list: ['imputation of missing values']\n",
      "entity_token: [tensor([24034, 15012,  2116,  1104,  3764,  4718])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: values: In this type of task, the machine learning • algorithm is given a new example x Rn, but\n",
      "content_token: tensor([ 101, 4718,  131, 1130, 1142, 2076, 1104, 4579,  117, 1103, 3395, 3776,\n",
      "         794, 9932, 1110, 1549,  170, 1207, 1859,  193,  155, 1179,  117, 1133,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: example x Rn, but with some entries x of x i ∈ missing. The algorithm must provide a prediction of\n",
      "content_token: tensor([  101,  1859,   193,   155,  1179,   117,  1133,  1114,  1199, 10813,\n",
      "          193,  1104,   193,   178,   850,  3764,   119,  1109,  9932,  1538,\n",
      "         2194,   170, 20770,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a prediction of the values of the missing entries. 102 CHAPTER 5. MACHINE LEARNING BASICS\n",
      "content_token: tensor([  101,   170, 20770,  1104,  1103,  4718,  1104,  1103,  3764, 10813,\n",
      "          119,  9081,  8203,   126,   119, 25424,  3048, 11607,  2036,   149,\n",
      "        12420,  2069, 27451, 11780, 12465, 13882, 12122,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: LEARNING BASICS Denoising: In this type of task, the machine learning algorithm is given in •\n",
      "content_token: tensor([  101,   149, 12420,  2069, 27451, 11780, 12465, 13882, 12122, 14760,\n",
      "         8586,  1158,   131,  1130,  1142,  2076,  1104,  4579,   117,  1103,\n",
      "         3395,  3776,  9932,  1110,  1549,  1107,   794,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is given in • inputa corrupted example x˜ Rn obtainedbyan unknowncorruption process ∈ from a clean\n",
      "content_token: tensor([  101,  1110,  1549,  1107,   794,  7758,  1161, 14644,  1174,  1859,\n",
      "          100,   155,  1179,  3836,  2665,  1389,  3655, 19248, 20910,  2116,\n",
      "         1965,   850,  1121,   170,  4044,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∈ from a clean example x Rn. The learner must predict the clean example ∈ x from its corrupted\n",
      "content_token: tensor([  101,   850,  1121,   170,  4044,  1859,   193,   155,  1179,   119,\n",
      "         1109,  3858,  1200,  1538, 17163,  1103,  4044,  1859,   850,   193,\n",
      "         1121,  1157, 14644,  1174,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from its corrupted version x˜, or more generally predict the conditional probability distribution\n",
      "content_token: tensor([  101,  1121,  1157, 14644,  1174,  1683,   100,   117,  1137,  1167,\n",
      "         2412, 17163,  1103, 21152,  9750,  3735,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distribution p(x x˜). | Density estimation or probability mass function estimation: In • the\n",
      "content_token: tensor([  101,  3735,   185,   113,   193,   100,   114,   119,   197, 14760,\n",
      "        13730, 12890, 21517,  1137,  9750,  3367,  3053, 12890, 21517,   131,\n",
      "         1130,   794,  1103,   102])\n",
      "entity_list: ['Density estimation']\n",
      "entity_token: [tensor([14760, 13730, 12890, 21517])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In • the density estimation problem, the machine learning algorithm is asked to learn a function p\n",
      "content_token: tensor([  101,  1130,   794,  1103,  3476, 12890, 21517,  2463,   117,  1103,\n",
      "         3395,  3776,  9932,  1110,  1455,  1106,  3858,   170,  3053,   185,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learn a function p : Rn R, where p (x) can be interpreted model model → as a probability density\n",
      "content_token: tensor([ 101, 3858,  170, 3053,  185,  131,  155, 1179,  155,  117, 1187,  185,\n",
      "         113,  193,  114, 1169, 1129, 9829, 2235, 2235,  845, 1112,  170, 9750,\n",
      "        3476,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: probability density function (if x is continuous) or a probability mass function (if x is discrete)\n",
      "content_token: tensor([  101,  9750,  3476,  3053,   113,  1191,   193,  1110,  6803,   114,\n",
      "         1137,   170,  9750,  3367,  3053,   113,  1191,   193,  1110, 18535,\n",
      "          114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (if x is discrete) on the space that the examples were drawn from. To do such a task well (we will\n",
      "content_token: tensor([  101,   113,  1191,   193,  1110, 18535,   114,  1113,  1103,  2000,\n",
      "         1115,  1103,  5136,  1127,  3795,  1121,   119,  1706,  1202,  1216,\n",
      "          170,  4579,  1218,   113,  1195,  1209,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: task well (we will specify exactly what that means when we discuss performance measures P), the\n",
      "content_token: tensor([  101,  4579,  1218,   113,  1195,  1209, 22829,  2839,  1184,  1115,\n",
      "         2086,  1165,  1195,  6265,  2099,  5252,   153,   114,   117,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: measures P), the algorithm needs to learn the structure of the data it has seen. It must know where\n",
      "content_token: tensor([ 101, 5252,  153,  114,  117, 1103, 9932, 2993, 1106, 3858, 1103, 2401,\n",
      "        1104, 1103, 2233, 1122, 1144, 1562,  119, 1135, 1538, 1221, 1187,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: It must know where examples cluster tightly and where they are unlikely to occur. Most of the tasks\n",
      "content_token: tensor([  101,  1135,  1538,  1221,  1187,  5136, 10005,  6852,  1105,  1187,\n",
      "         1152,  1132,  9803,  1106,  4467,   119,  2082,  1104,  1103,  8249,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Most of the tasks described above require the learning algorithm to at least implicitly capture the\n",
      "content_token: tensor([  101,  2082,  1104,  1103,  8249,  1758,  1807,  4752,  1103,  3776,\n",
      "         9932,  1106,  1120,  1655, 24034, 18726,  1193,  4821,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: capture the structure of the probability distribution. Density estimation allows us to explicitly\n",
      "content_token: tensor([  101,  4821,  1103,  2401,  1104,  1103,  9750,  3735,   119, 14760,\n",
      "        13730, 12890, 21517,  3643,  1366,  1106, 12252,   102])\n",
      "entity_list: ['probability distribution', 'Density estimation']\n",
      "entity_token: [tensor([9750, 3735]), tensor([14760, 13730, 12890, 21517])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: us to explicitly capture that distribution. In principle, we can then perform computations on that\n",
      "content_token: tensor([  101,  1366,  1106, 12252,  4821,  1115,  3735,   119,  1130,  6708,\n",
      "          117,  1195,  1169,  1173,  3870,  3254, 19675,  1116,  1113,  1115,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on that distribution in order to solve the other tasks as well. For example, if we have performed\n",
      "content_token: tensor([ 101, 1113, 1115, 3735, 1107, 1546, 1106, 9474, 1103, 1168, 8249, 1112,\n",
      "        1218,  119, 1370, 1859,  117, 1191, 1195, 1138, 1982,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we have performed density estimation to obtain a probability distribution p(x), we can use that\n",
      "content_token: tensor([  101,  1195,  1138,  1982,  3476, 12890, 21517,  1106,  6268,   170,\n",
      "         9750,  3735,   185,   113,   193,   114,   117,  1195,  1169,  1329,\n",
      "         1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we can use that distribution to solve the missing value imputation task. If a value x is missing\n",
      "content_token: tensor([  101,  1195,  1169,  1329,  1115,  3735,  1106,  9474,  1103,  3764,\n",
      "         2860, 24034, 15012,  2116,  4579,   119,  1409,   170,  2860,   193,\n",
      "         1110,  3764,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: value x is missing and all of the other values, denoted x , are given, i i − then we know the\n",
      "content_token: tensor([  101,  2860,   193,  1110,  3764,  1105,  1155,  1104,  1103,  1168,\n",
      "         4718,   117, 21307,   193,   117,  1132,  1549,   117,   178,   178,\n",
      "          851,  1173,  1195,  1221,  1103,   102])\n",
      "entity_list: ['missing value imputation']\n",
      "entity_token: [tensor([ 3764,  2860, 24034, 15012,  2116])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: − then we know the distribution over it is given by p(x x ). In practice, i i | − density\n",
      "content_token: tensor([ 101,  851, 1173, 1195, 1221, 1103, 3735, 1166, 1122, 1110, 1549, 1118,\n",
      "         185,  113,  193,  193,  114,  119, 1130, 2415,  117,  178,  178,  197,\n",
      "         851, 3476,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: i i | − density estimation does not always allow us to solve all of these related tasks, because in\n",
      "content_token: tensor([  101,   178,   178,   197,   851,  3476, 12890, 21517,  1674,  1136,\n",
      "         1579,  2621,  1366,  1106,  9474,  1155,  1104,  1292,  2272,  8249,\n",
      "          117,  1272,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: tasks, because in many cases the required operations on p(x) are computationally intractable. Of\n",
      "content_token: tensor([  101,  8249,   117,  1272,  1107,  1242,  2740,  1103,  2320,  2500,\n",
      "         1113,   185,   113,   193,   114,  1132, 19903,  1193,  1107, 15017,\n",
      "         1895,   119,  2096,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: intractable. Of course, many other tasks and types of tasks are possible. The types of tasks we\n",
      "content_token: tensor([  101,  1107, 15017,  1895,   119,  2096,  1736,   117,  1242,  1168,\n",
      "         8249,  1105,  3322,  1104,  8249,  1132,  1936,   119,  1109,  3322,\n",
      "         1104,  8249,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: types of tasks we list here are intended only to provide examples of what machine learning can do,\n",
      "content_token: tensor([ 101, 3322, 1104, 8249, 1195, 2190, 1303, 1132, 3005, 1178, 1106, 2194,\n",
      "        5136, 1104, 1184, 3395, 3776, 1169, 1202,  117,  102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning can do, not to define a rigid taxonomy of tasks. P 5.1.2 The Performance Measure, In order\n",
      "content_token: tensor([  101,  3776,  1169,  1202,   117,  1136,  1106,  9410,   170, 12135,\n",
      "         3641, 19608,  1104,  8249,   119,   153,   126,   119,   122,   119,\n",
      "          123,  1109,  6724,  2508,  2225,  3313,   117,  1130,  1546,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Measure, In order to evaluate the abilities of a machine learning algorithm, we must design a\n",
      "content_token: tensor([  101,  2508,  2225,  3313,   117,  1130,  1546,  1106, 17459,  1103,\n",
      "         7134,  1104,   170,  3395,  3776,  9932,   117,  1195,  1538,  1902,\n",
      "          170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we must design a quantitative measure of its performance. Usually this performance measure P is\n",
      "content_token: tensor([  101,  1195,  1538,  1902,   170, 25220,  4929,  1104,  1157,  2099,\n",
      "          119, 12378,  1142,  2099,  4929,   153,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: measure P is specific to the task T being carried out by the system. For tasks such as\n",
      "content_token: tensor([ 101, 4929,  153, 1110, 2747, 1106, 1103, 4579,  157, 1217, 2446, 1149,\n",
      "        1118, 1103, 1449,  119, 1370, 8249, 1216, 1112,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: For tasks such as classification, classification with missing inputs, and tran- scription, we often\n",
      "content_token: tensor([  101,  1370,  8249,  1216,  1112,  5393,   117,  5393,  1114,  3764,\n",
      "        22743,   117,  1105,   189,  4047,   118,  5444,  1988,   117,  1195,\n",
      "         1510,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: scription, we often measure the accuracy of the model. Accuracy is just the proportion of examples\n",
      "content_token: tensor([  101,  5444,  1988,   117,  1195,  1510,  4929,  1103, 10893,  1104,\n",
      "         1103,  2235,   119,   138, 19515,  4084,  3457,  1110,  1198,  1103,\n",
      "        10807,  1104,  5136,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of examples for which the model produces the correct output. We can 103 CHAPTER 5. MACHINE LEARNING\n",
      "content_token: tensor([  101,  1104,  5136,  1111,  1134,  1103,  2235,  6570,  1103,  5663,\n",
      "         5964,   119,  1284,  1169,  9550,  8203,   126,   119, 25424,  3048,\n",
      "        11607,  2036,   149, 12420,  2069, 27451, 11780,   102])\n",
      "entity_list: ['input: of examples for which the model produces the correct output. We can 103 CHAPTER 5. MACHINE LEARNING \\n\\noutput: machine learning']\n",
      "entity_token: [tensor([ 7758,   131,  1104,  5136,  1111,  1134,  1103,  2235,  6570,  1103,\n",
      "         5663,  5964,   119,  1284,  1169,  9550,  8203,   126,   119, 25424,\n",
      "         3048, 11607,  2036,   149, 12420,  2069, 27451, 11780,  5964,   131,\n",
      "         3395,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 5. MACHINE LEARNING BASICS also obtain equivalent information by measuring the error rate, the\n",
      "content_token: tensor([  101,   126,   119, 25424,  3048, 11607,  2036,   149, 12420,  2069,\n",
      "        27451, 11780, 12465, 13882, 12122,  1145,  6268,  4976,  1869,  1118,\n",
      "        10099,  1103,  7353,  2603,   117,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the error rate, the proportion of examples for which the model produces an incorrect output. We\n",
      "content_token: tensor([  101,  1103,  7353,  2603,   117,  1103, 10807,  1104,  5136,  1111,\n",
      "         1134,  1103,  2235,  6570,  1126, 18238,  5964,   119,  1284,   102])\n",
      "entity_list: ['model']\n",
      "entity_token: [tensor([2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: output. We often refer to the error rate as the expected 0-1 loss. The 0-1 loss on a particular\n",
      "content_token: tensor([ 101, 5964,  119, 1284, 1510, 5991, 1106, 1103, 7353, 2603, 1112, 1103,\n",
      "        2637,  121,  118,  122, 2445,  119, 1109,  121,  118,  122, 2445, 1113,\n",
      "         170, 2440,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on a particular example is 0 if it is correctly classified and 1 if it is not. For tasks such as\n",
      "content_token: tensor([  101,  1113,   170,  2440,  1859,  1110,   121,  1191,  1122,  1110,\n",
      "        11214,  5667,  1105,   122,  1191,  1122,  1110,  1136,   119,  1370,\n",
      "         8249,  1216,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: For tasks such as density estimation, it does not make sense to measure accuracy, error rate, or\n",
      "content_token: tensor([  101,  1370,  8249,  1216,  1112,  3476, 12890, 21517,   117,  1122,\n",
      "         1674,  1136,  1294,  2305,  1106,  4929, 10893,   117,  7353,  2603,\n",
      "          117,  1137,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error rate, or any other kind of 0-1 loss. Instead, we must use a different performance metric that\n",
      "content_token: tensor([  101,  7353,  2603,   117,  1137,  1251,  1168,  1912,  1104,   121,\n",
      "          118,   122,  2445,   119,  3743,   117,  1195,  1538,  1329,   170,\n",
      "         1472,  2099, 12676,  1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: metric that gives the model a continuous-valued score for each example. The most common approach is\n",
      "content_token: tensor([  101, 12676,  1115,  3114,  1103,  2235,   170,  6803,   118, 11165,\n",
      "         2794,  1111,  1296,  1859,   119,  1109,  1211,  1887,  3136,  1110,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: common approach is to report the average log-probability the model assigns to some examples.\n",
      "content_token: tensor([  101,  1887,  3136,  1110,  1106,  2592,  1103,  1903,  9366,   118,\n",
      "         9750,  1103,  2235, 27043,  1106,  1199,  5136,   119,   102])\n",
      "entity_list: ['log-probability']\n",
      "entity_token: [tensor([9366,  118, 9750])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to some examples. Usually we are interested in how well the machine learning algorithm performs on\n",
      "content_token: tensor([  101,  1106,  1199,  5136,   119, 12378,  1195,  1132,  3888,  1107,\n",
      "         1293,  1218,  1103,  3395,  3776,  9932, 10383,  1113,   102])\n",
      "entity_list: ['machine learning algorithm']\n",
      "entity_token: [tensor([3395, 3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: performs on data that it has not seen before, since this determines how well it will work when\n",
      "content_token: tensor([  101, 10383,  1113,  2233,  1115,  1122,  1144,  1136,  1562,  1196,\n",
      "          117,  1290,  1142, 17579,  1293,  1218,  1122,  1209,  1250,  1165,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it will work when deployed in the real world. We therefore evaluate these performance measures\n",
      "content_token: tensor([  101,  1122,  1209,  1250,  1165,  6925,  1107,  1103,  1842,  1362,\n",
      "          119,  1284,  3335, 17459,  1292,  2099,  5252,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: measures using a test set of data that is separate from the data used for training the machine\n",
      "content_token: tensor([ 101, 5252, 1606,  170, 2774, 1383, 1104, 2233, 1115, 1110, 2767, 1121,\n",
      "        1103, 2233, 1215, 1111, 2013, 1103, 3395,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the machine learning system. The choice of performance measure may seem straightforward and\n",
      "content_token: tensor([  101,  1103,  3395,  3776,  1449,   119,  1109,  3026,  1104,  2099,\n",
      "         4929,  1336,  3166, 21546,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: straightforward and objective, but it is often difficult to choose a performance measure that\n",
      "content_token: tensor([  101, 21546,  1105,  7649,   117,  1133,  1122,  1110,  1510,  2846,\n",
      "         1106,  4835,   170,  2099,  4929,  1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: measure that corresponds well to the desired behavior of the system. In some cases, this is because\n",
      "content_token: tensor([  101,  4929,  1115, 15497,  1218,  1106,  1103,  8759,  4658,  1104,\n",
      "         1103,  1449,   119,  1130,  1199,  2740,   117,  1142,  1110,  1272,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this is because it is difficult to decide what should be measured. For example, when performing a\n",
      "content_token: tensor([ 101, 1142, 1110, 1272, 1122, 1110, 2846, 1106, 4958, 1184, 1431, 1129,\n",
      "        7140,  119, 1370, 1859,  117, 1165, 4072,  170,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: when performing a transcription task, should wemeasure the accuracy of the system at transcribing\n",
      "content_token: tensor([  101,  1165,  4072,   170, 15416,  4579,   117,  1431,  1195,  3263,\n",
      "         2225,  3313,  1103, 10893,  1104,  1103,  1449,  1120, 14715, 27362,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: at transcribing entire sequences, or should we use a more fine-grained performance measure that\n",
      "content_token: tensor([  101,  1120, 14715, 27362,  2072, 10028,   117,  1137,  1431,  1195,\n",
      "         1329,   170,  1167,  2503,   118,  9478,  1174,  2099,  4929,  1115,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: measure that gives partial credit for getting some elements of the sequence correct? When\n",
      "content_token: tensor([ 101, 4929, 1115, 3114, 7597, 4755, 1111, 2033, 1199, 3050, 1104, 1103,\n",
      "        4954, 5663,  136, 1332,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: correct? When performing a regression task, should we penalize the system more if it frequently\n",
      "content_token: tensor([  101,  5663,   136,  1332,  4072,   170,  1231, 24032,  4579,   117,\n",
      "         1431,  1195,  8228, 10584,  3171,  1103,  1449,  1167,  1191,  1122,\n",
      "         3933,   102])\n",
      "entity_list: ['regression task']\n",
      "entity_token: [tensor([ 1231, 24032,  4579])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: if it frequently makes medium-sized mistakes or if it rarely makes very large mistakes? These kinds\n",
      "content_token: tensor([  101,  1191,  1122,  3933,  2228,  5143,   118,  6956, 12572,  1137,\n",
      "         1191,  1122,  6034,  2228,  1304,  1415, 12572,   136,  1636,  7553,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: These kinds of design choices depend on the application. In other cases, we know what quantity we\n",
      "content_token: tensor([  101,  1636,  7553,  1104,  1902,  9940, 12864,  1113,  1103,  4048,\n",
      "          119,  1130,  1168,  2740,   117,  1195,  1221,  1184, 11978,  1195,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: what quantity we would ideally like to measure, but measuring it is impractical. For example, this\n",
      "content_token: tensor([  101,  1184, 11978,  1195,  1156,  7891,  1193,  1176,  1106,  4929,\n",
      "          117,  1133, 10099,  1122,  1110, 24034, 18890,   119,  1370,  1859,\n",
      "          117,  1142,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: For example, this arises frequently in the context of density estimation. Many of the best\n",
      "content_token: tensor([  101,  1370,  1859,   117,  1142, 20251,  3933,  1107,  1103,  5618,\n",
      "         1104,  3476, 12890, 21517,   119,  2408,  1104,  1103,  1436,   102])\n",
      "entity_list: ['density estimation']\n",
      "entity_token: [tensor([ 3476, 12890, 21517])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Many of the best probabilistic models represent probability distributions only implicitly.\n",
      "content_token: tensor([  101,  2408,  1104,  1103,  1436,  5250,  2822, 15197,  5562,  3584,\n",
      "         4248,  9750, 23190,  1178, 24034, 18726,  1193,   119,   102])\n",
      "entity_list: ['probabilistic models']\n",
      "entity_token: [tensor([ 5250,  2822, 15197,  5562,  3584])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: only implicitly. Computing the actual probability value assigned to a specific point in space in\n",
      "content_token: tensor([  101,  1178, 24034, 18726,  1193,   119, 20463,  1103,  4315,  9750,\n",
      "         2860,  3346,  1106,   170,  2747,  1553,  1107,  2000,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: point in space in many such models is intractable. In these cases, one must design an alternative\n",
      "content_token: tensor([  101,  1553,  1107,  2000,  1107,  1242,  1216,  3584,  1110,  1107,\n",
      "        15017,  1895,   119,  1130,  1292,  2740,   117,  1141,  1538,  1902,\n",
      "         1126,  4174,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: an alternative criterion that still corresponds to the design objectives, or design a good\n",
      "content_token: tensor([  101,  1126,  4174, 26440,  1115,  1253, 15497,  1106,  1103,  1902,\n",
      "        11350,   117,  1137,  1902,   170,  1363,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: or design a good approximation to the desired criterion. E 5.1.3 The Experience, Machine learning\n",
      "content_token: tensor([  101,  1137,  1902,   170,  1363, 22519,  1106,  1103,  8759, 26440,\n",
      "          119,   142,   126,   119,   122,   119,   124,  1109, 15843,   117,\n",
      "         7792,  3776,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Machine learning algorithms can be broadly categorized as unsupervised or supervised by what kind\n",
      "content_token: tensor([  101,  7792,  3776, 14975,  1169,  1129, 14548, 22429,  1112,  8362,\n",
      "         6385,  3365, 16641,  1181,  1137, 14199,  1118,  1184,  1912,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by what kind of experience they are allowed to have during the learning process. Most of the\n",
      "content_token: tensor([ 101, 1118, 1184, 1912, 1104, 2541, 1152, 1132, 2148, 1106, 1138, 1219,\n",
      "        1103, 3776, 1965,  119, 2082, 1104, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Most of the learning algorithms in this book can be understood as being allowed to experience an\n",
      "content_token: tensor([  101,  2082,  1104,  1103,  3776, 14975,  1107,  1142,  1520,  1169,\n",
      "         1129,  4628,  1112,  1217,  2148,  1106,  2541,  1126,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to experience an entire dataset. A dataset is a collection of many examples, as 104 CHAPTER 5.\n",
      "content_token: tensor([ 101, 1106, 2541, 1126, 2072, 2233, 9388,  119,  138, 2233, 9388, 1110,\n",
      "         170, 2436, 1104, 1242, 5136,  117, 1112, 9377, 8203,  126,  119,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as 104 CHAPTER 5. MACHINE LEARNING BASICS defined in section 5.1.1. Sometimes we will also call\n",
      "content_token: tensor([  101,  1112,  9377,  8203,   126,   119, 25424,  3048, 11607,  2036,\n",
      "          149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,  3393,  1107,\n",
      "         2237,   126,   119,   122,   119,   122,   119,  5875,  1195,  1209,\n",
      "         1145,  1840,   102])\n",
      "entity_list: ['MACHINE LEARNING BASICS']\n",
      "entity_token: [tensor([25424,  3048, 11607,  2036,   149, 12420,  2069, 27451, 11780, 12465,\n",
      "        13882, 12122])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we will also call examples data points. One of the oldest datasets studied by statisticians and\n",
      "content_token: tensor([  101,  1195,  1209,  1145,  1840,  5136,  2233,  1827,   119,  1448,\n",
      "         1104,  1103,  3778,  2233, 27948,  2376,  1118,   188, 19756,  5562,\n",
      "         5895,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: statisticians and machine learning re- searchers is the Iris dataset (Fisher, 1936). It is a\n",
      "content_token: tensor([  101,   188, 19756,  5562,  5895,  1105,  3395,  3776,  1231,   118,\n",
      "         3403,  1468,  1110,  1103, 13476,  2233,  9388,   113,  8476,   117,\n",
      "         3419,   114,   119,  1135,  1110,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 1936). It is a collection of measurements of different parts of 150 iris plants. Each individual\n",
      "content_token: tensor([  101,  3419,   114,   119,  1135,  1110,   170,  2436,  1104, 12307,\n",
      "         1104,  1472,  2192,  1104,  4214,   178,  4889,  3546,   119,  2994,\n",
      "         2510,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Each individual plant corresponds to one example. The features within each example are the\n",
      "content_token: tensor([  101,  2994,  2510,  2582, 15497,  1106,  1141,  1859,   119,  1109,\n",
      "         1956,  1439,  1296,  1859,  1132,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: example are the measurements of each of the parts of the plant: the sepal length, sepal width,\n",
      "content_token: tensor([  101,  1859,  1132,  1103, 12307,  1104,  1296,  1104,  1103,  2192,\n",
      "         1104,  1103,  2582,   131,  1103, 14516, 12320,  2251,   117, 14516,\n",
      "        12320,  9346,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sepal width, petal length and petal width. The dataset also records which species each plant\n",
      "content_token: tensor([  101, 14516, 12320,  9346,   117, 11109,  1348,  2251,  1105, 11109,\n",
      "         1348,  9346,   119,  1109,  2233,  9388,  1145,  3002,  1134,  1530,\n",
      "         1296,  2582,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: species each plant belonged to. Three different species are represented in the dataset.\n",
      "content_token: tensor([ 101, 1530, 1296, 2582, 5609, 1106,  119, 2677, 1472, 1530, 1132, 2533,\n",
      "        1107, 1103, 2233, 9388,  119,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in the dataset. Unsupervised learning algorithms experience a dataset containing many features,\n",
      "content_token: tensor([  101,  1107,  1103,  2233,  9388,   119, 12118,  6385,  3365, 16641,\n",
      "         1181,  3776, 14975,  2541,   170,  2233,  9388,  4051,  1242,  1956,\n",
      "          117,   102])\n",
      "entity_list: ['unsupervised learning algorithms']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: many features, then learn useful properties of the structure of this dataset. In the context of\n",
      "content_token: tensor([ 101, 1242, 1956,  117, 1173, 3858, 5616, 4625, 1104, 1103, 2401, 1104,\n",
      "        1142, 2233, 9388,  119, 1130, 1103, 5618, 1104,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In the context of deep learning, we usually want to learn the entire probability distribution that\n",
      "content_token: tensor([ 101, 1130, 1103, 5618, 1104, 1996, 3776,  117, 1195, 1932, 1328, 1106,\n",
      "        3858, 1103, 2072, 9750, 3735, 1115,  102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distribution that generated a dataset, whether explicitly as in density estimation or implicitly\n",
      "content_token: tensor([  101,  3735,  1115,  6455,   170,  2233,  9388,   117,  2480, 12252,\n",
      "         1112,  1107,  3476, 12890, 21517,  1137, 24034, 18726,  1193,   102])\n",
      "entity_list: ['density estimation']\n",
      "entity_token: [tensor([ 3476, 12890, 21517])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: or implicitly for tasks like synthesis or denoising. Some other unsupervised learning algorithms\n",
      "content_token: tensor([  101,  1137, 24034, 18726,  1193,  1111,  8249,  1176, 11362,  1137,\n",
      "        10552,  8586,  1158,   119,  1789,  1168,  8362,  6385,  3365, 16641,\n",
      "         1181,  3776, 14975,   102])\n",
      "entity_list: ['denoising', 'unsupervised learning algorithms']\n",
      "entity_token: [tensor([10552,  8586,  1158]), tensor([ 8362,  6385,  3365, 16641,  1181,  3776, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning algorithms perform other roles, like clustering, which consists of dividing the dataset\n",
      "content_token: tensor([  101,  3776, 14975,  3870,  1168,  3573,   117,  1176, 10005,  1158,\n",
      "          117,  1134,  2923,  1104, 18699,  1103,  2233,  9388,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the dataset into clusters of similar examples. Supervised learning algorithms experience a dataset\n",
      "content_token: tensor([  101,  1103,  2233,  9388,  1154, 13687,  1104,  1861,  5136,   119,\n",
      "         3198, 16641,  1181,  3776, 14975,  2541,   170,  2233,  9388,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a dataset containing features, but each example is also associated with a label or target. For\n",
      "content_token: tensor([ 101,  170, 2233, 9388, 4051, 1956,  117, 1133, 1296, 1859, 1110, 1145,\n",
      "        2628, 1114,  170, 3107, 1137, 4010,  119, 1370,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: or target. For example, the Iris dataset is annotated with the species of each iris plant. A\n",
      "content_token: tensor([  101,  1137,  4010,   119,  1370,  1859,   117,  1103, 13476,  2233,\n",
      "         9388,  1110,  1126, 12512,  2913,  1114,  1103,  1530,  1104,  1296,\n",
      "          178,  4889,  2582,   119,   138,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: each iris plant. A supervised learning algorithm can study the Iris dataset and learn to classify\n",
      "content_token: tensor([  101,  1296,   178,  4889,  2582,   119,   138, 14199,  3776,  9932,\n",
      "         1169,  2025,  1103, 13476,  2233,  9388,  1105,  3858,  1106,  1705,\n",
      "         6120,   102])\n",
      "entity_list: ['supervised learning algorithm']\n",
      "entity_token: [tensor([14199,  3776,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learn to classify iris plants into three different species based on their measurements. Roughly\n",
      "content_token: tensor([  101,  3858,  1106,  1705,  6120,   178,  4889,  3546,  1154,  1210,\n",
      "         1472,  1530,  1359,  1113,  1147, 12307,   119, 19479,  1193,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Roughly speaking, unsupervised learning involves observing several examples of a random vector x,\n",
      "content_token: tensor([  101, 19479,  1193,  3522,   117,  8362,  6385,  3365, 16641,  1181,\n",
      "         3776,  6808, 15639,  1317,  5136,  1104,   170,  7091,  9479,   193,\n",
      "          117,   102])\n",
      "entity_list: ['unsupervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a random vector x, and attempting to implicitly or explicitly learn the proba- bility distribution\n",
      "content_token: tensor([  101,   170,  7091,  9479,   193,   117,  1105,  6713,  1106, 24034,\n",
      "        18726,  1193,  1137, 12252,  3858,  1103,  5250,  2822,   118, 16516,\n",
      "        11796,  3735,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: bility distribution p(x), or some interesting properties of that distribution, while supervised\n",
      "content_token: tensor([  101, 16516, 11796,  3735,   185,   113,   193,   114,   117,  1137,\n",
      "         1199,  5426,  4625,  1104,  1115,  3735,   117,  1229, 14199,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: while supervised learning involves observing several examples of a random vector x and an\n",
      "content_token: tensor([  101,  1229, 14199,  3776,  6808, 15639,  1317,  5136,  1104,   170,\n",
      "         7091,  9479,   193,  1105,  1126,   102])\n",
      "entity_list: ['supervised learning']\n",
      "entity_token: [tensor([14199,  3776])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: vector x and an associated value or vector y, and learning to predict y from x, usually by\n",
      "content_token: tensor([  101,  9479,   193,  1105,  1126,  2628,  2860,  1137,  9479,   194,\n",
      "          117,  1105,  3776,  1106, 17163,   194,  1121,   193,   117,  1932,\n",
      "         1118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from x, usually by estimating p(y x). The term supervised learning originates from the view of |\n",
      "content_token: tensor([  101,  1121,   193,   117,  1932,  1118, 12890, 27182,   185,   113,\n",
      "          194,   193,   114,   119,  1109,  1858, 14199,  3776, 18025,  1121,\n",
      "         1103,  2458,  1104,   197,   102])\n",
      "entity_list: ['supervised learning']\n",
      "entity_token: [tensor([14199,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from the view of | the target y being provided by an instructor or teacher who shows the machine\n",
      "content_token: tensor([  101,  1121,  1103,  2458,  1104,   197,  1103,  4010,   194,  1217,\n",
      "         2136,  1118,  1126, 10332,  1137,  3218,  1150,  2196,  1103,  3395,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: shows the machine learning system what to do. In unsupervised learning, there is no instructor or\n",
      "content_token: tensor([  101,  2196,  1103,  3395,  3776,  1449,  1184,  1106,  1202,   119,\n",
      "         1130,  8362,  6385,  3365, 16641,  1181,  3776,   117,  1175,  1110,\n",
      "         1185, 10332,  1137,   102])\n",
      "entity_list: ['unsupervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is no instructor or teacher, and the algorithm must learn to make sense of the data without this\n",
      "content_token: tensor([  101,  1110,  1185, 10332,  1137,  3218,   117,  1105,  1103,  9932,\n",
      "         1538,  3858,  1106,  1294,  2305,  1104,  1103,  2233,  1443,  1142,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: data without this guide. Unsupervised learning and supervised learning are not formally defined\n",
      "content_token: tensor([  101,  2233,  1443,  1142,  6388,   119, 12118,  6385,  3365, 16641,\n",
      "         1181,  3776,  1105, 14199,  3776,  1132,  1136,  5708,  3393,   102])\n",
      "entity_list: ['unsupervised learning', 'supervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776]), tensor([14199,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: formally defined terms. The lines between them are often blurred. Many machine learning\n",
      "content_token: tensor([  101,  5708,  3393,  2538,   119,  1109,  2442,  1206,  1172,  1132,\n",
      "         1510, 20611,   119,  2408,  3395,  3776,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: machine learning technologies can be used to perform both tasks. For example, the chain rule of\n",
      "content_token: tensor([ 101, 3395, 3776, 7951, 1169, 1129, 1215, 1106, 3870, 1241, 8249,  119,\n",
      "        1370, 1859,  117, 1103, 4129, 3013, 1104,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the chain rule of probability states that for a vector x Rn, the joint distribution can be\n",
      "content_token: tensor([ 101, 1103, 4129, 3013, 1104, 9750, 2231, 1115, 1111,  170, 9479,  193,\n",
      "         155, 1179,  117, 1103, 4091, 3735, 1169, 1129,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distribution can be decomposed as ∈ n p(x) = p(x x ,...,x ). (5.1) i 1 i 1 | − i=1  This\n",
      "content_token: tensor([  101,  3735,  1169,  1129,  1260,  8178, 13541,  1112,   850,   183,\n",
      "          185,   113,   193,   114,   134,   185,   113,   193,   193,   117,\n",
      "          119,   119,   119,   117,   193,   114,   119,   113,   126,   119,\n",
      "          122,   114,   178,   122,   178,   122,   197,   851,   178,   134,\n",
      "          122,  1188,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: i 1 | − i=1  This decomposition means that we can solvethe ostensibly unsupervised problem of\n",
      "content_token: tensor([  101,   178,   122,   197,   851,   178,   134,   122,  1188, 25898,\n",
      "         2086,  1115,  1195,  1169,  9474, 10681, 28029,  8362,  6385,  3365,\n",
      "        16641,  1181,  2463,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: problem of modeling p(x) by splitting it into n supervised learning problems. Alternatively, we 105\n",
      "content_token: tensor([  101,  2463,  1104, 13117,   185,   113,   193,   114,  1118, 15601,\n",
      "         1122,  1154,   183, 14199,  3776,  2645,   119, 23104,   117,  1195,\n",
      "         8359,   102])\n",
      "entity_list: ['supervised learning problems']\n",
      "entity_token: [tensor([14199,  3776,  2645])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we 105 CHAPTER 5. MACHINE LEARNING BASICS can solve the supervised learning problem of learning p(y\n",
      "content_token: tensor([  101,  1195,  8359,  8203,   126,   119, 25424,  3048, 11607,  2036,\n",
      "          149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,  1169,  9474,\n",
      "         1103, 14199,  3776,  2463,  1104,  3776,   185,   113,   194,   102])\n",
      "entity_list: ['supervised learning problem']\n",
      "entity_token: [tensor([14199,  3776,  2463])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of learning p(y x) by using traditional | unsupervised learning technologies to learn the joint\n",
      "content_token: tensor([  101,  1104,  3776,   185,   113,   194,   193,   114,  1118,  1606,\n",
      "         2361,   197,  8362,  6385,  3365, 16641,  1181,  3776,  7951,  1106,\n",
      "         3858,  1103,  4091,   102])\n",
      "entity_list: ['unsupervised learning technologies']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776,  7951])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to learn the joint distribution p(x,y) and inferring p(x,y) p(y x) = . (5.2) | p(x,y ) y   Though\n",
      "content_token: tensor([ 101, 1106, 3858, 1103, 4091, 3735,  185,  113,  193,  117,  194,  114,\n",
      "        1105, 1107, 6732, 3384,  185,  113,  193,  117,  194,  114,  185,  113,\n",
      "         194,  193,  114,  134,  119,  113,  126,  119,  123,  114,  197,  185,\n",
      "         113,  193,  117,  194,  114,  194, 3473,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ) y   Though unsupervised learning and supervised learning are not completely formal or distinct\n",
      "content_token: tensor([  101,   114,   194,  3473,  8362,  6385,  3365, 16641,  1181,  3776,\n",
      "         1105, 14199,  3776,  1132,  1136,  2423,  4698,  1137,  4966,   102])\n",
      "entity_list: ['unsupervised learning', 'supervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776]), tensor([14199,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: formal or distinct concepts, they do help to roughly categorize some of the things we do with\n",
      "content_token: tensor([  101,  4698,  1137,  4966,  8550,   117,  1152,  1202,  1494,  1106,\n",
      "         4986,  5855, 23820, 28021,  1162,  1199,  1104,  1103,  1614,  1195,\n",
      "         1202,  1114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: things we do with machine learning algorithms. Traditionally, people refer to regression,\n",
      "content_token: tensor([  101,  1614,  1195,  1202,  1114,  3395,  3776, 14975,   119, 19324,\n",
      "          117,  1234,  5991,  1106,  1231, 24032,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to regression, classification and structured output problems as supervised learning. Density\n",
      "content_token: tensor([  101,  1106,  1231, 24032,   117,  5393,  1105, 15695,  5964,  2645,\n",
      "         1112, 14199,  3776,   119, 14760, 13730,   102])\n",
      "entity_list: ['structured output problems']\n",
      "entity_token: [tensor([15695,  5964,  2645])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning. Density estimation in support of other tasks is usually considered unsupervised learning.\n",
      "content_token: tensor([  101,  3776,   119, 14760, 13730, 12890, 21517,  1107,  1619,  1104,\n",
      "         1168,  8249,  1110,  1932,  1737,  8362,  6385,  3365, 16641,  1181,\n",
      "         3776,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning. Other variants of the learning paradigm are possible. For example, in semi- supervised\n",
      "content_token: tensor([  101,  3776,   119,  2189, 10317,  1104,  1103,  3776, 26213,  1132,\n",
      "         1936,   119,  1370,  1859,   117,  1107,  3533,   118, 14199,   102])\n",
      "entity_list: ['semi-supervised']\n",
      "entity_token: [tensor([ 3533,   118, 14199])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in semi- supervised learning, some examples include a supervision target but others do not. In\n",
      "content_token: tensor([  101,  1107,  3533,   118, 14199,  3776,   117,  1199,  5136,  1511,\n",
      "          170, 10955,  4010,  1133,  1639,  1202,  1136,   119,  1130,   102])\n",
      "entity_list: ['semi-supervised learning']\n",
      "entity_token: [tensor([ 3533,   118, 14199,  3776])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: others do not. In multi-instance learning, an entire collection of examples is labeled as\n",
      "content_token: tensor([  101,  1639,  1202,  1136,   119,  1130,  4321,   118,  5374,  3776,\n",
      "          117,  1126,  2072,  2436,  1104,  5136,  1110, 12893,  1112,   102])\n",
      "entity_list: ['multi-instance learning']\n",
      "entity_token: [tensor([4321,  118, 5374, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is labeled as containing or not containing an example of a class, but the individual members of the\n",
      "content_token: tensor([  101,  1110, 12893,  1112,  4051,  1137,  1136,  4051,  1126,  1859,\n",
      "         1104,   170,  1705,   117,  1133,  1103,  2510,  1484,  1104,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: members of the collection are not labeled. For a recent example of multi-instance learning with\n",
      "content_token: tensor([  101,  1484,  1104,  1103,  2436,  1132,  1136, 12893,   119,  1370,\n",
      "          170,  2793,  1859,  1104,  4321,   118,  5374,  3776,  1114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning with deep models, see Kotzias et al. (2015). Some machine learning algorithms do not just\n",
      "content_token: tensor([  101,  3776,  1114,  1996,  3584,   117,  1267, 19892,  5745,  7346,\n",
      "         3084,  2393,   119,   113,  1410,   114,   119,  1789,  3395,  3776,\n",
      "        14975,  1202,  1136,  1198,   102])\n",
      "entity_list: ['deep models']\n",
      "entity_token: [tensor([1996, 3584])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: do not just experience a fixed dataset. For example, reinforcement learning algorithms interact\n",
      "content_token: tensor([  101,  1202,  1136,  1198,  2541,   170,  4275,  2233,  9388,   119,\n",
      "         1370,  1859,   117, 21293,  1880,  3776, 14975, 12254,   102])\n",
      "entity_list: ['reinforcement learning algorithms']\n",
      "entity_token: [tensor([21293,  1880,  3776, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithms interact with an environment, so there is a feedback loop between the learning system\n",
      "content_token: tensor([  101, 14975, 12254,  1114,  1126,  3750,   117,  1177,  1175,  1110,\n",
      "          170, 13032,  7812,  1206,  1103,  3776,  1449,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the learning system and its experiences. Such algorithms are beyond the scope of this book. Please\n",
      "content_token: tensor([  101,  1103,  3776,  1449,  1105,  1157,  5758,   119,  5723, 14975,\n",
      "         1132,  2894,  1103,  9668,  1104,  1142,  1520,   119,  4203,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this book. Please see Sutton and Barto (1998) or Bertsekas and Tsitsiklis (1996) for information\n",
      "content_token: tensor([  101,  1142,  1520,   119,  4203,  1267, 11163,  1105, 13045,  1186,\n",
      "          113,  1772,   114,  1137, 15035,  2217, 13257,  1105,   157,  5053,\n",
      "         2145,  4847,  6137,   113,  1820,   114,  1111,  1869,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for information about reinforcement learning, and Mnih et al. (2013) for the deep learning approach\n",
      "content_token: tensor([  101,  1111,  1869,  1164, 21293,  1880,  3776,   117,  1105,   150,\n",
      "         2605,  1324,  3084,  2393,   119,   113,  1381,   114,  1111,  1103,\n",
      "         1996,  3776,  3136,   102])\n",
      "entity_list: ['deep learning', 'reinforcement learning']\n",
      "entity_token: [tensor([1996, 3776]), tensor([21293,  1880,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning approach to reinforcement learning. Most machine learning algorithms simply experience a\n",
      "content_token: tensor([  101,  3776,  3136,  1106, 21293,  1880,  3776,   119,  2082,  3395,\n",
      "         3776, 14975,  2566,  2541,   170,   102])\n",
      "entity_list: ['reinforcement learning', 'machine learning', 'learning algorithms']\n",
      "entity_token: [tensor([21293,  1880,  3776]), tensor([3395, 3776]), tensor([ 3776, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 2, 2, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: simply experience a dataset. A dataset can be described in many ways. In all cases, a dataset is a\n",
      "content_token: tensor([ 101, 2566, 2541,  170, 2233, 9388,  119,  138, 2233, 9388, 1169, 1129,\n",
      "        1758, 1107, 1242, 3242,  119, 1130, 1155, 2740,  117,  170, 2233, 9388,\n",
      "        1110,  170,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a dataset is a collection of examples, which are in turn collections of features. One common way of\n",
      "content_token: tensor([ 101,  170, 2233, 9388, 1110,  170, 2436, 1104, 5136,  117, 1134, 1132,\n",
      "        1107, 1885, 6286, 1104, 1956,  119, 1448, 1887, 1236, 1104,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: One common way of describing a dataset is with a design matrix. A design matrix is a matrix\n",
      "content_token: tensor([ 101, 1448, 1887, 1236, 1104, 7645,  170, 2233, 9388, 1110, 1114,  170,\n",
      "        1902, 8952,  119,  138, 1902, 8952, 1110,  170, 8952,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: matrix is a matrix containing a different example in each row. Each column of the matrix\n",
      "content_token: tensor([ 101, 8952, 1110,  170, 8952, 4051,  170, 1472, 1859, 1107, 1296, 5105,\n",
      "         119, 2994, 5551, 1104, 1103, 8952,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the matrix corresponds to a different feature. For instance, the Iris dataset contains 150\n",
      "content_token: tensor([  101,  1104,  1103,  8952, 15497,  1106,   170,  1472,  2672,   119,\n",
      "         1370,  5374,   117,  1103, 13476,  2233,  9388,  2515,  4214,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: contains 150 examples with four features for each example. This means we can represent the dataset\n",
      "content_token: tensor([ 101, 2515, 4214, 5136, 1114, 1300, 1956, 1111, 1296, 1859,  119, 1188,\n",
      "        2086, 1195, 1169, 4248, 1103, 2233, 9388,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the dataset with a design matrix X R150 4, where X is the sepal length of × i,1 ∈ plant i, X is the\n",
      "content_token: tensor([  101,  1103,  2233,  9388,  1114,   170,  1902,  8952,   161,   155,\n",
      "        16337,  1568,   125,   117,  1187,   161,  1110,  1103, 14516, 12320,\n",
      "         2251,  1104,   240,   178,   117,   122,   850,  2582,   178,   117,\n",
      "          161,  1110,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∈ plant i, X is the sepal width of plant i, etc. We will describe most of the learning i,2\n",
      "content_token: tensor([  101,   850,  2582,   178,   117,   161,  1110,  1103, 14516, 12320,\n",
      "         9346,  1104,  2582,   178,   117,  3576,   119,  1284,  1209,  5594,\n",
      "         1211,  1104,  1103,  3776,   178,   117,   123,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the learning i,2 algorithms in this book in terms of how they operate on design matrix datasets.\n",
      "content_token: tensor([  101,  1104,  1103,  3776,   178,   117,   123, 14975,  1107,  1142,\n",
      "         1520,  1107,  2538,  1104,  1293,  1152,  4732,  1113,  1902,  8952,\n",
      "         2233, 27948,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: matrix datasets. Of course, to describe a dataset as a design matrix, it must be possible to\n",
      "content_token: tensor([  101,  8952,  2233, 27948,   119,  2096,  1736,   117,  1106,  5594,\n",
      "          170,  2233,  9388,  1112,   170,  1902,  8952,   117,  1122,  1538,\n",
      "         1129,  1936,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: must be possible to describe each example as a vector, and each of these vectors must be the same\n",
      "content_token: tensor([  101,  1538,  1129,  1936,  1106,  5594,  1296,  1859,  1112,   170,\n",
      "         9479,   117,  1105,  1296,  1104,  1292, 21118,  1538,  1129,  1103,\n",
      "         1269,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: must be the same size. This is not always possible. For example, if you have a collection of\n",
      "content_token: tensor([ 101, 1538, 1129, 1103, 1269, 2060,  119, 1188, 1110, 1136, 1579, 1936,\n",
      "         119, 1370, 1859,  117, 1191, 1128, 1138,  170, 2436, 1104,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a collection of photographs with different widths and heights, then different photographs will\n",
      "content_token: tensor([  101,   170,  2436,  1104,  6810,  1114,  1472,  9346,  1116,  1105,\n",
      "        16291,   117,  1173,  1472,  6810,  1209,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: photographs will contain different numbers of pixels, so not all of the photographs may be\n",
      "content_token: tensor([  101,  6810,  1209,  4651,  1472,  2849,  1104,   185, 28076,  1116,\n",
      "          117,  1177,  1136,  1155,  1104,  1103,  6810,  1336,  1129,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: photographs may be described with the same length of vector. Section 9.7 and chapter 10 describe\n",
      "content_token: tensor([ 101, 6810, 1336, 1129, 1758, 1114, 1103, 1269, 2251, 1104, 9479,  119,\n",
      "        6177,  130,  119,  128, 1105, 6073, 1275, 5594,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: chapter 10 describe how to handle different 106 CHAPTER 5. MACHINE LEARNING BASICS types of such\n",
      "content_token: tensor([  101,  6073,  1275,  5594,  1293,  1106,  4282,  1472,  9920,  8203,\n",
      "          126,   119, 25424,  3048, 11607,  2036,   149, 12420,  2069, 27451,\n",
      "        11780, 12465, 13882, 12122,  3322,  1104,  1216,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: types of such heterogeneous data. In cases like these, rather than describing the dataset as a\n",
      "content_token: tensor([  101,  3322,  1104,  1216,  1119, 25710, 27054,  2285,  2233,   119,\n",
      "         1130,  2740,  1176,  1292,   117,  1897,  1190,  7645,  1103,  2233,\n",
      "         9388,  1112,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the dataset as a matrix with m rows, we will describe it as a set containing m elements:\n",
      "content_token: tensor([  101,  1103,  2233,  9388,  1112,   170,  8952,  1114,   182, 10389,\n",
      "          117,  1195,  1209,  5594,  1122,  1112,   170,  1383,  4051,   182,\n",
      "         3050,   131,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: m elements: x(1),x(2),...,x(m) . This notation does not imply that any two example vectors { } x(i)\n",
      "content_token: tensor([  101,   182,  3050,   131,   193,   113,   122,   114,   117,   193,\n",
      "          113,   123,   114,   117,   119,   119,   119,   117,   193,   113,\n",
      "          182,   114,   119,  1188, 16049,  1674,  1136, 21276,  1115,  1251,\n",
      "         1160,  1859, 21118,   196,   198,   193,   113,   178,   114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: vectors { } x(i) and x(j) have the same size. In the case of supervised learning, the example\n",
      "content_token: tensor([  101, 21118,   196,   198,   193,   113,   178,   114,  1105,   193,\n",
      "          113,   179,   114,  1138,  1103,  1269,  2060,   119,  1130,  1103,\n",
      "         1692,  1104, 14199,  3776,   117,  1103,  1859,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the example contains a label or target as well as a collection of features. For example, if we want\n",
      "content_token: tensor([ 101, 1103, 1859, 2515,  170, 3107, 1137, 4010, 1112, 1218, 1112,  170,\n",
      "        2436, 1104, 1956,  119, 1370, 1859,  117, 1191, 1195, 1328,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: example, if we want to use a learning algorithm to perform object recognition from photographs, we\n",
      "content_token: tensor([ 101, 1859,  117, 1191, 1195, 1328, 1106, 1329,  170, 3776, 9932, 1106,\n",
      "        3870, 4231, 4453, 1121, 6810,  117, 1195,  102])\n",
      "entity_list: ['object recognition']\n",
      "entity_token: [tensor([4231, 4453])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: photographs, we need to specify which object appears in each of the photos. We might do this with a\n",
      "content_token: tensor([  101,  6810,   117,  1195,  1444,  1106, 22829,  1134,  4231,  2691,\n",
      "         1107,  1296,  1104,  1103,  7630,   119,  1284,  1547,  1202,  1142,\n",
      "         1114,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: do this with a numeric code, with 0 signifying a person, 1 signifying a car, 2 signifying a cat,\n",
      "content_token: tensor([  101,  1202,  1142,  1114,   170,   183, 15447,  4907,  3463,   117,\n",
      "         1114,   121,  2951,  8985,   170,  1825,   117,   122,  2951,  8985,\n",
      "          170,  1610,   117,   123,  2951,  8985,   170,  5855,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 2 signifying a cat, etc. Often when working with a dataset containing a design matrix of feature\n",
      "content_token: tensor([  101,   123,  2951,  8985,   170,  5855,   117,  3576,   119, 12812,\n",
      "         1165,  1684,  1114,   170,  2233,  9388,  4051,   170,  1902,  8952,\n",
      "         1104,  2672,   102])\n",
      "entity_list: ['design matrix']\n",
      "entity_token: [tensor([1902, 8952])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: matrix of feature observations X, we also provide a vector of labels y, with y providing the label\n",
      "content_token: tensor([  101,  8952,  1104,  2672,  9959,   161,   117,  1195,  1145,  2194,\n",
      "          170,  9479,  1104, 11080,   194,   117,  1114,   194,  3558,  1103,\n",
      "         3107,   102])\n",
      "entity_list: ['design matrix']\n",
      "entity_token: [tensor([1902, 8952])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: providing the label for example i. i Of course, sometimes the label may be more than just a single\n",
      "content_token: tensor([ 101, 3558, 1103, 3107, 1111, 1859,  178,  119,  178, 2096, 1736,  117,\n",
      "        2121, 1103, 3107, 1336, 1129, 1167, 1190, 1198,  170, 1423,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: than just a single number. For example, if we want to train a speech recognition system to\n",
      "content_token: tensor([ 101, 1190, 1198,  170, 1423, 1295,  119, 1370, 1859,  117, 1191, 1195,\n",
      "        1328, 1106, 2669,  170, 4055, 4453, 1449, 1106,  102])\n",
      "entity_list: ['speech recognition system']\n",
      "entity_token: [tensor([4055, 4453, 1449])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: system to transcribe entire sentences, then the label for each example sentence is a sequence of\n",
      "content_token: tensor([  101,  1449,  1106, 14715, 17770,  2072, 12043,   117,  1173,  1103,\n",
      "         3107,  1111,  1296,  1859,  5650,  1110,   170,  4954,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is a sequence of words. Just as there is no formal definition of supervised and unsupervised\n",
      "content_token: tensor([  101,  1110,   170,  4954,  1104,  1734,   119,  2066,  1112,  1175,\n",
      "         1110,  1185,  4698,  5754,  1104, 14199,  1105,  8362,  6385,  3365,\n",
      "        16641,  1181,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and unsupervised learning, there is no rigid taxonomy of datasets or experiences. The structures\n",
      "content_token: tensor([  101,  1105,  8362,  6385,  3365, 16641,  1181,  3776,   117,  1175,\n",
      "         1110,  1185, 12135,  3641, 19608,  1104,  2233, 27948,  1137,  5758,\n",
      "          119,  1109,  4413,   102])\n",
      "entity_list: ['supervised learning', 'unsupervised learning']\n",
      "entity_token: [tensor([14199,  3776]), tensor([ 8362,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The structures described here cover most cases, but it is always possible to design new ones for\n",
      "content_token: tensor([ 101, 1109, 4413, 1758, 1303, 2267, 1211, 2740,  117, 1133, 1122, 1110,\n",
      "        1579, 1936, 1106, 1902, 1207, 3200, 1111,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: design new ones for new applications. 5.1.4 Example: Linear Regression Our definition of a machine\n",
      "content_token: tensor([  101,  1902,  1207,  3200,  1111,  1207,  4683,   119,   126,   119,\n",
      "          122,   119,   125, 16409, 26671,   131,  2800,  1813, 23287, 26779,\n",
      "         3458,  5754,  1104,   170,  3395,   102])\n",
      "entity_list: ['Linear Regression']\n",
      "entity_token: [tensor([ 2800,  1813, 23287, 26779])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of a machine learning algorithm as an algorithm that is capable of improving a computer program’s\n",
      "content_token: tensor([ 101, 1104,  170, 3395, 3776, 9932, 1112, 1126, 9932, 1115, 1110, 4451,\n",
      "        1104, 9248,  170, 2775, 1788,  787,  188,  102])\n",
      "entity_list: ['machine learning algorithm']\n",
      "entity_token: [tensor([3395, 3776, 9932])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: computer program’s performance at some task via experience is somewhat abstract. To make this more\n",
      "content_token: tensor([  101,  2775,  1788,   787,   188,  2099,  1120,  1199,  4579,  2258,\n",
      "         2541,  1110,  4742, 11108,   119,  1706,  1294,  1142,  1167,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: To make this more concrete, we present an example of a simple machine learning algorithm: linear\n",
      "content_token: tensor([ 101, 1706, 1294, 1142, 1167, 5019,  117, 1195, 1675, 1126, 1859, 1104,\n",
      "         170, 3014, 3395, 3776, 9932,  131, 7378,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithm: linear regression. We will return to this example repeatedly as we introduce more\n",
      "content_token: tensor([  101,  9932,   131,  7378,  1231, 24032,   119,  1284,  1209,  1862,\n",
      "         1106,  1142,  1859,  8038,  1112,  1195,  8698,  1167,   102])\n",
      "entity_list: ['linear regression algorithm']\n",
      "entity_token: [tensor([ 7378,  1231, 24032,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we introduce more machine learning concepts that help to understand its behavior. As the name\n",
      "content_token: tensor([ 101, 1195, 8698, 1167, 3395, 3776, 8550, 1115, 1494, 1106, 2437, 1157,\n",
      "        4658,  119, 1249, 1103, 1271,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: As the name implies, linear regression solves a regression problem. In other words, the goal is to\n",
      "content_token: tensor([  101,  1249,  1103,  1271, 12942,   117,  7378,  1231, 24032,  9474,\n",
      "         1116,   170,  1231, 24032,  2463,   119,  1130,  1168,  1734,   117,\n",
      "         1103,  2273,  1110,  1106,   102])\n",
      "entity_list: ['linear regression']\n",
      "entity_token: [tensor([ 7378,  1231, 24032])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the goal is to build a system that can take a vector x Rn as input and ∈ predict the value of a\n",
      "content_token: tensor([  101,  1103,  2273,  1110,  1106,  3076,   170,  1449,  1115,  1169,\n",
      "         1321,   170,  9479,   193,   155,  1179,  1112,  7758,  1105,   850,\n",
      "        17163,  1103,  2860,  1104,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the value of a scalar y R as its output. In the case of linear regression, ∈ the output is a linear\n",
      "content_token: tensor([  101,  1103,  2860,  1104,   170,   188,  7867,  1813,   194,   155,\n",
      "         1112,  1157,  5964,   119,  1130,  1103,  1692,  1104,  7378,  1231,\n",
      "        24032,   117,   850,  1103,  5964,  1110,   170,  7378,   102])\n",
      "entity_list: ['linear regression']\n",
      "entity_token: [tensor([ 7378,  1231, 24032])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: output is a linear function of the input. Let yˆ be the value that our model predicts y should take\n",
      "content_token: tensor([  101,  5964,  1110,   170,  7378,  3053,  1104,  1103,  7758,   119,\n",
      "         2421,   100,  1129,  1103,  2860,  1115,  1412,  2235, 17163,  1116,\n",
      "          194,  1431,  1321,   102])\n",
      "entity_list: ['model']\n",
      "entity_token: [tensor([2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: y should take on. We define the output to be yˆ= w x (5.3)  where w Rn is a vector of parameters.\n",
      "content_token: tensor([  101,   194,  1431,  1321,  1113,   119,  1284,  9410,  1103,  5964,\n",
      "         1106,  1129,   100,   134,   192,   193,   113,   126,   119,   124,\n",
      "          114,  1187,   192,   155,  1179,  1110,   170,  9479,  1104, 11934,\n",
      "          119,   102])\n",
      "entity_list: ['y_hat']\n",
      "entity_token: [tensor([ 194,  168, 6131])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of parameters. ∈ Parameters are values that control the behaviorof the system. In this case, w is i\n",
      "content_token: tensor([  101,  1104, 11934,   119,   850, 23994, 19401,  1116,  1132,  4718,\n",
      "         1115,  1654,  1103,  4658, 10008,  1103,  1449,   119,  1130,  1142,\n",
      "         1692,   117,   192,  1110,   178,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this case, w is i the coefficient that we multiply by feature x before summing up the contributions\n",
      "content_token: tensor([  101,  1142,  1692,   117,   192,  1110,   178,  1103, 21130,  1115,\n",
      "         1195,  4321,  1643,  1193,  1118,  2672,   193,  1196,  7584,  5031,\n",
      "         1146,  1103,  5353,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the contributions i from all the features. We can think of w as a set of weights that determine how\n",
      "content_token: tensor([  101,  1103,  5353,   178,  1121,  1155,  1103,  1956,   119,  1284,\n",
      "         1169,  1341,  1104,   192,  1112,   170,  1383,  1104, 17981,  1115,\n",
      "         4959,  1293,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that determine how each feature affects the prediction. If a feature x receives a positive weight w\n",
      "content_token: tensor([  101,  1115,  4959,  1293,  1296,  2672, 13974,  1103, 20770,   119,\n",
      "         1409,   170,  2672,   193,  7881,   170,  3112,  2841,   192,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a positive weight w , i i 107 CHAPTER 5. MACHINE LEARNING BASICS then increasing the value of that\n",
      "content_token: tensor([  101,   170,  3112,  2841,   192,   117,   178,   178, 10428,  8203,\n",
      "          126,   119, 25424,  3048, 11607,  2036,   149, 12420,  2069, 27451,\n",
      "        11780, 12465, 13882, 12122,  1173,  4138,  1103,  2860,  1104,  1115,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the value of that feature increases the value of our prediction yˆ. If a feature receives a\n",
      "content_token: tensor([  101,  1103,  2860,  1104,  1115,  2672,  6986,  1103,  2860,  1104,\n",
      "         1412, 20770,   100,   119,  1409,   170,  2672,  7881,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: feature receives a negative weight, then increasing the value of that feature decreases the value\n",
      "content_token: tensor([  101,  2672,  7881,   170,  4366,  2841,   117,  1173,  4138,  1103,\n",
      "         2860,  1104,  1115,  2672, 19377,  1103,  2860,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: decreases the value of our prediction. If a feature’s weight is large in magnitude, then it has a\n",
      "content_token: tensor([  101, 19377,  1103,  2860,  1104,  1412, 20770,   119,  1409,   170,\n",
      "         2672,   787,   188,  2841,  1110,  1415,  1107, 10094,   117,  1173,\n",
      "         1122,  1144,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: then it has a large effect on the prediction. If a feature’s weight is zero, it has no effect on\n",
      "content_token: tensor([  101,  1173,  1122,  1144,   170,  1415,  2629,  1113,  1103, 20770,\n",
      "          119,  1409,   170,  2672,   787,   188,  2841,  1110,  6756,   117,\n",
      "         1122,  1144,  1185,  2629,  1113,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it has no effect on the prediction. We thus have a definition of our task T: to predict y from x by\n",
      "content_token: tensor([  101,  1122,  1144,  1185,  2629,  1113,  1103, 20770,   119,  1284,\n",
      "         2456,  1138,   170,  5754,  1104,  1412,  4579,   157,   131,  1106,\n",
      "        17163,   194,  1121,   193,  1118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: predict y from x by outputting yˆ= w x. Next we need a definition of our performance measure, P. \n",
      "content_token: tensor([  101, 17163,   194,  1121,   193,  1118,  5964,  1916,   100,   134,\n",
      "          192,   193,   119,  5893,  1195,  1444,   170,  5754,  1104,  1412,\n",
      "         2099,  4929,   117,   153,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: measure, P.  Suppose that we have a design matrix of m example inputs that we will not use for\n",
      "content_token: tensor([  101,  4929,   117,   153,   119, 15463,  8661,  6787,  1115,  1195,\n",
      "         1138,   170,  1902,  8952,  1104,   182,  1859, 22743,  1115,  1195,\n",
      "         1209,  1136,  1329,  1111,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we will not use for training, only for evaluating how well the model performs. We also have a\n",
      "content_token: tensor([  101,  1195,  1209,  1136,  1329,  1111,  2013,   117,  1178,  1111,\n",
      "        27698,  1293,  1218,  1103,  2235, 10383,   119,  1284,  1145,  1138,\n",
      "          170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: We also have a vector of regression targets providing the correct value of y for each of these\n",
      "content_token: tensor([  101,  1284,  1145,  1138,   170,  9479,  1104,  1231, 24032,  7539,\n",
      "         3558,  1103,  5663,  2860,  1104,   194,  1111,  1296,  1104,  1292,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: y for each of these examples. Because this dataset will only be used for evaluation, we call it the\n",
      "content_token: tensor([  101,   194,  1111,  1296,  1104,  1292,  5136,   119,  2279,  1142,\n",
      "         2233,  9388,  1209,  1178,  1129,  1215,  1111, 10540,   117,  1195,\n",
      "         1840,  1122,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we call it the test set. We refer to the design matrix of inputs as X(test) and the vector of\n",
      "content_token: tensor([  101,  1195,  1840,  1122,  1103,  2774,  1383,   119,  1284,  5991,\n",
      "         1106,  1103,  1902,  8952,  1104, 22743,  1112,   161,   113,  2774,\n",
      "          114,  1105,  1103,  9479,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and the vector of regression targets as y(test). One way of measuring the performance of the model\n",
      "content_token: tensor([  101,  1105,  1103,  9479,  1104,  1231, 24032,  7539,  1112,   194,\n",
      "          113,  2774,   114,   119,  1448,  1236,  1104, 10099,  1103,  2099,\n",
      "         1104,  1103,  2235,   102])\n",
      "entity_list: [\"'model'\"]\n",
      "entity_token: [tensor([ 112, 2235,  112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the model is to compute the mean squared error of the model on the test set. If yˆ(test) gives\n",
      "content_token: tensor([  101,  1104,  1103,  2235,  1110,  1106,  3254, 22662,  1103,  1928,\n",
      "        23215,  7353,  1104,  1103,  2235,  1113,  1103,  2774,  1383,   119,\n",
      "         1409,   100,   113,  2774,   114,  3114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: If yˆ(test) gives the predictions of the model on the test set, then the mean squared error is\n",
      "content_token: tensor([  101,  1409,   100,   113,  2774,   114,  3114,  1103, 23770,  1104,\n",
      "         1103,  2235,  1113,  1103,  2774,  1383,   117,  1173,  1103,  1928,\n",
      "        23215,  7353,  1110,   102])\n",
      "entity_list: [\"'model'\"]\n",
      "entity_token: [tensor([ 112, 2235,  112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: squared error is given by 1 MSE = (yˆ(test) y(test))2. (5.4) test m − i i  Intuitively, one can\n",
      "content_token: tensor([  101, 23215,  7353,  1110,  1549,  1118,   122, 10978,  2036,   134,\n",
      "          113,   100,   113,  2774,   114,   194,   113,  2774,   114,   114,\n",
      "          123,   119,   113,   126,   119,   125,   114,  2774,   182,   851,\n",
      "          178,   178,  1130,  7926,  8588,  1193,   117,  1141,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: one can see that this error measure decreases to 0 when yˆ(test) =y(test). We can also see that 1\n",
      "content_token: tensor([  101,  1141,  1169,  1267,  1115,  1142,  7353,  4929, 19377,  1106,\n",
      "          121,  1165,   100,   113,  2774,   114,   134,   194,   113,  2774,\n",
      "          114,   119,  1284,  1169,  1145,  1267,  1115,   122,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can also see that 1 MSE = ˆy(test) y(test) 2 , (5.5) test m|| − ||2 so the error increases whenever\n",
      "content_token: tensor([  101,  1169,  1145,  1267,  1115,   122, 10978,  2036,   134,   100,\n",
      "          113,  2774,   114,   194,   113,  2774,   114,   123,   117,   113,\n",
      "          126,   119,   126,   114,  2774,   182,   197,   197,   851,   197,\n",
      "          197,   123,  1177,  1103,  7353,  6986,  7747,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: increases whenever the Euclidean distance between the predictions and the targets increases. To\n",
      "content_token: tensor([  101,  6986,  7747,  1103,   142, 21977, 18498,  1389,  2462,  1206,\n",
      "         1103, 23770,  1105,  1103,  7539,  6986,   119,  1706,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: increases. To make a machine learning algorithm, we need to design an algorithm that will improve\n",
      "content_token: tensor([ 101, 6986,  119, 1706, 1294,  170, 3395, 3776, 9932,  117, 1195, 1444,\n",
      "        1106, 1902, 1126, 9932, 1115, 1209, 4607,  102])\n",
      "entity_list: ['machine learning algorithm']\n",
      "entity_token: [tensor([3395, 3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that will improve the weights w in a way that reduces MSE when the algorithm test is allowed to\n",
      "content_token: tensor([  101,  1115,  1209,  4607,  1103, 17981,   192,  1107,   170,  1236,\n",
      "         1115, 13822, 10978,  2036,  1165,  1103,  9932,  2774,  1110,  2148,\n",
      "         1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: test is allowed to gain experience by observing a training set (X(train),y(train)). One intuitive\n",
      "content_token: tensor([  101,  2774,  1110,  2148,  1106,  4361,  2541,  1118, 15639,   170,\n",
      "         2013,  1383,   113,   161,   113,  2669,   114,   117,   194,   113,\n",
      "         2669,   114,   114,   119,  1448,  1107,  7926,  8588,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: One intuitive way of doing this (which we will justify later, in section 5.5.1) is just to minimize\n",
      "content_token: tensor([  101,  1448,  1107,  7926,  8588,  1236,  1104,  1833,  1142,   113,\n",
      "         1134,  1195,  1209, 17422,  1224,   117,  1107,  2237,   126,   119,\n",
      "          126,   119,   122,   114,  1110,  1198,  1106, 20220,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is just to minimize the mean squared error on the training set, MSE . train To minimize MSE , we\n",
      "content_token: tensor([  101,  1110,  1198,  1106, 20220,  1103,  1928, 23215,  7353,  1113,\n",
      "         1103,  2013,  1383,   117, 10978,  2036,   119,  2669,  1706, 20220,\n",
      "        10978,  2036,   117,  1195,   102])\n",
      "entity_list: ['mean squared error', 'MSE']\n",
      "entity_token: [tensor([ 1928, 23215,  7353]), tensor([10978,  2036])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 2, 1, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: minimize MSE , we can simply solve for where its gradient is 0: train MSE = 0 (5.6) w train ∇ 1\n",
      "content_token: tensor([  101, 20220, 10978,  2036,   117,  1195,  1169,  2566,  9474,  1111,\n",
      "         1187,  1157, 19848,  1110,   121,   131,  2669, 10978,  2036,   134,\n",
      "          121,   113,   126,   119,   127,   114,   192,  2669,   100,   122,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 0 (5.6) w train ∇ 1 yˆ(train) y(train) 2 = 0 (5.7) w 2 ⇒ ∇ m|| − || 1 X(train)w y(train) 2 = 0\n",
      "content_token: tensor([ 101,  121,  113,  126,  119,  127,  114,  192, 2669,  100,  122,  100,\n",
      "         113, 2669,  114,  194,  113, 2669,  114,  123,  134,  121,  113,  126,\n",
      "         119,  128,  114,  192,  123,  848,  100,  182,  197,  197,  851,  197,\n",
      "         197,  122,  161,  113, 2669,  114,  192,  194,  113, 2669,  114,  123,\n",
      "         134,  121,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: y(train) 2 = 0 (5.8) ⇒ m∇w || − ||2 108 CHAPTER 5. MACHINE LEARNING BASICS Linear regression\n",
      "content_token: tensor([  101,   194,   113,  2669,   114,   123,   134,   121,   113,   126,\n",
      "          119,   129,   114,   848,   100,   197,   197,   851,   197,   197,\n",
      "          123, 10601,  8203,   126,   119, 25424,  3048, 11607,  2036,   149,\n",
      "        12420,  2069, 27451, 11780, 12465, 13882, 12122,  2800,  1813,  1231,\n",
      "        24032,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Linear regression example Optimization of w 3 0.55 2 0.50 0.45 1 0.40 0 0.35 1 − 0.30 2 0.25 − 3\n",
      "content_token: tensor([  101,  2800,  1813,  1231, 24032,  1859,  9126,  3121,  3080,  8569,\n",
      "         1104,   192,   124,   121,   119,  3731,   123,   121,   119,  1851,\n",
      "          121,   119,  2532,   122,   121,   119,  1969,   121,   121,   119,\n",
      "         2588,   122,   851,   121,   119,  1476,   123,   121,   119,  1512,\n",
      "          851,   124,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 1 − 0.30 2 0.25 − 3 0.20 − 1.0 0.5 0.0 0.5 1.0 0.5 1.0 1.5 − − x w 1 1 Figure 5.1: A linear\n",
      "content_token: tensor([  101,   122,   851,   121,   119,  1476,   123,   121,   119,  1512,\n",
      "          851,   124,   121,   119,  1406,   851,   122,   119,   121,   121,\n",
      "          119,   126,   121,   119,   121,   121,   119,   126,   122,   119,\n",
      "          121,   121,   119,   126,   122,   119,   121,   122,   119,   126,\n",
      "          851,   851,   193,   192,   122,   122, 15982,   126,   119,   122,\n",
      "          131,   138,  7378,   102])\n",
      "entity_list: ['Linear regression']\n",
      "entity_token: [tensor([ 2800,  1813,  1231, 24032])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 5.1: A linear regression problem, with a training set consisting of ten data points, each\n",
      "content_token: tensor([  101,   126,   119,   122,   131,   138,  7378,  1231, 24032,  2463,\n",
      "          117,  1114,   170,  2013,  1383,  4721,  1104,  1995,  2233,  1827,\n",
      "          117,  1296,   102])\n",
      "entity_list: ['Linear regression']\n",
      "entity_token: [tensor([ 2800,  1813,  1231, 24032])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: data points, each containing one feature. Because there is only one feature, the weight vector w\n",
      "content_token: tensor([ 101, 2233, 1827,  117, 1296, 4051, 1141, 2672,  119, 2279, 1175, 1110,\n",
      "        1178, 1141, 2672,  117, 1103, 2841, 9479,  192,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the weight vector w contains only a single parameter to learn, w . (Left)Observe that linear\n",
      "content_token: tensor([  101,  1103,  2841,  9479,   192,  2515,  1178,   170,  1423, 17816,\n",
      "         1106,  3858,   117,   192,   119,   113,  8123,   114,   152,  4832,\n",
      "        22552,  1115,  7378,   102])\n",
      "entity_list: ['weight vector w']\n",
      "entity_token: [tensor([2841, 9479,  192])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that linear regression learns 1 to set w such that the line y =w x comes as close as possible to\n",
      "content_token: tensor([  101,  1115,  7378,  1231, 24032, 10123,   122,  1106,  1383,   192,\n",
      "         1216,  1115,  1103,  1413,   194,   134,   192,   193,  2502,  1112,\n",
      "         1601,  1112,  1936,  1106,   102])\n",
      "entity_list: ['linear regression']\n",
      "entity_token: [tensor([ 7378,  1231, 24032])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as possible to passing through all the 1 1 training points. (Right)The plotted point indicates the\n",
      "content_token: tensor([ 101, 1112, 1936, 1106, 3744, 1194, 1155, 1103,  122,  122, 2013, 1827,\n",
      "         119,  113, 4114,  114, 1109, 4928, 1906, 1553, 6653, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: point indicates the value of w found by the normal 1 equations, which we can see minimizes the mean\n",
      "content_token: tensor([  101,  1553,  6653,  1103,  2860,  1104,   192,  1276,  1118,  1103,\n",
      "         2999,   122, 11838,   117,  1134,  1195,  1169,  1267, 20220,  1116,\n",
      "         1103,  1928,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: minimizes the mean squared error on the training set. X(train)w y(train)  X(train)w y(train) = 0\n",
      "content_token: tensor([  101, 20220,  1116,  1103,  1928, 23215,  7353,  1113,  1103,  2013,\n",
      "         1383,   119,   161,   113,  2669,   114,   192,   194,   113,  2669,\n",
      "          114,   161,   113,  2669,   114,   192,   194,   113,  2669,   114,\n",
      "          134,   121,   102])\n",
      "entity_list: ['mean squared error']\n",
      "entity_token: [tensor([ 1928, 23215,  7353])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: y(train) = 0 (5.9) w ⇒ ∇ − −     w X(train) X(train)w 2w X(train) y(train)+y(train) y(train) =\n",
      "content_token: tensor([ 101,  194,  113, 2669,  114,  134,  121,  113,  126,  119,  130,  114,\n",
      "         192,  848,  100,  851,  851,  192,  161,  113, 2669,  114,  161,  113,\n",
      "        2669,  114,  192,  123, 2246,  161,  113, 2669,  114,  194,  113, 2669,\n",
      "         114,  116,  194,  113, 2669,  114,  194,  113, 2669,  114,  134,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: y(train) = 0 w      ⇒ ∇ −  (5.10) 2X(train) X(train)w 2X(train) y(train) = 0 (5.11)   ⇒ −\n",
      "content_token: tensor([ 101,  194,  113, 2669,  114,  134,  121,  192,  848,  100,  851,  113,\n",
      "         126,  119, 1275,  114,  123, 3190,  113, 2669,  114,  161,  113, 2669,\n",
      "         114,  192,  123, 3190,  113, 2669,  114,  194,  113, 2669,  114,  134,\n",
      "         121,  113,  126,  119, 1429,  114,  848,  851,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: = 0 (5.11)   ⇒ − 1 w = X(train) X(train) − X(train) y(train) (5.12)   ⇒   The system of\n",
      "content_token: tensor([ 101,  134,  121,  113,  126,  119, 1429,  114,  848,  851,  122,  192,\n",
      "         134,  161,  113, 2669,  114,  161,  113, 2669,  114,  851,  161,  113,\n",
      "        2669,  114,  194,  113, 2669,  114,  113,  126,  119, 1367,  114,  848,\n",
      "        1109, 1449, 1104,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ⇒   The system of equations whose solution is given by equation 5.12 is known as the normal\n",
      "content_token: tensor([  101,   848,  1109,  1449,  1104, 11838,  2133,  5072,  1110,  1549,\n",
      "         1118,  8381,   126,   119,  1367,  1110,  1227,  1112,  1103,  2999,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: known as the normal equations. Evaluating equation 5.12 constitutes a simple learning algorithm.\n",
      "content_token: tensor([  101,  1227,  1112,  1103,  2999, 11838,   119,  9734,  7535,  3798,\n",
      "         8381,   126,   119,  1367, 18592,   170,  3014,  3776,  9932,   119,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning algorithm. For an example of the linear regression learning algorithm in action, see\n",
      "content_token: tensor([  101,  3776,  9932,   119,  1370,  1126,  1859,  1104,  1103,  7378,\n",
      "         1231, 24032,  3776,  9932,  1107,  2168,   117,  1267,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in action, see figure 5.1. It is worth noting that the term linear regression is often used to\n",
      "content_token: tensor([  101,  1107,  2168,   117,  1267,  2482,   126,   119,   122,   119,\n",
      "         1135,  1110,  3869,  9095,  1115,  1103,  1858,  7378,  1231, 24032,\n",
      "         1110,  1510,  1215,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is often used to refer to a slightly more sophisticated model with one additional parameter—an\n",
      "content_token: tensor([  101,  1110,  1510,  1215,  1106,  5991,  1106,   170,  2776,  1167,\n",
      "        12580,  2235,  1114,  1141,  2509, 17816,   783,  1126,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parameter—an intercept term b. In this model yˆ= w x+b (5.13)  so the mapping from parameters to\n",
      "content_token: tensor([  101, 17816,   783,  1126, 22205,  1858,   171,   119,  1130,  1142,\n",
      "         2235,   100,   134,   192,   193,   116,   171,   113,   126,   119,\n",
      "         1492,   114,  1177,  1103, 13970,  1121, 11934,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from parameters to predictions is still a linear function but the mapping from features to\n",
      "content_token: tensor([  101,  1121, 11934,  1106, 23770,  1110,  1253,   170,  7378,  3053,\n",
      "         1133,  1103, 13970,  1121,  1956,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from features to predictions is now an affine function. This extension to affine functions means\n",
      "content_token: tensor([  101,  1121,  1956,  1106, 23770,  1110,  1208,  1126,   170, 16274,\n",
      "         1162,  3053,   119,  1188,  4973,  1106,   170, 16274,  1162,  4226,\n",
      "         2086,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: functions means that the plot of the model’s predictions still looks like a line, but it need not\n",
      "content_token: tensor([  101,  4226,  2086,  1115,  1103,  4928,  1104,  1103,  2235,   787,\n",
      "          188, 23770,  1253,  2736,  1176,   170,  1413,   117,  1133,  1122,\n",
      "         1444,  1136,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: but it need not pass through the origin. Instead of adding the bias parameter 109 y )niart(ESM\n",
      "content_token: tensor([  101,  1133,  1122,  1444,  1136,  2789,  1194,  1103,  4247,   119,\n",
      "         3743,  1104,  5321,  1103, 15069, 17816, 11523,   194,   114, 11437,\n",
      "         9349,   113,   142, 16450,   102])\n",
      "entity_list: ['bias parameter']\n",
      "entity_token: [tensor([15069, 17816])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 5. MACHINE LEARNING BASICS b, one can continue to use the model with only weights but\n",
      "content_token: tensor([  101,  8203,   126,   119, 25424,  3048, 11607,  2036,   149, 12420,\n",
      "         2069, 27451, 11780, 12465, 13882, 12122,   171,   117,  1141,  1169,\n",
      "         2760,  1106,  1329,  1103,  2235,  1114,  1178, 17981,  1133,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: only weights but augment x with an extra entry that is always set to 1. The weight corresponding to\n",
      "content_token: tensor([  101,  1178, 17981,  1133, 12686, 14294,   193,  1114,  1126,  3908,\n",
      "         3990,  1115,  1110,  1579,  1383,  1106,   122,   119,  1109,  2841,\n",
      "         7671,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: corresponding to the extra 1 entry plays the role of the bias parameter. We will frequently use the\n",
      "content_token: tensor([  101,  7671,  1106,  1103,  3908,   122,  3990,  2399,  1103,  1648,\n",
      "         1104,  1103, 15069, 17816,   119,  1284,  1209,  3933,  1329,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: frequently use the term “linear” when referring to affine functions throughout this book. The\n",
      "content_token: tensor([  101,  3933,  1329,  1103,  1858,   789,  7378,   790,  1165,  7455,\n",
      "         1106,   170, 16274,  1162,  4226,  2032,  1142,  1520,   119,  1109,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this book. The intercept term b is often called the bias parameter of the affine transfor- mation.\n",
      "content_token: tensor([  101,  1142,  1520,   119,  1109, 22205,  1858,   171,  1110,  1510,\n",
      "         1270,  1103, 15069, 17816,  1104,  1103,   170, 16274,  1162, 14715,\n",
      "        14467,  1197,   118, 22591,  1988,   119,   102])\n",
      "entity_list: ['bias parameter']\n",
      "entity_token: [tensor([15069, 17816])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: transfor- mation. This terminology derives from the point of view that the output of the\n",
      "content_token: tensor([  101, 14715, 14467,  1197,   118, 22591,  1988,   119,  1188, 20925,\n",
      "        12301,  1121,  1103,  1553,  1104,  2458,  1115,  1103,  5964,  1104,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the output of the transformation is biased toward being b in the absence of any input. This term is\n",
      "content_token: tensor([  101,  1103,  5964,  1104,  1103,  9047,  1110, 15069,  1174,  1755,\n",
      "         1217,   171,  1107,  1103,  5884,  1104,  1251,  7758,   119,  1188,\n",
      "         1858,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: input. This term is different from the idea of a statistical bias, in which a statistical\n",
      "content_token: tensor([  101,  7758,   119,  1188,  1858,  1110,  1472,  1121,  1103,  1911,\n",
      "         1104,   170, 11435, 15069,   117,  1107,  1134,   170, 11435,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: which a statistical estimation algorithm’s expected estimate of a quantity is not equal to the true\n",
      "content_token: tensor([  101,  1134,   170, 11435, 12890, 21517,  9932,   787,   188,  2637,\n",
      "        10301,  1104,   170, 11978,  1110,  1136,  4463,  1106,  1103,  2276,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: equal to the true quantity. Linearregressionis ofcourse anextremelysimple andlimitedlearning\n",
      "content_token: tensor([  101,  4463,  1106,  1103,  2276, 11978,   119,  2800,  1813,  1874,\n",
      "        24032,  1548,  1104, 16461,  1126, 11708,  7877, 10212,  6834,  4060,\n",
      "         7136,  1105, 24891, 10334, 19094,  4558,  1158,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: andlimitedlearning algorithm, but it provides an example of how a learning algorithm can work. In\n",
      "content_token: tensor([  101,  1105, 24891, 10334, 19094,  4558,  1158,  9932,   117,  1133,\n",
      "         1122,  2790,  1126,  1859,  1104,  1293,   170,  3776,  9932,  1169,\n",
      "         1250,   119,  1130,   102])\n",
      "entity_list: ['output: learning algorithm']\n",
      "entity_token: [tensor([5964,  131, 3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can work. In the subsequent sections we will describe some of the basic principles underlying\n",
      "content_token: tensor([  101,  1169,  1250,   119,  1130,  1103,  4194,  4886,  1195,  1209,\n",
      "         5594,  1199,  1104,  1103,  3501,  6551, 10311,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: underlying learning algorithm design and demonstrate how these principles can be used to build more\n",
      "content_token: tensor([  101, 10311,  3776,  9932,  1902,  1105, 10541,  1293,  1292,  6551,\n",
      "         1169,  1129,  1215,  1106,  3076,  1167,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: used to build more complicated learning algorithms. 5.2 Capacity, Overfitting and Underfitting The\n",
      "content_token: tensor([  101,  1215,  1106,  3076,  1167,  8277,  3776, 14975,   119,   126,\n",
      "          119,   123, 17212, 19905,   117,  3278, 14067,  1916,  1105,  2831,\n",
      "        14067,  1916,  1109,   102])\n",
      "entity_list: ['Overfitting', 'Underfitting']\n",
      "entity_token: [tensor([ 3278, 14067,  1916]), tensor([ 2831, 14067,  1916])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 2, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Underfitting The central challenge in machine learning is that we must perform well on new,\n",
      "content_token: tensor([  101,  2831, 14067,  1916,  1109,  2129,  4506,  1107,  3395,  3776,\n",
      "         1110,  1115,  1195,  1538,  3870,  1218,  1113,  1207,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: well on new, previously unseen inputs—not just those on which our model was trained. The ability to\n",
      "content_token: tensor([  101,  1218,  1113,  1207,   117,  2331, 19508, 22743,   783,  1136,\n",
      "         1198,  1343,  1113,  1134,  1412,  2235,  1108,  3972,   119,  1109,\n",
      "         2912,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The ability to perform well on previously unobserved inputs is called generalization. Typically,\n",
      "content_token: tensor([  101,  1109,  2912,  1106,  3870,  1218,  1113,  2331,  8362, 12809,\n",
      "        17886,  1181, 22743,  1110,  1270,  1704,  2734,   119, 16304,   117,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Typically, when training a machine learning model, we have access to a training set, we can compute\n",
      "content_token: tensor([  101, 16304,   117,  1165,  2013,   170,  3395,  3776,  2235,   117,\n",
      "         1195,  1138,  2469,  1106,   170,  2013,  1383,   117,  1195,  1169,\n",
      "         3254, 22662,   102])\n",
      "entity_list: ['machine learning model']\n",
      "entity_token: [tensor([3395, 3776, 2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: set, we can compute some error measure on the training set called the training error, and we reduce\n",
      "content_token: tensor([  101,  1383,   117,  1195,  1169,  3254, 22662,  1199,  7353,  4929,\n",
      "         1113,  1103,  2013,  1383,  1270,  1103,  2013,  7353,   117,  1105,\n",
      "         1195,  4851,   102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and we reduce this training error. So far, what we have described is simply an optimization\n",
      "content_token: tensor([  101,  1105,  1195,  4851,  1142,  2013,  7353,   119,  1573,  1677,\n",
      "          117,  1184,  1195,  1138,  1758,  1110,  2566,  1126, 25161,   102])\n",
      "entity_list: ['training error']\n",
      "entity_token: [tensor([2013, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: an optimization problem. What separates machine learning from optimization is that we want the\n",
      "content_token: tensor([  101,  1126, 25161,  2463,   119,  1327, 20229,  3395,  3776,  1121,\n",
      "        25161,  1110,  1115,  1195,  1328,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is that we want the generalization error, also called the test error, to be low as well. The\n",
      "content_token: tensor([ 101, 1110, 1115, 1195, 1328, 1103, 1704, 2734, 7353,  117, 1145, 1270,\n",
      "        1103, 2774, 7353,  117, 1106, 1129, 1822, 1112, 1218,  119, 1109,  102])\n",
      "entity_list: ['generalization error']\n",
      "entity_token: [tensor([1704, 2734, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: be low as well. The generalization error is defined as the expected value of the error on a new\n",
      "content_token: tensor([ 101, 1129, 1822, 1112, 1218,  119, 1109, 1704, 2734, 7353, 1110, 3393,\n",
      "        1112, 1103, 2637, 2860, 1104, 1103, 7353, 1113,  170, 1207,  102])\n",
      "entity_list: ['generalization error']\n",
      "entity_token: [tensor([1704, 2734, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the error on a new input. Here the expectation is taken across different possible inputs, drawn\n",
      "content_token: tensor([  101,  1103,  7353,  1113,   170,  1207,  7758,   119,  3446,  1103,\n",
      "        19351,  1110,  1678,  1506,  1472,  1936, 22743,   117,  3795,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: inputs, drawn from the distribution of inputs we expect the system to encounter in practice. We\n",
      "content_token: tensor([  101, 22743,   117,  3795,  1121,  1103,  3735,  1104, 22743,  1195,\n",
      "         5363,  1103,  1449,  1106,  8107,  1107,  2415,   119,  1284,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in practice. We typically estimate the generalization error of a machine learning model by\n",
      "content_token: tensor([  101,  1107,  2415,   119,  1284,  3417, 10301,  1103,  1704,  2734,\n",
      "         7353,  1104,   170,  3395,  3776,  2235,  1118,   102])\n",
      "entity_list: [' machine learning model']\n",
      "entity_token: [tensor([3395, 3776, 2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning model by measuring its performance on a test setof examples that were collected separately\n",
      "content_token: tensor([  101,  3776,  2235,  1118, 10099,  1157,  2099,  1113,   170,  2774,\n",
      "         1383, 10008,  5136,  1115,  1127,  4465, 10380,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: separately from the training set. In our linear regression example, we trained the model by\n",
      "content_token: tensor([  101, 10380,  1121,  1103,  2013,  1383,   119,  1130,  1412,  7378,\n",
      "         1231, 24032,  1859,   117,  1195,  3972,  1103,  2235,  1118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the model by minimizing the training error, 1 X(train)w y(train) 2 , (5.14) m(train)|| − ||2 but we\n",
      "content_token: tensor([  101,  1103,  2235,  1118,  8715, 25596,  1103,  2013,  7353,   117,\n",
      "          122,   161,   113,  2669,   114,   192,   194,   113,  2669,   114,\n",
      "          123,   117,   113,   126,   119,  1489,   114,   182,   113,  2669,\n",
      "          114,   197,   197,   851,   197,   197,   123,  1133,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: − ||2 but we actually care about the test error, 1 X(test)w y(test) 2. m(test)|| − ||2 How can we\n",
      "content_token: tensor([ 101,  851,  197,  197,  123, 1133, 1195, 2140, 1920, 1164, 1103, 2774,\n",
      "        7353,  117,  122,  161,  113, 2774,  114,  192,  194,  113, 2774,  114,\n",
      "         123,  119,  182,  113, 2774,  114,  197,  197,  851,  197,  197,  123,\n",
      "        1731, 1169, 1195,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: − ||2 How can we affect performance on the test set when we get to observe only the 110 CHAPTER 5.\n",
      "content_token: tensor([  101,   851,   197,   197,   123,  1731,  1169,  1195,  6975,  2099,\n",
      "         1113,  1103,  2774,  1383,  1165,  1195,  1243,  1106, 12326,  1178,\n",
      "         1103,  6745,  8203,   126,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the 110 CHAPTER 5. MACHINE LEARNING BASICS training set? The field of statistical learning theory\n",
      "content_token: tensor([  101,  1103,  6745,  8203,   126,   119, 25424,  3048, 11607,  2036,\n",
      "          149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,  2013,  1383,\n",
      "          136,  1109,  1768,  1104, 11435,  3776,  2749,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning theory provides some answers. If the training and the test set are collected arbitrarily,\n",
      "content_token: tensor([  101,  3776,  2749,  2790,  1199,  6615,   119,  1409,  1103,  2013,\n",
      "         1105,  1103,  2774,  1383,  1132,  4465,   170, 26281,  2875, 22190,\n",
      "         5264,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: arbitrarily, there is indeed little we can do. If we are allowed to make some assumptions about how\n",
      "content_token: tensor([  101,   170, 26281,  2875, 22190,  5264,   117,  1175,  1110,  5750,\n",
      "         1376,  1195,  1169,  1202,   119,  1409,  1195,  1132,  2148,  1106,\n",
      "         1294,  1199, 19129,  1164,  1293,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: about how the training and test set are collected, then we can make some progress. The train and\n",
      "content_token: tensor([ 101, 1164, 1293, 1103, 2013, 1105, 2774, 1383, 1132, 4465,  117, 1173,\n",
      "        1195, 1169, 1294, 1199, 5070,  119, 1109, 2669, 1105,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The train and test data are generated bya probability distribution over datasets called the data\n",
      "content_token: tensor([  101,  1109,  2669,  1105,  2774,  2233,  1132,  6455,  1118,  1161,\n",
      "         9750,  3735,  1166,  2233, 27948,  1270,  1103,  2233,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: called the data generating process. We typically make a set of assumptions known collectively as\n",
      "content_token: tensor([  101,  1270,  1103,  2233, 12713,  1965,   119,  1284,  3417,  1294,\n",
      "          170,  1383,  1104, 19129,  1227, 14998,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: collectively as the i.i.d. assumptions. These assumptions are that the examples in each dataset are\n",
      "content_token: tensor([  101, 14998,  1112,  1103,   178,   119,   178,   119,   173,   119,\n",
      "        19129,   119,  1636, 19129,  1132,  1115,  1103,  5136,  1107,  1296,\n",
      "         2233,  9388,  1132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in each dataset are independent from each other, and that the train set and test set are\n",
      "content_token: tensor([ 101, 1107, 1296, 2233, 9388, 1132, 2457, 1121, 1296, 1168,  117, 1105,\n",
      "        1115, 1103, 2669, 1383, 1105, 2774, 1383, 1132,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and test set are identically distributed, drawn from the same probability distribution as each\n",
      "content_token: tensor([ 101, 1105, 2774, 1383, 1132, 6742, 1193, 4901,  117, 3795, 1121, 1103,\n",
      "        1269, 9750, 3735, 1112, 1296,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as each other. This assumption allows us to describe the data gen- erating process with a\n",
      "content_token: tensor([  101,  1112,  1296,  1168,   119,  1188, 13457,  3643,  1366,  1106,\n",
      "         5594,  1103,  2233,   176,  1424,   118,  3386,  1916,  1965,  1114,\n",
      "          170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: process with a probability distribution over a single example. The same distribution is then used\n",
      "content_token: tensor([ 101, 1965, 1114,  170, 9750, 3735, 1166,  170, 1423, 1859,  119, 1109,\n",
      "        1269, 3735, 1110, 1173, 1215,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is then used to generate every train example and every test example. We call that shared underlying\n",
      "content_token: tensor([  101,  1110,  1173,  1215,  1106,  9509,  1451,  2669,  1859,  1105,\n",
      "         1451,  2774,  1859,   119,  1284,  1840,  1115,  3416, 10311,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: shared underlying distribution the data generating distribution, denoted p . This probabilistic\n",
      "content_token: tensor([  101,  3416, 10311,  3735,  1103,  2233, 12713,  3735,   117, 21307,\n",
      "          185,   119,  1188,  5250,  2822, 15197,  5562,   102])\n",
      "entity_list: ['data generating distribution']\n",
      "entity_token: [tensor([ 2233, 12713,  3735])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This probabilistic framework and the i.i.d. assumptions allow us to data mathematically study the\n",
      "content_token: tensor([  101,  1188,  5250,  2822, 15197,  5562,  8297,  1105,  1103,   178,\n",
      "          119,   178,   119,   173,   119, 19129,  2621,  1366,  1106,  2233,\n",
      "         9988,  1193,  2025,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: study the relationship between training error and test error. One immediate connection we can\n",
      "content_token: tensor([ 101, 2025, 1103, 2398, 1206, 2013, 7353, 1105, 2774, 7353,  119, 1448,\n",
      "        5670, 3797, 1195, 1169,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: connection we can observe between the training and test error is that the expected training error\n",
      "content_token: tensor([  101,  3797,  1195,  1169, 12326,  1206,  1103,  2013,  1105,  2774,\n",
      "         7353,  1110,  1115,  1103,  2637,  2013,  7353,   102])\n",
      "entity_list: ['training error']\n",
      "entity_token: [tensor([2013, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: training error of a randomly selected model is equal to the expected test error of that model.\n",
      "content_token: tensor([  101,  2013,  7353,  1104,   170, 19729,  2700,  2235,  1110,  4463,\n",
      "         1106,  1103,  2637,  2774,  7353,  1104,  1115,  2235,   119,   102])\n",
      "entity_list: ['training error', 'expected test error', 'model']\n",
      "entity_token: [tensor([2013, 7353]), tensor([2637, 2774, 7353]), tensor([2235])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 1, 0, 0, 2, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of that model. Suppose we have a probability distribution p(x,y) and we sample from it repeatedly\n",
      "content_token: tensor([  101,  1104,  1115,  2235,   119, 15463,  8661,  6787,  1195,  1138,\n",
      "          170,  9750,  3735,   185,   113,   193,   117,   194,   114,  1105,\n",
      "         1195,  6876,  1121,  1122,  8038,   102])\n",
      "entity_list: ['model']\n",
      "entity_token: [tensor([2235])]\n",
      "label: tensor([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from it repeatedly to generate the train set and the test set. For some fixed value w, the expected\n",
      "content_token: tensor([ 101, 1121, 1122, 8038, 1106, 9509, 1103, 2669, 1383, 1105, 1103, 2774,\n",
      "        1383,  119, 1370, 1199, 4275, 2860,  192,  117, 1103, 2637,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: w, the expected training set error is exactly the same as the expected test set error, because both\n",
      "content_token: tensor([ 101,  192,  117, 1103, 2637, 2013, 1383, 7353, 1110, 2839, 1103, 1269,\n",
      "        1112, 1103, 2637, 2774, 1383, 7353,  117, 1272, 1241,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error, because both expectations are formed using the same dataset sampling process. The only\n",
      "content_token: tensor([  101,  7353,   117,  1272,  1241, 11471,  1132,  1824,  1606,  1103,\n",
      "         1269,  2233,  9388, 18200,  1965,   119,  1109,  1178,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: process. The only difference between the two conditions is the name we assign to the dataset we\n",
      "content_token: tensor([  101,  1965,   119,  1109,  1178,  3719,  1206,  1103,  1160,  2975,\n",
      "         1110,  1103,  1271,  1195, 27430,  1106,  1103,  2233,  9388,  1195,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to the dataset we sample. Of course, when we use a machine learning algorithm, we do not fix the\n",
      "content_token: tensor([ 101, 1106, 1103, 2233, 9388, 1195, 6876,  119, 2096, 1736,  117, 1165,\n",
      "        1195, 1329,  170, 3395, 3776, 9932,  117, 1195, 1202, 1136, 8239, 1103,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we do not fix the parameters ahead of time, then sample both datasets. We sample the training set,\n",
      "content_token: tensor([  101,  1195,  1202,  1136,  8239,  1103, 11934,  3075,  1104,  1159,\n",
      "          117,  1173,  6876,  1241,  2233, 27948,   119,  1284,  6876,  1103,\n",
      "         2013,  1383,   117,   102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the training set, then use it to choose the parameters to reduce training set error, then sample\n",
      "content_token: tensor([  101,  1103,  2013,  1383,   117,  1173,  1329,  1122,  1106,  4835,\n",
      "         1103, 11934,  1106,  4851,  2013,  1383,  7353,   117,  1173,  6876,\n",
      "          102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error, then sample the test set. Under this process, the expected test error is greater than or\n",
      "content_token: tensor([ 101, 7353,  117, 1173, 6876, 1103, 2774, 1383,  119, 2831, 1142, 1965,\n",
      "         117, 1103, 2637, 2774, 7353, 1110, 3407, 1190, 1137,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is greater than or equal to the expected value of training error. The factors determining how well\n",
      "content_token: tensor([  101,  1110,  3407,  1190,  1137,  4463,  1106,  1103,  2637,  2860,\n",
      "         1104,  2013,  7353,   119,  1109,  5320, 13170,  1293,  1218,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: how well a machine learning algorithm will perform are its ability to: 1. Make the training error\n",
      "content_token: tensor([ 101, 1293, 1218,  170, 3395, 3776, 9932, 1209, 3870, 1132, 1157, 2912,\n",
      "        1106,  131,  122,  119, 7102, 1103, 2013, 7353,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the training error small. 2. Make the gap between training and test error small. These two factors\n",
      "content_token: tensor([ 101, 1103, 2013, 7353, 1353,  119,  123,  119, 7102, 1103, 7275, 1206,\n",
      "        2013, 1105, 2774, 7353, 1353,  119, 1636, 1160, 5320,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: These two factors correspond to the two central challenges in machine learning: underfitting and\n",
      "content_token: tensor([  101,  1636,  1160,  5320, 18420,  1106,  1103,  1160,  2129,  7806,\n",
      "         1107,  3395,  3776,   131,  1223, 14067,  1916,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: underfitting and overfitting. Underfitting occurs when the model is not able to obtain a\n",
      "content_token: tensor([  101,  1223, 14067,  1916,  1105,  1166, 14067,  1916,   119,  2831,\n",
      "        14067,  1916,  4365,  1165,  1103,  2235,  1110,  1136,  1682,  1106,\n",
      "         6268,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap\n",
      "content_token: tensor([  101,  1682,  1106,  6268,   170, 13230,  1822,  7353,  2860,  1113,\n",
      "         1103,  2013,  1383,   119,  3278, 14067,  1916,  4365,  1165,  1103,\n",
      "         7275,   102])\n",
      "entity_list: ['underfitting', 'overfitting']\n",
      "entity_token: [tensor([ 1223, 14067,  1916]), tensor([ 1166, 14067,  1916])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: occurs when the gap between the training error and test error is too large. We can control whether\n",
      "content_token: tensor([ 101, 4365, 1165, 1103, 7275, 1206, 1103, 2013, 7353, 1105, 2774, 7353,\n",
      "        1110, 1315, 1415,  119, 1284, 1169, 1654, 2480,  102])\n",
      "entity_list: ['overfitting']\n",
      "entity_token: [tensor([ 1166, 14067,  1916])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can control whether a model is more likely to overfit or underfit by altering its capacity.\n",
      "content_token: tensor([  101,  1169,  1654,  2480,   170,  2235,  1110,  1167,  2620,  1106,\n",
      "         1166, 14067,  1137,  1223, 14067,  1118, 25595,  1157,  3211,   119,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: its capacity. Informally, a model’s capacity is its ability to fit a wide variety of 111 CHAPTER 5.\n",
      "content_token: tensor([  101,  1157,  3211,   119,  1130, 13199,  2716,   117,   170,  2235,\n",
      "          787,   188,  3211,  1110,  1157,  2912,  1106,  4218,   170,  2043,\n",
      "         2783,  1104, 11084,  8203,   126,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of 111 CHAPTER 5. MACHINE LEARNING BASICS functions. Models with low capacity may struggle to fit\n",
      "content_token: tensor([  101,  1104, 11084,  8203,   126,   119, 25424,  3048, 11607,  2036,\n",
      "          149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,  4226,   119,\n",
      "        24025,  1114,  1822,  3211,  1336,  5637,  1106,  4218,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: may struggle to fit the training set. Models with high capacity can overfit by memorizing\n",
      "content_token: tensor([  101,  1336,  5637,  1106,  4218,  1103,  2013,  1383,   119, 24025,\n",
      "         1114,  1344,  3211,  1169,  1166, 14067,  1118,  1143, 26271,  4404,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by memorizing properties of the training set that do not serve them well on the test set. One way\n",
      "content_token: tensor([  101,  1118,  1143, 26271,  4404,  4625,  1104,  1103,  2013,  1383,\n",
      "         1115,  1202,  1136,  2867,  1172,  1218,  1113,  1103,  2774,  1383,\n",
      "          119,  1448,  1236,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: test set. One way to control the capacity of a learning algorithm is by choosing its hypothesis\n",
      "content_token: tensor([  101,  2774,  1383,   119,  1448,  1236,  1106,  1654,  1103,  3211,\n",
      "         1104,   170,  3776,  9932,  1110,  1118, 11027,  1157, 11066,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: its hypothesis space, the set of functions that the learning algorithm is allowed to select as\n",
      "content_token: tensor([  101,  1157, 11066,  2000,   117,  1103,  1383,  1104,  4226,  1115,\n",
      "         1103,  3776,  9932,  1110,  2148,  1106,  8247,  1112,   102])\n",
      "entity_list: ['learning algorithm']\n",
      "entity_token: [tensor([3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to select as being the solution. For example, the linear regression algorithm has the set of all\n",
      "content_token: tensor([  101,  1106,  8247,  1112,  1217,  1103,  5072,   119,  1370,  1859,\n",
      "          117,  1103,  7378,  1231, 24032,  9932,  1144,  1103,  1383,  1104,\n",
      "         1155,   102])\n",
      "entity_list: ['linear regression algorithm']\n",
      "entity_token: [tensor([ 7378,  1231, 24032,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: has the set of all linear functions of its input as its hypothesis space. We can generalize linear\n",
      "content_token: tensor([  101,  1144,  1103,  1383,  1104,  1155,  7378,  4226,  1104,  1157,\n",
      "         7758,  1112,  1157, 11066,  2000,   119,  1284,  1169,  1704,  3708,\n",
      "         7378,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: generalize linear regression to include polynomials, rather than just linear functions, in its\n",
      "content_token: tensor([  101,  1704,  3708,  7378,  1231, 24032,  1106,  1511, 19068,  1116,\n",
      "          117,  1897,  1190,  1198,  7378,  4226,   117,  1107,  1157,   102])\n",
      "entity_list: ['hypothesis space', 'linear regression']\n",
      "entity_token: [tensor([11066,  2000]), tensor([ 7378,  1231, 24032])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: functions, in its hypothesis space. Doing so increases the model’s capacity. A polynomial of degree\n",
      "content_token: tensor([  101,  4226,   117,  1107,  1157, 11066,  2000,   119, 27691,  1177,\n",
      "         6986,  1103,  2235,   787,   188,  3211,   119,   138, 19068,  1104,\n",
      "         2178,   102])\n",
      "entity_list: [\"model's capacity\"]\n",
      "entity_token: [tensor([2235,  112,  188, 3211])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of degree one gives us the linear regression model with which we are already familiar, with\n",
      "content_token: tensor([  101,  1104,  2178,  1141,  3114,  1366,  1103,  7378,  1231, 24032,\n",
      "         2235,  1114,  1134,  1195,  1132,  1640,  4509,   117,  1114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: familiar, with prediction yˆ= b+wx. (5.15) By introducing x2 as another feature provided to the\n",
      "content_token: tensor([  101,  4509,   117,  1114, 20770,   100,   134,   171,   116,   192,\n",
      "         1775,   119,   113,   126,   119,  1405,   114,  1650, 11100,   193,\n",
      "         1477,  1112,  1330,  2672,  2136,  1106,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: provided to the linear regression model, we can learn a model that is quadratic as a function of x:\n",
      "content_token: tensor([  101,  2136,  1106,  1103,  7378,  1231, 24032,  2235,   117,  1195,\n",
      "         1169,  3858,   170,  2235,  1115,  1110,   186, 18413, 21961,  1112,\n",
      "          170,  3053,  1104,   193,   131,   102])\n",
      "entity_list: ['linear regression model']\n",
      "entity_token: [tensor([ 7378,  1231, 24032,  2235])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as a function of x: yˆ= b+w x+w x2 . (5.16) 1 2 Though this model implements a quadratic function\n",
      "content_token: tensor([  101,  1112,   170,  3053,  1104,   193,   131,   100,   134,   171,\n",
      "          116,   192,   193,   116,   192,   193,  1477,   119,   113,   126,\n",
      "          119,  1479,   114,   122,   123,  3473,  1142,  2235, 24935,   170,\n",
      "          186, 18413, 21961,  3053,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: quadratic function of its input, the output is still a linear function of the parameters, so we can\n",
      "content_token: tensor([  101,   186, 18413, 21961,  3053,  1104,  1157,  7758,   117,  1103,\n",
      "         5964,  1110,  1253,   170,  7378,  3053,  1104,  1103, 11934,   117,\n",
      "         1177,  1195,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: so we can still use the normal equations to train the model in closed form. We can continue to add\n",
      "content_token: tensor([  101,  1177,  1195,  1169,  1253,  1329,  1103,  2999, 11838,  1106,\n",
      "         2669,  1103,  2235,  1107,  1804,  1532,   119,  1284,  1169,  2760,\n",
      "         1106,  5194,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can continue to add more powers of x as additional features, for example to obtain a polynomial of\n",
      "content_token: tensor([  101,  1169,  2760,  1106,  5194,  1167,  3758,  1104,   193,  1112,\n",
      "         2509,  1956,   117,  1111,  1859,  1106,  6268,   170, 19068,  1104,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a polynomial of degree 9: 9 yˆ= b+ w xi . (5.17) i i=1  Machine learning algorithms will generally\n",
      "content_token: tensor([  101,   170, 19068,  1104,  2178,   130,   131,   130,   100,   134,\n",
      "          171,   116,   192,   193,  1182,   119,   113,   126,   119,  1542,\n",
      "          114,   178,   178,   134,   122,  7792,  3776, 14975,  1209,  2412,\n",
      "          102])\n",
      "entity_list: ['Machine learning algorithms']\n",
      "entity_token: [tensor([ 7792,  3776, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 2, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: will generally perform best when their capacity is appropriate for the true complexity of the task\n",
      "content_token: tensor([  101,  1209,  2412,  3870,  1436,  1165,  1147,  3211,  1110,  5806,\n",
      "         1111,  1103,  2276, 12133,  1104,  1103,  4579,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the task they need to perform and the amount of training data they are provided with. Models\n",
      "content_token: tensor([  101,  1104,  1103,  4579,  1152,  1444,  1106,  3870,  1105,  1103,\n",
      "         2971,  1104,  2013,  2233,  1152,  1132,  2136,  1114,   119, 24025,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with. Models with insufficient capacity are unable to solve complex tasks. Models with high\n",
      "content_token: tensor([  101,  1114,   119, 24025,  1114, 14733,  3211,  1132,  3372,  1106,\n",
      "         9474,  2703,  8249,   119, 24025,  1114,  1344,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Models with high capacity can solve complex tasks, but when their capacity is higher than needed to\n",
      "content_token: tensor([  101, 24025,  1114,  1344,  3211,  1169,  9474,  2703,  8249,   117,\n",
      "         1133,  1165,  1147,  3211,  1110,  2299,  1190,  1834,  1106,   102])\n",
      "entity_list: ['output: Models with high capacity']\n",
      "entity_token: [tensor([ 5964,   131, 24025,  1114,  1344,  3211])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: than needed to solve the present task they may overfit. Figure 5.2 shows this principle in action.\n",
      "content_token: tensor([  101,  1190,  1834,  1106,  9474,  1103,  1675,  4579,  1152,  1336,\n",
      "         1166, 14067,   119, 15982,   126,   119,   123,  2196,  1142,  6708,\n",
      "         1107,  2168,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in action. We compare a linear, quadratic and degree-9 predictor attempting to fit a problem where\n",
      "content_token: tensor([  101,  1107,  2168,   119,  1284, 14133,   170,  7378,   117,   186,\n",
      "        18413, 21961,  1105,  2178,   118,   130, 17163,  1766,  6713,  1106,\n",
      "         4218,   170,  2463,  1187,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: fit a problem where the true underlying function is quadratic. The linear function is unable to\n",
      "content_token: tensor([  101,  4218,   170,  2463,  1187,  1103,  2276, 10311,  3053,  1110,\n",
      "          186, 18413, 21961,   119,  1109,  7378,  3053,  1110,  3372,  1106,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is unable to capture the curvature in the true underlying problem, so it underfits. The degree-9\n",
      "content_token: tensor([  101,  1110,  3372,  1106,  4821,  1103, 16408, 13461,  5332,  1107,\n",
      "         1103,  2276, 10311,  2463,   117,  1177,  1122,  1223, 14067,  1116,\n",
      "          119,  1109,  2178,   118,   130,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The degree-9 predictor is capable of representing the correct function, but it is also capable of\n",
      "content_token: tensor([  101,  1109,  2178,   118,   130, 17163,  1766,  1110,  4451,  1104,\n",
      "         4311,  1103,  5663,  3053,   117,  1133,  1122,  1110,  1145,  4451,\n",
      "         1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is also capable of representing infinitely many other functions that pass exactly through the\n",
      "content_token: tensor([  101,  1110,  1145,  4451,  1104,  4311, 13157,  1193,  1242,  1168,\n",
      "         4226,  1115,  2789,  2839,  1194,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: exactly through the training points, because we 112 CHAPTER 5. MACHINE LEARNING BASICS have more\n",
      "content_token: tensor([  101,  2839,  1194,  1103,  2013,  1827,   117,  1272,  1195, 11150,\n",
      "         8203,   126,   119, 25424,  3048, 11607,  2036,   149, 12420,  2069,\n",
      "        27451, 11780, 12465, 13882, 12122,  1138,  1167,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: BASICS have more parameters than training examples. We have little chance of choosing a solution\n",
      "content_token: tensor([  101, 12465, 13882, 12122,  1138,  1167, 11934,  1190,  2013,  5136,\n",
      "          119,  1284,  1138,  1376,  2640,  1104, 11027,   170,  5072,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: choosing a solution that generalizes well when so many wildly different solutions exist. In this\n",
      "content_token: tensor([  101, 11027,   170,  5072,  1115,  1704,  9534,  1218,  1165,  1177,\n",
      "         1242, 13999,  1472,  7995,  4056,   119,  1130,  1142,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: exist. In this example, the quadratic model is perfectly matched to the true structure of the task\n",
      "content_token: tensor([  101,  4056,   119,  1130,  1142,  1859,   117,  1103,   186, 18413,\n",
      "        21961,  2235,  1110,  6150, 10260,  1106,  1103,  2276,  2401,  1104,\n",
      "         1103,  4579,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the task so it generalizes well to new data.      \n",
      "content_token: tensor([ 101, 1104, 1103, 4579, 1177, 1122, 1704, 9534, 1218, 1106, 1207, 2233,\n",
      "         119,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content:        Figure 5.2: We fit three models to this example training set. The training\n",
      "content_token: tensor([  101, 15982,   126,   119,   123,   131,  1284,  4218,  1210,  3584,\n",
      "         1106,  1142,  1859,  2013,  1383,   119,  1109,  2013,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: set. The training data was generated synthetically, by randomly sampling x values and choosing y\n",
      "content_token: tensor([  101,  1383,   119,  1109,  2013,  2233,  1108,  6455, 13922,  2716,\n",
      "          117,  1118, 19729, 18200,   193,  4718,  1105, 11027,   194,   102])\n",
      "entity_list: ['output: training data']\n",
      "entity_token: [tensor([5964,  131, 2013, 2233])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and choosing y deterministically by evaluating a quadratic function. (Left)A linear function fit to\n",
      "content_token: tensor([  101,  1105, 11027,   194,  1260,  2083, 25685,  5668,  2716,  1118,\n",
      "        27698,   170,   186, 18413, 21961,  3053,   119,   113,  8123,   114,\n",
      "          138,  7378,  3053,  4218,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: function fit to the data suffers from underfitting—it cannot capture the curvature that is present\n",
      "content_token: tensor([  101,  3053,  4218,  1106,  1103,  2233, 18907,  1121,  1223, 14067,\n",
      "         1916,   783,  1122,  2834,  4821,  1103, 16408, 13461,  5332,  1115,\n",
      "         1110,  1675,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that is present in the data. (Center)A quadratic function fit to the data generalizes well to\n",
      "content_token: tensor([  101,  1115,  1110,  1675,  1107,  1103,  2233,   119,   113,  1945,\n",
      "          114,   138,   186, 18413, 21961,  3053,  4218,  1106,  1103,  2233,\n",
      "         1704,  9534,  1218,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: generalizes well to unseen points. It does not suffer from a significant amount of overfitting or\n",
      "content_token: tensor([  101,  1704,  9534,  1218,  1106, 19508,  1827,   119,  1135,  1674,\n",
      "         1136,  8813,  1121,   170,  2418,  2971,  1104,  1166, 14067,  1916,\n",
      "         1137,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of overfitting or underfitting. (Right)A polynomial of degree 9 fit to the data suffers from\n",
      "content_token: tensor([  101,  1104,  1166, 14067,  1916,  1137,  1223, 14067,  1916,   119,\n",
      "          113,  4114,   114,   138, 19068,  1104,  2178,   130,  4218,  1106,\n",
      "         1103,  2233, 18907,  1121,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: data suffers from overfitting. Here we used the Moore-Penrose pseudoinverse to solve the\n",
      "content_token: tensor([  101,  2233, 18907,  1121,  1166, 14067,  1916,   119,  3446,  1195,\n",
      "         1215,  1103,  4673,   118, 23544, 10127, 23563,  1394, 10840,  1106,\n",
      "         9474,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to solve the underdetermined normal equations. The solution passes through all of the training\n",
      "content_token: tensor([  101,  1106,  9474,  1103,  1223, 26514, 26486,  2999, 11838,   119,\n",
      "         1109,  5072,  4488,  1194,  1155,  1104,  1103,  2013,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: all of the training points exactly, but we have not been lucky enough for it to extract the correct\n",
      "content_token: tensor([  101,  1155,  1104,  1103,  2013,  1827,  2839,   117,  1133,  1195,\n",
      "         1138,  1136,  1151,  6918,  1536,  1111,  1122,  1106, 16143,  1103,\n",
      "         5663,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: extract the correct structure. It now has a deep valley in between two training points that does\n",
      "content_token: tensor([  101, 16143,  1103,  5663,  2401,   119,  1135,  1208,  1144,   170,\n",
      "         1996,  4524,  1107,  1206,  1160,  2013,  1827,  1115,  1674,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: points that does not appear in the true underlying function. It also increases sharply on the left\n",
      "content_token: tensor([  101,  1827,  1115,  1674,  1136,  2845,  1107,  1103,  2276, 10311,\n",
      "         3053,   119,  1135,  1145,  6986,  8930,  1113,  1103,  1286,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sharply on the left side of the data, while the true function decreases in this area. So far we\n",
      "content_token: tensor([  101,  8930,  1113,  1103,  1286,  1334,  1104,  1103,  2233,   117,\n",
      "         1229,  1103,  2276,  3053, 19377,  1107,  1142,  1298,   119,  1573,\n",
      "         1677,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: area. So far we have described only one way of changing a model’s capacity: by changing the number\n",
      "content_token: tensor([ 101, 1298,  119, 1573, 1677, 1195, 1138, 1758, 1178, 1141, 1236, 1104,\n",
      "        4787,  170, 2235,  787,  188, 3211,  131, 1118, 4787, 1103, 1295,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: changing the number of input features it has, and simultaneously adding new parameters associated\n",
      "content_token: tensor([  101,  4787,  1103,  1295,  1104,  7758,  1956,  1122,  1144,   117,\n",
      "         1105,  7344,  5321,  1207, 11934,  2628,   102])\n",
      "entity_list: ['parameters']\n",
      "entity_token: [tensor([11934])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: associated with those features. There are in fact many ways of changing a model’s capacity.\n",
      "content_token: tensor([ 101, 2628, 1114, 1343, 1956,  119, 1247, 1132, 1107, 1864, 1242, 3242,\n",
      "        1104, 4787,  170, 2235,  787,  188, 3211,  119,  102])\n",
      "entity_list: ['model’s capacity']\n",
      "entity_token: [tensor([2235,  787,  188, 3211])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a model’s capacity. Capacity is not determined only by the choice of model. The model specifies\n",
      "content_token: tensor([  101,   170,  2235,   787,   188,  3211,   119, 17212, 19905,  1110,\n",
      "         1136,  3552,  1178,  1118,  1103,  3026,  1104,  2235,   119,  1109,\n",
      "         2235,   188, 25392,  9387,   102])\n",
      "entity_list: ['model’s capacity']\n",
      "entity_token: [tensor([2235,  787,  188, 3211])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The model specifies which family of functions the learning algorithm can choose from when varying\n",
      "content_token: tensor([  101,  1109,  2235,   188, 25392,  9387,  1134,  1266,  1104,  4226,\n",
      "         1103,  3776,  9932,  1169,  4835,  1121,  1165,  9507,   102])\n",
      "entity_list: ['learning algorithm']\n",
      "entity_token: [tensor([3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from when varying the parameters in order to reduce a training objective. This is called the\n",
      "content_token: tensor([  101,  1121,  1165,  9507,  1103, 11934,  1107,  1546,  1106,  4851,\n",
      "          170,  2013,  7649,   119,  1188,  1110,  1270,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This is called the representational capacity of the model. In many cases, finding the best function\n",
      "content_token: tensor([ 101, 1188, 1110, 1270, 1103, 6368, 1348, 3211, 1104, 1103, 2235,  119,\n",
      "        1130, 1242, 2740,  117, 4006, 1103, 1436, 3053,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the best function within this family is a very difficult optimization problem. In practice, the\n",
      "content_token: tensor([  101,  1103,  1436,  3053,  1439,  1142,  1266,  1110,   170,  1304,\n",
      "         2846, 25161,  2463,   119,  1130,  2415,   117,  1103,   102])\n",
      "entity_list: ['optimization problem,']\n",
      "entity_token: [tensor([25161,  2463,   117])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In practice, the learning algorithm does not actually find the best function, but merely one that\n",
      "content_token: tensor([ 101, 1130, 2415,  117, 1103, 3776, 9932, 1674, 1136, 2140, 1525, 1103,\n",
      "        1436, 3053,  117, 1133, 5804, 1141, 1115,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: but merely one that significantly reduces the training error. These additional limitations, such as\n",
      "content_token: tensor([  101,  1133,  5804,  1141,  1115,  5409, 13822,  1103,  2013,  7353,\n",
      "          119,  1636,  2509, 13004,   117,  1216,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: such as 113    CHAPTER 5. MACHINE LEARNING BASICS the imperfection of the optimization\n",
      "content_token: tensor([  101,  1216,  1112, 12206,  8203,   126,   119, 25424,  3048, 11607,\n",
      "         2036,   149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,  1103,\n",
      "        24034,  1200, 11916,  1988,  1104,  1103, 25161,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the optimization algorithm, mean that the learning algorithm’s effective capacity may be less\n",
      "content_token: tensor([  101,  1104,  1103, 25161,  9932,   117,  1928,  1115,  1103,  3776,\n",
      "         9932,   787,   188,  3903,  3211,  1336,  1129,  1750,   102])\n",
      "entity_list: ['optimization algorithm', 'learning algorithm']\n",
      "entity_token: [tensor([25161,  9932]), tensor([3776, 9932])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: may be less than the representational capacity of the model family. Our modern ideas about\n",
      "content_token: tensor([ 101, 1336, 1129, 1750, 1190, 1103, 6368, 1348, 3211, 1104, 1103, 2235,\n",
      "        1266,  119, 3458, 2030, 4133, 1164,  102])\n",
      "entity_list: ['model family']\n",
      "entity_token: [tensor([2235, 1266])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: modern ideas about improving the generalization of machine learning models are refinements of\n",
      "content_token: tensor([  101,  2030,  4133,  1164,  9248,  1103,  1704,  2734,  1104,  3395,\n",
      "         3776,  3584,  1132,  1231, 24191,  4385,  1104,   102])\n",
      "entity_list: ['machine learning models']\n",
      "entity_token: [tensor([3395, 3776, 3584])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are refinements of thought dating back to philosophers at least as early as Ptolemy. Many early\n",
      "content_token: tensor([  101,  1132,  1231, 24191,  4385,  1104,  1354,  4676,  1171,  1106,\n",
      "        20692,  1120,  1655,  1112,  1346,  1112, 26262,   119,  2408,  1346,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Ptolemy. Many early scholars invoke a principle of parsimony that is now most widely known as\n",
      "content_token: tensor([  101, 26262,   119,  2408,  1346,  5716,  1107, 14638,   170,  6708,\n",
      "         1104, 14247,  5053, 27027,  1115,  1110,  1208,  1211,  3409,  1227,\n",
      "         1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: widely known as Occam’s razor (c. 1287-1347). This principle states that among competing hypotheses\n",
      "content_token: tensor([  101,  3409,  1227,  1112,   152, 19495,  1306,   787,   188, 20015,\n",
      "          113,   172,   119, 11965,  1559,   118, 15917,  1559,   114,   119,\n",
      "         1188,  6708,  2231,  1115,  1621,  6259,   177,  1183, 11439, 18769,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: hypotheses that explain known observations equally well, one should choose the “simplest” one. This\n",
      "content_token: tensor([  101,   177,  1183, 11439, 18769,  1115,  4137,  1227,  9959,  7808,\n",
      "         1218,   117,  1141,  1431,  4835,  1103,   789, 23565,   790,  1141,\n",
      "          119,  1188,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: one. This idea was formalized and made more precise in the 20th century by the founders of\n",
      "content_token: tensor([  101,  1141,   119,  1188,  1911,  1108,  4698,  2200,  1105,  1189,\n",
      "         1167, 10515,  1107,  1103,  3116,  1432,  1118,  1103,  9004,  1104,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by the founders of statistical learning theory (Vapnik and Chervonenkis, 1971; Vapnik, 1982; Blumer\n",
      "content_token: tensor([  101,  1118,  1103,  9004,  1104, 11435,  3776,  2749,   113,   159,\n",
      "        11478,  7923,  1105, 20394,  1200, 19988,  1424, 15860,   117,  2507,\n",
      "          132,   159, 11478,  7923,   117,  2294,   132, 15223,  4027,   102])\n",
      "entity_list: ['statistical learning theory', 'Vapnik', 'Chervonenkis', 'Vapnik']\n",
      "entity_token: [tensor([11435,  3776,  2749]), tensor([  159, 11478,  7923]), tensor([20394,  1200, 19988,  1424, 15860]), tensor([  159, 11478,  7923])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 0, 2, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 1982; Blumer et al., 1989; Vapnik, 1995). Statistical learning theory provides various means of\n",
      "content_token: tensor([  101,  2294,   132, 15223,  4027,  3084,  2393,   119,   117,  2056,\n",
      "          132,   159, 11478,  7923,   117,  1876,   114,   119, 12121,  3776,\n",
      "         2749,  2790,  1672,  2086,  1104,   102])\n",
      "entity_list: ['statistical learning theory', 'Vapnik']\n",
      "entity_token: [tensor([11435,  3776,  2749]), tensor([  159, 11478,  7923])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: various means of quantifying model capacity. Among these, the most well-known is the\n",
      "content_token: tensor([  101,  1672,  2086,  1104,   186, 27280,  8985,  2235,  3211,   119,\n",
      "         3841,  1292,   117,  1103,  1211,  1218,   118,  1227,  1110,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: well-known is the Vapnik-Chervonenkis dimension, or VC dimension. The VC dimension measures the\n",
      "content_token: tensor([  101,  1218,   118,  1227,  1110,  1103,   159, 11478,  7923,   118,\n",
      "        20394,  1200, 19988,  1424, 15860, 11025,   117,  1137, 20559, 11025,\n",
      "          119,  1109, 20559, 11025,  5252,  1103,   102])\n",
      "entity_list: ['Vapnik-Chervonenkis dimension', 'VC dimension']\n",
      "entity_token: [tensor([  159, 11478,  7923,   118, 20394,  1200, 19988,  1424, 15860, 11025]), tensor([20559, 11025])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 0, 0, 2, 1,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: measures the capacity of a binary classifier. The VC dimension is defined as being the largest\n",
      "content_token: tensor([  101,  5252,  1103,  3211,  1104,   170, 13480,  1705, 17792,   119,\n",
      "         1109, 20559, 11025,  1110,  3393,  1112,  1217,  1103,  2026,   102])\n",
      "entity_list: ['binary classifier', 'VC dimension']\n",
      "entity_token: [tensor([13480,  1705, 17792]), tensor([20559, 11025])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: being the largest possible value of m for which there exists a training set of m different x points\n",
      "content_token: tensor([ 101, 1217, 1103, 2026, 1936, 2860, 1104,  182, 1111, 1134, 1175, 5903,\n",
      "         170, 2013, 1383, 1104,  182, 1472,  193, 1827,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: different x points that the classifier can label arbitrarily. Quantifying the capacity of the model\n",
      "content_token: tensor([  101,  1472,   193,  1827,  1115,  1103,  1705, 17792,  1169,  3107,\n",
      "          170, 26281,  2875, 22190,  5264,   119,   154, 27280,  8985,  1103,\n",
      "         3211,  1104,  1103,  2235,   102])\n",
      "entity_list: ['classifier', 'model']\n",
      "entity_token: [tensor([ 1705, 17792]), tensor([2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the model allows statistical learning theory to make quantitative predictions. The most\n",
      "content_token: tensor([  101,  1104,  1103,  2235,  3643, 11435,  3776,  2749,  1106,  1294,\n",
      "        25220, 23770,   119,  1109,  1211,   102])\n",
      "entity_list: ['statistical learning theory']\n",
      "entity_token: [tensor([11435,  3776,  2749])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The most important results in statistical learning theory show that the discrepancy between\n",
      "content_token: tensor([  101,  1109,  1211,  1696,  2686,  1107, 11435,  3776,  2749,  1437,\n",
      "         1115,  1103,  6187,  1874, 10224,  3457,  1206,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: discrepancy between training error and generalization error is bounded from above by a quantity\n",
      "content_token: tensor([  101,  6187,  1874, 10224,  3457,  1206,  2013,  7353,  1105,  1704,\n",
      "         2734,  7353,  1110, 10350,  1121,  1807,  1118,   170, 11978,   102])\n",
      "entity_list: ['training error', 'generalization error']\n",
      "entity_token: [tensor([2013, 7353]), tensor([1704, 2734, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: above by a quantity that grows as the model capacity grows but shrinks as the number of training\n",
      "content_token: tensor([  101,  1807,  1118,   170, 11978,  1115,  7096,  1112,  1103,  2235,\n",
      "         3211,  7096,  1133, 26406,  1116,  1112,  1103,  1295,  1104,  2013,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: number of training examples increases (Vapnik and Chervonenkis, 1971; Vapnik, 1982; Blumer et al.,\n",
      "content_token: tensor([  101,  1295,  1104,  2013,  5136,  6986,   113,   159, 11478,  7923,\n",
      "         1105, 20394,  1200, 19988,  1424, 15860,   117,  2507,   132,   159,\n",
      "        11478,  7923,   117,  2294,   132, 15223,  4027,  3084,  2393,   119,\n",
      "          117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Blumer et al., 1989; Vapnik, 1995). These bounds provide intellectual justification that machine\n",
      "content_token: tensor([  101, 15223,  4027,  3084,  2393,   119,   117,  2056,   132,   159,\n",
      "        11478,  7923,   117,  1876,   114,   119,  1636, 22379,  2194,  8066,\n",
      "        22647,  1115,  3395,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that machine learning algorithms can work, but they are rarely used in practice when working with\n",
      "content_token: tensor([  101,  1115,  3395,  3776, 14975,  1169,  1250,   117,  1133,  1152,\n",
      "         1132,  6034,  1215,  1107,  2415,  1165,  1684,  1114,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: when working with deep learning algorithms. This is in part because the bounds are often quite\n",
      "content_token: tensor([  101,  1165,  1684,  1114,  1996,  3776, 14975,   119,  1188,  1110,\n",
      "         1107,  1226,  1272,  1103, 22379,  1132,  1510,  2385,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are often quite loose and in part because it can be quite difficult to determine the capacity of\n",
      "content_token: tensor([ 101, 1132, 1510, 2385, 5768, 1105, 1107, 1226, 1272, 1122, 1169, 1129,\n",
      "        2385, 2846, 1106, 4959, 1103, 3211, 1104,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the capacity of deep learning algorithms. The problem of determining the capacity of a deep\n",
      "content_token: tensor([  101,  1103,  3211,  1104,  1996,  3776, 14975,   119,  1109,  2463,\n",
      "         1104, 13170,  1103,  3211,  1104,   170,  1996,   102])\n",
      "entity_list: ['deep learning algorithms']\n",
      "entity_token: [tensor([ 1996,  3776, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: capacity of a deep learning model is especially difficult because the effective capacity is limited\n",
      "content_token: tensor([ 101, 3211, 1104,  170, 1996, 3776, 2235, 1110, 2108, 2846, 1272, 1103,\n",
      "        3903, 3211, 1110, 2609,  102])\n",
      "entity_list: ['deep learning model']\n",
      "entity_token: [tensor([1996, 3776, 2235])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: capacity is limited by the capabilities of the optimization algorithm, and we have little\n",
      "content_token: tensor([  101,  3211,  1110,  2609,  1118,  1103,  9816,  1104,  1103, 25161,\n",
      "         9932,   117,  1105,  1195,  1138,  1376,   102])\n",
      "entity_list: ['optimization algorithm']\n",
      "entity_token: [tensor([25161,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and we have little theoretical understanding of the very general non-convex optimization problems\n",
      "content_token: tensor([  101,  1105,  1195,  1138,  1376, 10093,  4287,  1104,  1103,  1304,\n",
      "         1704,  1664,   118, 20137, 25161,  2645,   102])\n",
      "entity_list: ['non-convex optimization problems']\n",
      "entity_token: [tensor([ 1664,   118, 20137, 25161,  2645])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: problems involved in deep learning. We must remember that while simpler functions are more likely\n",
      "content_token: tensor([  101,  2645,  2017,  1107,  1996,  3776,   119,  1284,  1538,  2676,\n",
      "         1115,  1229, 17633,  4226,  1132,  1167,  2620,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are more likely to generalize (to have a small gap between training and test error) we must still\n",
      "content_token: tensor([ 101, 1132, 1167, 2620, 1106, 1704, 3708,  113, 1106, 1138,  170, 1353,\n",
      "        7275, 1206, 2013, 1105, 2774, 7353,  114, 1195, 1538, 1253,  102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we must still choose a sufficiently complex hypothesis to achieve low training error. Typically,\n",
      "content_token: tensor([  101,  1195,  1538,  1253,  4835,   170, 13230,  2703, 11066,  1106,\n",
      "         5515,  1822,  2013,  7353,   119, 16304,   117,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error. Typically, training error decreases until it asymptotes to the minimum possible error value\n",
      "content_token: tensor([  101,  7353,   119, 16304,   117,  2013,  7353, 19377,  1235,  1122,\n",
      "         1112, 17162,  6451, 27468,  1106,  1103,  5867,  1936,  7353,  2860,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error value as model capacity increases (assuming the error measure has a minimum value).\n",
      "content_token: tensor([  101,  7353,  2860,  1112,  2235,  3211,  6986,   113, 11577,  1103,\n",
      "         7353,  4929,  1144,   170,  5867,  2860,   114,   119,   102])\n",
      "entity_list: ['model capacity']\n",
      "entity_token: [tensor([2235, 3211])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a minimum value). Typically, generalization error has a U-shaped curve as a function of model\n",
      "content_token: tensor([  101,   170,  5867,  2860,   114,   119, 16304,   117,  1704,  2734,\n",
      "         7353,  1144,   170,   158,   118,  4283,  7660,  1112,   170,  3053,\n",
      "         1104,  2235,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a function of model capacity. This is illustrated in figure 5.3. To reach the most extreme case of\n",
      "content_token: tensor([ 101,  170, 3053, 1104, 2235, 3211,  119, 1188, 1110, 8292, 1107, 2482,\n",
      "         126,  119,  124,  119, 1706, 2519, 1103, 1211, 6122, 1692, 1104,  102])\n",
      "entity_list: ['model capacity']\n",
      "entity_token: [tensor([2235, 3211])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: extreme case of arbitrarily high capacity, we introduce 114 CHAPTER 5. MACHINE LEARNING BASICS\n",
      "content_token: tensor([  101,  6122,  1692,  1104,   170, 26281,  2875, 22190,  5264,  1344,\n",
      "         3211,   117,  1195,  8698, 12620,  8203,   126,   119, 25424,  3048,\n",
      "        11607,  2036,   149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: LEARNING BASICS Training error Underfitting zone Overfittingzone Generalization error\n",
      "content_token: tensor([  101,   149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,  5513,\n",
      "         7353,  2831, 14067,  1916,  4834,  3278, 14067,  1916, 19315,  1615,\n",
      "         2734,  7353,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error Generalization gap 0 Optimal Capacity Capacity Figure 5.3: Typical relationship between\n",
      "content_token: tensor([  101,  7353,  1615,  2734,  7275,   121,  9126,  3121,  7435, 17212,\n",
      "        19905, 17212, 19905, 15982,   126,   119,   124,   131, 23125,  2398,\n",
      "         1206,   102])\n",
      "entity_list: ['Generalization gap']\n",
      "entity_token: [tensor([1615, 2734, 7275])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: between capacity and error. Training and test error behave differently. At the left end of the\n",
      "content_token: tensor([  101,  1206,  3211,  1105,  7353,   119,  5513,  1105,  2774,  7353,\n",
      "        18492, 11677,   119,  1335,  1103,  1286,  1322,  1104,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the left end of the graph, training error and generalization error are both high. This is the\n",
      "content_token: tensor([  101,  1103,  1286,  1322,  1104,  1103, 10873,   117,  2013,  7353,\n",
      "         1105,  1704,  2734,  7353,  1132,  1241,  1344,   119,  1188,  1110,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: high. This is the underfitting regime. As we increase capacity, training error decreases, but the\n",
      "content_token: tensor([  101,  1344,   119,  1188,  1110,  1103,  1223, 14067,  1916,  6716,\n",
      "          119,  1249,  1195,  2773,  3211,   117,  2013,  7353, 19377,   117,\n",
      "         1133,  1103,   102])\n",
      "entity_list: ['capacity']\n",
      "entity_token: [tensor([3211])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: decreases, but the gap between training and generalization error increases. Eventually, the size of\n",
      "content_token: tensor([  101, 19377,   117,  1133,  1103,  7275,  1206,  2013,  1105,  1704,\n",
      "         2734,  7353,  6986,   119,  6382,   117,  1103,  2060,  1104,   102])\n",
      "entity_list: ['training error', 'generalization error']\n",
      "entity_token: [tensor([2013, 7353]), tensor([1704, 2734, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the size of this gap outweighs the decrease in training error, and we enter the overfitting regime,\n",
      "content_token: tensor([  101,  1103,  2060,  1104,  1142,  7275,  1149, 24078,  5084,  1116,\n",
      "         1103,  9711,  1107,  2013,  7353,   117,  1105,  1195,  3873,  1103,\n",
      "         1166, 14067,  1916,  6716,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: overfitting regime, where capacity is too large, above the optimal capacity. the concept of\n",
      "content_token: tensor([  101,  1166, 14067,  1916,  6716,   117,  1187,  3211,  1110,  1315,\n",
      "         1415,   117,  1807,  1103, 17307,  3211,   119,  1103,  3400,  1104,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the concept of non-parametric models. So far, we have seen only parametric models, such as linear\n",
      "content_token: tensor([  101,  1103,  3400,  1104,  1664,   118, 18311, 13689,  3584,   119,\n",
      "         1573,  1677,   117,  1195,  1138,  1562,  1178, 18311, 13689,  3584,\n",
      "          117,  1216,  1112,  7378,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: such as linear regression. Parametric models learn a function described by a parameter vector whose\n",
      "content_token: tensor([  101,  1216,  1112,  7378,  1231, 24032,   119, 23994, 13689,  3584,\n",
      "         3858,   170,  3053,  1758,  1118,   170, 17816,  9479,  2133,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: vector whose size is finite and fixed before any data is observed. Non-parametric models have no\n",
      "content_token: tensor([  101,  9479,  2133,  2060,  1110, 10996,  1105,  4275,  1196,  1251,\n",
      "         2233,  1110,  4379,   119,  7922,   118, 18311, 13689,  3584,  1138,\n",
      "         1185,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: models have no such limitation. Sometimes, non-parametric models are just theoretical abstractions\n",
      "content_token: tensor([  101,  3584,  1138,  1185,  1216, 26802,   119,  5875,   117,  1664,\n",
      "          118, 18311, 13689,  3584,  1132,  1198, 10093, 11108,  5266,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: abstractions (such as an algorithm that searches over all possible probability distributions) that\n",
      "content_token: tensor([  101, 11108,  5266,   113,  1216,  1112,  1126,  9932,  1115, 18806,\n",
      "         1166,  1155,  1936,  9750, 23190,   114,  1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distributions) that cannot be implemented in practice. However, we can also design practical\n",
      "content_token: tensor([  101, 23190,   114,  1115,  2834,  1129,  7042,  1107,  2415,   119,\n",
      "         1438,   117,  1195,  1169,  1145,  1902,  6691,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: design practical non-parametric models by making their complexity a function of the training set\n",
      "content_token: tensor([  101,  1902,  6691,  1664,   118, 18311, 13689,  3584,  1118,  1543,\n",
      "         1147, 12133,   170,  3053,  1104,  1103,  2013,  1383,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the training set size. One example of such an algorithm is nearest neighbor regression. Unlike\n",
      "content_token: tensor([  101,  1104,  1103,  2013,  1383,  2060,   119,  1448,  1859,  1104,\n",
      "         1216,  1126,  9932,  1110,  6830, 12179,  1231, 24032,   119,  5472,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regression. Unlike linear regression, which has a fixed-length vector of weights, the nearest\n",
      "content_token: tensor([  101,  1231, 24032,   119,  5472,  7378,  1231, 24032,   117,  1134,\n",
      "         1144,   170,  4275,   118,  2251,  9479,  1104, 17981,   117,  1103,\n",
      "         6830,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the nearest neighbor regression model simply stores the X and y from the training set. When asked\n",
      "content_token: tensor([  101,  1103,  6830, 12179,  1231, 24032,  2235,  2566,  4822,  1103,\n",
      "          161,  1105,   194,  1121,  1103,  2013,  1383,   119,  1332,  1455,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: set. When asked to classify a test point x, the model looks up the nearest entry in the training\n",
      "content_token: tensor([ 101, 1383,  119, 1332, 1455, 1106, 1705, 6120,  170, 2774, 1553,  193,\n",
      "         117, 1103, 2235, 2736, 1146, 1103, 6830, 3990, 1107, 1103, 2013,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in the training set and returns the associated regression target. In other words, yˆ =y wherei\n",
      "content_token: tensor([  101,  1107,  1103,  2013,  1383,  1105,  5166,  1103,  2628,  1231,\n",
      "        24032,  4010,   119,  1130,  1168,  1734,   117,   100,   134,   194,\n",
      "         1187,  1182,   102])\n",
      "entity_list: ['output: training set']\n",
      "entity_token: [tensor([5964,  131, 2013, 1383])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: words, yˆ =y wherei =argmin X x 2. i i,: 2 || − || The algorithm can also be generalized to\n",
      "content_token: tensor([  101,  1734,   117,   100,   134,   194,  1187,  1182,   134,   170,\n",
      "        10805,  7937,   161,   193,   123,   119,   178,   178,   117,   131,\n",
      "          123,   197,   197,   851,   197,   197,  1109,  9932,  1169,  1145,\n",
      "         1129, 22214,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: be generalized to distance metrics other than the L2 norm, such as learned distance metrics\n",
      "content_token: tensor([  101,  1129, 22214,  1106,  2462, 12676,  1116,  1168,  1190,  1103,\n",
      "          149,  1477, 18570,   117,  1216,  1112,  3560,  2462, 12676,  1116,\n",
      "          102])\n",
      "entity_list: ['L2 norm']\n",
      "entity_token: [tensor([  149,  1477, 18570])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: distance metrics (Goldberger et al., 2005). If the algorithm is allowed to break ties by averaging\n",
      "content_token: tensor([  101,  2462, 12676,  1116,   113, 20029,  1200,  3084,  2393,   119,\n",
      "          117,  1478,   114,   119,  1409,  1103,  9932,  1110,  2148,  1106,\n",
      "         2549,  7057,  1118, 15883,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ties by averaging the y values for all X that are tied for nearest, i i,: then this algorithm is\n",
      "content_token: tensor([  101,  7057,  1118, 15883,  1103,   194,  4718,  1111,  1155,   161,\n",
      "         1115,  1132,  4353,  1111,  6830,   117,   178,   178,   117,   131,\n",
      "         1173,  1142,  9932,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this algorithm is able to achieve the minimum possible training error (which might be greater than\n",
      "content_token: tensor([ 101, 1142, 9932, 1110, 1682, 1106, 5515, 1103, 5867, 1936, 2013, 7353,\n",
      "         113, 1134, 1547, 1129, 3407, 1190,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: be greater than zero, if two identical inputs are associated with different outputs) on any\n",
      "content_token: tensor([  101,  1129,  3407,  1190,  6756,   117,  1191,  1160,  6742, 22743,\n",
      "         1132,  2628,  1114,  1472,  5964,  1116,   114,  1113,  1251,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: outputs) on any regression dataset. Finally, we can also create a non-parametric learning algorithm\n",
      "content_token: tensor([  101,  5964,  1116,   114,  1113,  1251,  1231, 24032,  2233,  9388,\n",
      "          119,  4428,   117,  1195,  1169,  1145,  2561,   170,  1664,   118,\n",
      "        18311, 13689,  3776,  9932,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning algorithm by wrapping a 115 rorrE CHAPTER 5. MACHINE LEARNING BASICS parametric learning\n",
      "content_token: tensor([  101,  3776,  9932,  1118, 13261,   170, 10520,   187,  1766,  1197,\n",
      "         2036,  8203,   126,   119, 25424,  3048, 11607,  2036,   149, 12420,\n",
      "         2069, 27451, 11780, 12465, 13882, 12122, 18311, 13689,  3776,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parametric learning algorithm inside another algorithm that increases the number of parameters as\n",
      "content_token: tensor([  101, 18311, 13689,  3776,  9932,  1656,  1330,  9932,  1115,  6986,\n",
      "         1103,  1295,  1104, 11934,  1112,   102])\n",
      "entity_list: ['parametric learning algorithm']\n",
      "entity_token: [tensor([18311, 13689,  3776,  9932])]\n",
      "label: tensor([0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of parameters as needed. For example, we could imagine an outer loop of learning that changes the\n",
      "content_token: tensor([  101,  1104, 11934,  1112,  1834,   119,  1370,  1859,   117,  1195,\n",
      "         1180,  5403,  1126,  6144,  7812,  1104,  3776,  1115,  2607,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that changes the degree of the polynomial learned by linear regression on top of a polynomial\n",
      "content_token: tensor([  101,  1115,  2607,  1103,  2178,  1104,  1103, 19068,  3560,  1118,\n",
      "         7378,  1231, 24032,  1113,  1499,  1104,   170, 19068,   102])\n",
      "entity_list: ['linear regression']\n",
      "entity_token: [tensor([ 7378,  1231, 24032])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: top of a polynomial expansion of the input. The ideal model is an oracle that simply knows the true\n",
      "content_token: tensor([  101,  1499,  1104,   170, 19068,  4298,  1104,  1103,  7758,   119,\n",
      "         1109,  7891,  2235,  1110,  1126,  1137, 25001,  1115,  2566,  3520,\n",
      "         1103,  2276,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: knows the true probability distribution that generates the data. Even such a model will still incur\n",
      "content_token: tensor([  101,  3520,  1103,  2276,  9750,  3735,  1115, 21241,  1103,  2233,\n",
      "          119,  2431,  1216,   170,  2235,  1209,  1253,  1107, 10182,  1197,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: will still incur some error on many problems, because there may still be some noise in the\n",
      "content_token: tensor([  101,  1209,  1253,  1107, 10182,  1197,  1199,  7353,  1113,  1242,\n",
      "         2645,   117,  1272,  1175,  1336,  1253,  1129,  1199,  4647,  1107,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: some noise in the distribution. In the case of supervised learning, the mapping from x to y may be\n",
      "content_token: tensor([  101,  1199,  4647,  1107,  1103,  3735,   119,  1130,  1103,  1692,\n",
      "         1104, 14199,  3776,   117,  1103, 13970,  1121,   193,  1106,   194,\n",
      "         1336,  1129,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from x to y may be inherently stochastic, or y may be a deterministic function that involves other\n",
      "content_token: tensor([  101,  1121,   193,  1106,   194,  1336,  1129, 17575,  1193,   188,\n",
      "         2430,  7147,  5668,   117,  1137,   194,  1336,  1129,   170,  1260,\n",
      "         2083, 25685,  5668,  3053,  1115,  6808,  1168,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that involves other variables besides those included in x. The error incurred by an oracle making\n",
      "content_token: tensor([  101,  1115,  6808,  1168, 10986,  8655,  1343,  1529,  1107,   193,\n",
      "          119,  1109,  7353, 25240,  1118,  1126,  1137, 25001,  1543,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by an oracle making predictions from the true distribution p(x,y) is called the Bayes error.\n",
      "content_token: tensor([  101,  1118,  1126,  1137, 25001,  1543, 23770,  1121,  1103,  2276,\n",
      "         3735,   185,   113,   193,   117,   194,   114,  1110,  1270,  1103,\n",
      "         2410,  1279,  7353,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the Bayes error. Training and generalization error vary as the size of the training set varies.\n",
      "content_token: tensor([ 101, 1103, 2410, 1279, 7353,  119, 5513, 1105, 1704, 2734, 7353, 7907,\n",
      "        1112, 1103, 2060, 1104, 1103, 2013, 1383, 9544,  119,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: set varies. Expected generalization error can neverincrease as the number of training examples\n",
      "content_token: tensor([  101,  1383,  9544,   119, 16409, 26426,  1174,  1704,  2734,  7353,\n",
      "         1169,  1309,  1394, 13782,  6530,  1112,  1103,  1295,  1104,  2013,\n",
      "         5136,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: training examples increases. For non-parametric models, more data yields better generalization\n",
      "content_token: tensor([  101,  2013,  5136,  6986,   119,  1370,  1664,   118, 18311, 13689,\n",
      "         3584,   117,  1167,  2233, 17376,  1618,  1704,  2734,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: generalization until the best possible error is achieved. Any fixed parametric model with less than\n",
      "content_token: tensor([  101,  1704,  2734,  1235,  1103,  1436,  1936,  7353,  1110,  3890,\n",
      "          119,  6291,  4275, 18311, 13689,  2235,  1114,  1750,  1190,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with less than optimal capacity will asymptote to an error value that exceeds the Bayes error. See\n",
      "content_token: tensor([  101,  1114,  1750,  1190, 17307,  3211,  1209,  1112, 17162,  6451,\n",
      "        11860,  1106,  1126,  7353,  2860,  1115, 26553,  1103,  2410,  1279,\n",
      "         7353,   119,  3969,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Bayes error. See figure 5.4 for an illustration. Note that it is possible for the model to have\n",
      "content_token: tensor([  101,  2410,  1279,  7353,   119,  3969,  2482,   126,   119,   125,\n",
      "         1111,  1126, 17011,   119,  5322,  1115,  1122,  1110,  1936,  1111,\n",
      "         1103,  2235,  1106,  1138,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the model to have optimal capacity and yet still have a large gap between training and\n",
      "content_token: tensor([  101,  1103,  2235,  1106,  1138, 17307,  3211,  1105,  1870,  1253,\n",
      "         1138,   170,  1415,  7275,  1206,  2013,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: training and generalization error. In this situation, we may be able to reduce this gap by\n",
      "content_token: tensor([ 101, 2013, 1105, 1704, 2734, 7353,  119, 1130, 1142, 2820,  117, 1195,\n",
      "        1336, 1129, 1682, 1106, 4851, 1142, 7275, 1118,  102])\n",
      "entity_list: ['training and generalization error']\n",
      "entity_token: [tensor([2013, 1105, 1704, 2734, 7353])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: reduce this gap by gathering more training examples. 5.2.1 The No Free Lunch Theorem Learning\n",
      "content_token: tensor([  101,  4851,  1142,  7275,  1118,  7410,  1167,  2013,  5136,   119,\n",
      "          126,   119,   123,   119,   122,  1109,  1302,  4299, 14557, 11273,\n",
      "        15840, 16996,  9681,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Theorem Learning theory claims that a machine learning algorithm can generalize well from a finite\n",
      "content_token: tensor([  101, 15840, 16996,  9681,  2749,  3711,  1115,   170,  3395,  3776,\n",
      "         9932,  1169,  1704,  3708,  1218,  1121,   170, 10996,   102])\n",
      "entity_list: ['machine learning algorithm']\n",
      "entity_token: [tensor([3395, 3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: well from a finite training set of examples. This seems to contradict some basic principles of\n",
      "content_token: tensor([  101,  1218,  1121,   170, 10996,  2013,  1383,  1104,  5136,   119,\n",
      "         1188,  3093,  1106, 14255,  4487, 28113,  1199,  3501,  6551,  1104,\n",
      "          102])\n",
      "entity_list: ['training set', 'machine learning algorithm']\n",
      "entity_token: [tensor([2013, 1383]), tensor([3395, 3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: basic principles of logic. Inductive reasoning, or inferring general rules from a limited set of\n",
      "content_token: tensor([  101,  3501,  6551,  1104,  8738,   119,  1130, 13890,  2109, 14417,\n",
      "          117,  1137,  1107,  6732,  3384,  1704,  2995,  1121,   170,  2609,\n",
      "         1383,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a limited set of examples, is not logically valid. To logically infer a rule describing every\n",
      "content_token: tensor([  101,   170,  2609,  1383,  1104,  5136,   117,  1110,  1136, 11730,\n",
      "         1193,  9221,   119,  1706, 11730,  1193,  1107,  6732,   170,  3013,\n",
      "         7645,  1451,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: describing every member of a set, one must have information about every member of that set. In\n",
      "content_token: tensor([ 101, 7645, 1451, 1420, 1104,  170, 1383,  117, 1141, 1538, 1138, 1869,\n",
      "        1164, 1451, 1420, 1104, 1115, 1383,  119, 1130,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of that set. In part, machinelearning avoids this problembyoffering only probabilistic rules,\n",
      "content_token: tensor([  101,  1104,  1115,  1383,   119,  1130,  1226,   117,  3395, 19094,\n",
      "         4558,  1158,  3644,  1116,  1142,  2463,  2665,  5792,  5938,  1178,\n",
      "         5250,  2822, 15197,  5562,  2995,   117,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: rules, rather than the entirely certain rules used in purely logical reasoning. Machine learning\n",
      "content_token: tensor([  101,  2995,   117,  1897,  1190,  1103,  3665,  2218,  2995,  1215,\n",
      "         1107, 12098, 11730, 14417,   119,  7792,  3776,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Machine learning promises to find rules that are probably correct about most members of the set\n",
      "content_token: tensor([  101,  7792,  3776, 11323,  1106,  1525,  2995,  1115,  1132,  1930,\n",
      "         5663,  1164,  1211,  1484,  1104,  1103,  1383,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: members of the set they concern. Unfortunately, even this does not resolve the entire problem. The\n",
      "content_token: tensor([  101,  1484,  1104,  1103,  1383,  1152,  4517,   119,  7595,   117,\n",
      "         1256,  1142,  1674,  1136, 10820,  1103,  2072,  2463,   119,  1109,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: entire problem. The no free lunch theorem for machine learning (Wolpert, 1996) states that,\n",
      "content_token: tensor([  101,  2072,  2463,   119,  1109,  1185,  1714,  5953, 10384,  1111,\n",
      "         3395,  3776,   113,   160,  4063, 17786,   117,  1820,   114,  2231,\n",
      "         1115,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 1996) states that, averaged over all possible data generating distributions, every classification\n",
      "content_token: tensor([  101,  1820,   114,  2231,  1115,   117, 11445,  1166,  1155,  1936,\n",
      "         2233, 12713, 23190,   117,  1451,  5393,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: classification algorithm has the same error rate when classifying previously unobserved points. In\n",
      "content_token: tensor([  101,  5393,  9932,  1144,  1103,  1269,  7353,  2603,  1165,  1705,\n",
      "         8985,  2331,  8362, 12809, 17886,  1181,  1827,   119,  1130,   102])\n",
      "entity_list: ['classification algorithm']\n",
      "entity_token: [tensor([5393, 9932])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: points. In other words, in some sense, no machine learning algorithm is universally any better than\n",
      "content_token: tensor([  101,  1827,   119,  1130,  1168,  1734,   117,  1107,  1199,  2305,\n",
      "          117,  1185,  3395,  3776,  9932,  1110, 23578,  1251,  1618,  1190,\n",
      "          102])\n",
      "entity_list: ['machine learning algorithm']\n",
      "entity_token: [tensor([3395, 3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: any better than any other. The most sophisticated algorithm we can conceive of has the same average\n",
      "content_token: tensor([  101,  1251,  1618,  1190,  1251,  1168,   119,  1109,  1211, 12580,\n",
      "         9932,  1195,  1169, 14255, 23566,  1104,  1144,  1103,  1269,  1903,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the same average 116 CHAPTER 5. MACHINE LEARNING BASICS     \n",
      "content_token: tensor([  101,  1103,  1269,  1903, 13096,  8203,   126,   119, 25424,  3048,\n",
      "        11607,  2036,   149, 12420,  2069, 27451, 11780, 12465, 13882, 12122,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content:              \n",
      "content_token: tensor([101, 102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content:                             \n",
      "content_token: tensor([101, 102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content:             Figure 5.4: The effect of the training\n",
      "content_token: tensor([  101, 15982,   126,   119,   125,   131,  1109,  2629,  1104,  1103,\n",
      "         2013,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the training dataset size on the train and test error, as well as on the optimal model capacity.\n",
      "content_token: tensor([  101,  1104,  1103,  2013,  2233,  9388,  2060,  1113,  1103,  2669,\n",
      "         1105,  2774,  7353,   117,  1112,  1218,  1112,  1113,  1103, 17307,\n",
      "         2235,  3211,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: model capacity. We constructed a synthetic regression problem based on adding a moderate amount of\n",
      "content_token: tensor([  101,  2235,  3211,   119,  1284,  3033,   170, 13922,  1231, 24032,\n",
      "         2463,  1359,  1113,  5321,   170,  8828,  2971,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: moderate amount of noise to a degree-5 polynomial, generated a single test set, and then generated\n",
      "content_token: tensor([  101,  8828,  2971,  1104,  4647,  1106,   170,  2178,   118,   126,\n",
      "        19068,   117,  6455,   170,  1423,  2774,  1383,   117,  1105,  1173,\n",
      "         6455,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and then generated several different sizes of training set. For each size, we generated 40\n",
      "content_token: tensor([  101,  1105,  1173,  6455,  1317,  1472, 10855,  1104,  2013,  1383,\n",
      "          119,  1370,  1296,  2060,   117,  1195,  6455,  1969,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we generated 40 different training sets in order to plot error bars showing 95 percent confidence\n",
      "content_token: tensor([ 101, 1195, 6455, 1969, 1472, 2013, 3741, 1107, 1546, 1106, 4928, 7353,\n",
      "        6668, 4000, 4573, 3029, 6595,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: percent confidence intervals. (Top)The MSE on the training and test set for two different models: a\n",
      "content_token: tensor([  101,  3029,  6595, 14662,   119,   113,  3299,   114,  1109, 10978,\n",
      "         2036,  1113,  1103,  2013,  1105,  2774,  1383,  1111,  1160,  1472,\n",
      "         3584,   131,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: different models: a quadratic model, and a model with degree chosen to minimize the test error.\n",
      "content_token: tensor([  101,  1472,  3584,   131,   170,   186, 18413, 21961,  2235,   117,\n",
      "         1105,   170,  2235,  1114,  2178,  3468,  1106, 20220,  1103,  2774,\n",
      "         7353,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the test error. Both are fit in closed form. For the quadratic model, the training error increases\n",
      "content_token: tensor([  101,  1103,  2774,  7353,   119,  2695,  1132,  4218,  1107,  1804,\n",
      "         1532,   119,  1370,  1103,   186, 18413, 21961,  2235,   117,  1103,\n",
      "         2013,  7353,  6986,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error increases as the size of the training set increases. This is because larger datasets are\n",
      "content_token: tensor([  101,  7353,  6986,  1112,  1103,  2060,  1104,  1103,  2013,  1383,\n",
      "         6986,   119,  1188,  1110,  1272,  2610,  2233, 27948,  1132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: larger datasets are harder to fit. Simultaneously, the test error decreases, because fewer\n",
      "content_token: tensor([  101,  2610,  2233, 27948,  1132,  5747,  1106,  4218,   119, 14159,\n",
      "        13601, 23934,  1673,  9537,   117,  1103,  2774,  7353, 19377,   117,\n",
      "         1272,  8307,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: because fewer incorrect hypotheses are consistent with the training data. The quadratic model does\n",
      "content_token: tensor([  101,  1272,  8307, 18238,   177,  1183, 11439, 18769,  1132,  8080,\n",
      "         1114,  1103,  2013,  2233,   119,  1109,   186, 18413, 21961,  2235,\n",
      "         1674,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: model does not have enough capacity to solve the task, so its test error asymptotes to a high\n",
      "content_token: tensor([  101,  2235,  1674,  1136,  1138,  1536,  3211,  1106,  9474,  1103,\n",
      "         4579,   117,  1177,  1157,  2774,  7353,  1112, 17162,  6451, 27468,\n",
      "         1106,   170,  1344,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to a high value. The test error at optimal capacity asymptotes to the Bayes error. The training\n",
      "content_token: tensor([  101,  1106,   170,  1344,  2860,   119,  1109,  2774,  7353,  1120,\n",
      "        17307,  3211,  1112, 17162,  6451, 27468,  1106,  1103,  2410,  1279,\n",
      "         7353,   119,  1109,  2013,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error. The training error can fall below the Bayes error, due to the ability of the training\n",
      "content_token: tensor([ 101, 7353,  119, 1109, 2013, 7353, 1169, 2303, 2071, 1103, 2410, 1279,\n",
      "        7353,  117, 1496, 1106, 1103, 2912, 1104, 1103, 2013,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the training algorithm to memorize specific instances of the training set. As the training size\n",
      "content_token: tensor([  101,  1104,  1103,  2013,  9932,  1106,  1143, 26271,  3708,  2747,\n",
      "        12409,  1104,  1103,  2013,  1383,   119,  1249,  1103,  2013,  2060,\n",
      "          102])\n",
      "entity_list: ['training algorithm']\n",
      "entity_token: [tensor([2013, 9932])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the training size increases to infinity, the training error of any fixed-capacity model (here, the\n",
      "content_token: tensor([  101,  1103,  2013,  2060,  6986,  1106,  1107, 24415,   117,  1103,\n",
      "         2013,  7353,  1104,  1251,  4275,   118,  3211,  2235,   113,  1303,\n",
      "          117,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: model (here, the quadratic model) must rise to at least the Bayes error. (Bottom)As the training\n",
      "content_token: tensor([  101,  2235,   113,  1303,   117,  1103,   186, 18413, 21961,  2235,\n",
      "          114,  1538,  3606,  1106,  1120,  1655,  1103,  2410,  1279,  7353,\n",
      "          119,   113, 19984,   114,  1249,  1103,  2013,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the training set size increases, the optimal capacity (shown here as the degree of the optimal\n",
      "content_token: tensor([  101,  1103,  2013,  1383,  2060,  6986,   117,  1103, 17307,  3211,\n",
      "          113,  2602,  1303,  1112,  1103,  2178,  1104,  1103, 17307,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the optimal polynomial regressor) increases. The optimal capacity plateaus after reaching\n",
      "content_token: tensor([  101,  1104,  1103, 17307, 19068,  1231,  1403,  7370,  1766,   114,\n",
      "         6986,   119,  1109, 17307,  3211, 14404,  1116,  1170,  3634,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: after reaching sufficient complexity to solve the task. 117 \n",
      "content_token: tensor([  101,  1170,  3634,  6664, 12133,  1106,  9474,  1103,  4579,   119,\n",
      "        12737,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 117   CHAPTER 5. MACHINE LEARNING BASICS performance\n",
      "content_token: tensor([  101, 12737,  8203,   126,   119, 25424,  3048, 11607,  2036,   149,\n",
      "        12420,  2069, 27451, 11780, 12465, 13882, 12122,  2099,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: BASICS performance (over all possible tasks) as merely predicting that every point belongs to the\n",
      "content_token: tensor([  101, 12465, 13882, 12122,  2099,   113,  1166,  1155,  1936,  8249,\n",
      "          114,  1112,  5804, 17163,  1158,  1115,  1451,  1553,  7017,  1106,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: belongs to the same class. Fortunately, these results hold only when we average over all possible\n",
      "content_token: tensor([  101,  7017,  1106,  1103,  1269,  1705,   119, 18101,   117,  1292,\n",
      "         2686,  2080,  1178,  1165,  1195,  1903,  1166,  1155,  1936,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: over all possible data generating distributions. If we make assumptions about the kinds of\n",
      "content_token: tensor([  101,  1166,  1155,  1936,  2233, 12713, 23190,   119,  1409,  1195,\n",
      "         1294, 19129,  1164,  1103,  7553,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: about the kinds of probability distributions we encounter in real-world applications, then we can\n",
      "content_token: tensor([  101,  1164,  1103,  7553,  1104,  9750, 23190,  1195,  8107,  1107,\n",
      "         1842,   118,  1362,  4683,   117,  1173,  1195,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: then we can design learning algorithms that perform well on these distributions. This means that\n",
      "content_token: tensor([  101,  1173,  1195,  1169,  1902,  3776, 14975,  1115,  3870,  1218,\n",
      "         1113,  1292, 23190,   119,  1188,  2086,  1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This means that the goal of machine learning research is not to seek a universal learning algorithm\n",
      "content_token: tensor([ 101, 1188, 2086, 1115, 1103, 2273, 1104, 3395, 3776, 1844, 1110, 1136,\n",
      "        1106, 5622,  170, 8462, 3776, 9932,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what\n",
      "content_token: tensor([ 101, 3776, 9932, 1137, 1103, 7846, 1436, 3776, 9932,  119, 3743,  117,\n",
      "        1412, 2273, 1110, 1106, 2437, 1184,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to understand what kinds of distributions are relevant to the “real world” that an AI agent\n",
      "content_token: tensor([  101,  1106,  2437,  1184,  7553,  1104, 23190,  1132,  7503,  1106,\n",
      "         1103,   789,  1842,  1362,   790,  1115,  1126, 19016,  3677,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that an AI agent experiences, and what kinds of machine learning algorithms perform well on data\n",
      "content_token: tensor([  101,  1115,  1126, 19016,  3677,  5758,   117,  1105,  1184,  7553,\n",
      "         1104,  3395,  3776, 14975,  3870,  1218,  1113,  2233,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: well on data drawn from the kinds of data generating distributions we care about. 5.2.2\n",
      "content_token: tensor([  101,  1218,  1113,  2233,  3795,  1121,  1103,  7553,  1104,  2233,\n",
      "        12713, 23190,  1195,  1920,  1164,   119,   126,   119,   123,   119,\n",
      "          123,   102])\n",
      "entity_list: ['data generating distributions']\n",
      "entity_token: [tensor([ 2233, 12713, 23190])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: care about. 5.2.2 Regularization The no free lunch theorem implies that we must design our machine\n",
      "content_token: tensor([  101,  1920,  1164,   119,   126,   119,   123,   119,   123, 14381,\n",
      "         2734,  1109,  1185,  1714,  5953, 10384, 12942,  1115,  1195,  1538,\n",
      "         1902,  1412,  3395,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: design our machine learning algorithms to perform well on a specific task. We do so by building a\n",
      "content_token: tensor([  101,  1902,  1412,  3395,  3776, 14975,  1106,  3870,  1218,  1113,\n",
      "          170,  2747,  4579,   119,  1284,  1202,  1177,  1118,  1459,   170,\n",
      "          102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: do so by building a set of preferences into the learning algorithm. When these preferences are\n",
      "content_token: tensor([  101,  1202,  1177,  1118,  1459,   170,  1383,  1104, 20935,  1154,\n",
      "         1103,  3776,  9932,   119,  1332,  1292, 20935,  1132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: preferences are aligned with the learning problems we ask the algorithm to solve, it performs\n",
      "content_token: tensor([  101, 20935,  1132, 14006,  1114,  1103,  3776,  2645,  1195,  2367,\n",
      "         1103,  9932,  1106,  9474,   117,  1122, 10383,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: solve, it performs better. Sofar, theonly methodof modifyingalearning algorithmthatwehavediscussed\n",
      "content_token: tensor([  101,  9474,   117,  1122, 10383,  1618,   119,  1573, 14794,   117,\n",
      "         1103,  1320,  1193,  3442, 10008, 22015, 28050, 19094,  4558,  1158,\n",
      "         9932,  7702,  1204,  7921, 22145, 10396,  6697,  5591,   102])\n",
      "entity_list: ['learning algorithm']\n",
      "entity_token: [tensor([3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: concretely is to increase or decrease the model’s representationalcapacity by adding or removing\n",
      "content_token: tensor([  101,  5019,  1193,  1110,  1106,  2773,  1137,  9711,  1103,  2235,\n",
      "          787,   188,  6368,  1348, 25265, 19905,  1118,  5321,  1137,  9305,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: adding or removing functions from the hypothesis space of solutions the learning algorithm is able\n",
      "content_token: tensor([  101,  5321,  1137,  9305,  4226,  1121,  1103, 11066,  2000,  1104,\n",
      "         7995,  1103,  3776,  9932,  1110,  1682,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithm is able to choose. We gave the specific example of increasing or decreasing the degree of\n",
      "content_token: tensor([  101,  9932,  1110,  1682,  1106,  4835,   119,  1284,  1522,  1103,\n",
      "         2747,  1859,  1104,  4138,  1137, 18326,  1103,  2178,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the degree of a polynomial for a regression problem. The view we have described so far is\n",
      "content_token: tensor([  101,  1103,  2178,  1104,   170, 19068,  1111,   170,  1231, 24032,\n",
      "         2463,   119,  1109,  2458,  1195,  1138,  1758,  1177,  1677,  1110,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: described so far is oversimplified. The behavior of our algorithm is strongly affected not just by\n",
      "content_token: tensor([  101,  1758,  1177,  1677,  1110, 17074,  4060, 18580,   119,  1109,\n",
      "         4658,  1104,  1412,  9932,  1110,  5473,  4634,  1136,  1198,  1118,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: not just by how large we make the set of functions allowed in its hypothesis space, but by the\n",
      "content_token: tensor([  101,  1136,  1198,  1118,  1293,  1415,  1195,  1294,  1103,  1383,\n",
      "         1104,  4226,  2148,  1107,  1157, 11066,  2000,   117,  1133,  1118,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: space, but by the specific identity of those functions. The learning algorithm we have studied so\n",
      "content_token: tensor([ 101, 2000,  117, 1133, 1118, 1103, 2747, 4193, 1104, 1343, 4226,  119,\n",
      "        1109, 3776, 9932, 1195, 1138, 2376, 1177,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we have studied so far, linear regression, has a hypothesis space consisting of the set of linear\n",
      "content_token: tensor([  101,  1195,  1138,  2376,  1177,  1677,   117,  7378,  1231, 24032,\n",
      "          117,  1144,   170, 11066,  2000,  4721,  1104,  1103,  1383,  1104,\n",
      "         7378,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the set of linear functions of its input. These linear functions can be very useful for problems\n",
      "content_token: tensor([ 101, 1103, 1383, 1104, 7378, 4226, 1104, 1157, 7758,  119, 1636, 7378,\n",
      "        4226, 1169, 1129, 1304, 5616, 1111, 2645,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: useful for problems where the relationship between inputs and outputs truly is close to linear.\n",
      "content_token: tensor([  101,  5616,  1111,  2645,  1187,  1103,  2398,  1206, 22743,  1105,\n",
      "         5964,  1116,  5098,  1110,  1601,  1106,  7378,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is close to linear. They are less useful for problems that behave in a very nonlinear fashion. For\n",
      "content_token: tensor([  101,  1110,  1601,  1106,  7378,   119,  1220,  1132,  1750,  5616,\n",
      "         1111,  2645,  1115, 18492,  1107,   170,  1304,  1664, 24984,  4633,\n",
      "          119,  1370,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: fashion. For example, linear regression would not perform very well if we tried to use it to\n",
      "content_token: tensor([  101,  4633,   119,  1370,  1859,   117,  7378,  1231, 24032,  1156,\n",
      "         1136,  3870,  1304,  1218,  1191,  1195,  1793,  1106,  1329,  1122,\n",
      "         1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: tried to use it to predict sin(x) from x. We can thus control the performance of our algorithms by\n",
      "content_token: tensor([  101,  1793,  1106,  1329,  1122,  1106, 17163, 11850,   113,   193,\n",
      "          114,  1121,   193,   119,  1284,  1169,  2456,  1654,  1103,  2099,\n",
      "         1104,  1412, 14975,  1118,   102])\n",
      "entity_list: ['linear regression']\n",
      "entity_token: [tensor([ 7378,  1231, 24032])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: our algorithms by choosing what kind of functions we allow them to draw solutions from, as well as\n",
      "content_token: tensor([  101,  1412, 14975,  1118, 11027,  1184,  1912,  1104,  4226,  1195,\n",
      "         2621,  1172,  1106,  3282,  7995,  1121,   117,  1112,  1218,  1112,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from, as well as by controlling the amount of these functions. We can also give a learning\n",
      "content_token: tensor([ 101, 1121,  117, 1112, 1218, 1112, 1118, 9783, 1103, 2971, 1104, 1292,\n",
      "        4226,  119, 1284, 1169, 1145, 1660,  170, 3776,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: give a learning algorithm a preference for one solution in its hypothesis space to another. This\n",
      "content_token: tensor([  101,  1660,   170,  3776,  9932,   170, 12629,  1111,  1141,  5072,\n",
      "         1107,  1157, 11066,  2000,  1106,  1330,   119,  1188,   102])\n",
      "entity_list: ['learning algorithm', 'hypothesis space']\n",
      "entity_token: [tensor([3776, 9932]), tensor([11066,  2000])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to another. This means that both functions are eligible, but one is preferred. The unpreferred\n",
      "content_token: tensor([  101,  1106,  1330,   119,  1188,  2086,  1115,  1241,  4226,  1132,\n",
      "         7408,   117,  1133,  1141,  1110,  6349,   119,  1109,  8362,  1643,\n",
      "         1874, 26025,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The unpreferred solution will be chosen only if it fits the training 118 CHAPTER 5. MACHINE\n",
      "content_token: tensor([  101,  1109,  8362,  1643,  1874, 26025,  5072,  1209,  1129,  3468,\n",
      "         1178,  1191,  1122, 17976,  1103,  2013, 13176,  8203,   126,   119,\n",
      "        25424,  3048, 11607,  2036,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 5. MACHINE LEARNING BASICS data significantly better than the preferred solution. For\n",
      "content_token: tensor([  101,  8203,   126,   119, 25424,  3048, 11607,  2036,   149, 12420,\n",
      "         2069, 27451, 11780, 12465, 13882, 12122,  2233,  5409,  1618,  1190,\n",
      "         1103,  6349,  5072,   119,  1370,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: solution. For example, we can modify the training criterion for linear regression to include weight\n",
      "content_token: tensor([  101,  5072,   119,  1370,  1859,   117,  1195,  1169, 22015,  1103,\n",
      "         2013, 26440,  1111,  7378,  1231, 24032,  1106,  1511,  2841,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to include weight decay. To perform linear regression with weight decay, we minimize a sum\n",
      "content_token: tensor([  101,  1106,  1511,  2841, 14352,   119,  1706,  3870,  7378,  1231,\n",
      "        24032,  1114,  2841, 14352,   117,  1195, 20220,   170,  7584,   102])\n",
      "entity_list: ['linear regression', 'weight decay']\n",
      "entity_token: [tensor([ 7378,  1231, 24032]), tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we minimize a sum comprising both the mean squared error on the training and a criterion J(w) that\n",
      "content_token: tensor([  101,  1195, 20220,   170,  7584,  9472,  1241,  1103,  1928, 23215,\n",
      "         7353,  1113,  1103,  2013,  1105,   170, 26440,   147,   113,   192,\n",
      "          114,  1115,   102])\n",
      "entity_list: ['mean squared error']\n",
      "entity_token: [tensor([ 1928, 23215,  7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: criterion J(w) that expresses a preference for the weights to have smaller squaredL2 norm.\n",
      "content_token: tensor([  101, 26440,   147,   113,   192,   114,  1115, 18028,   170, 12629,\n",
      "         1111,  1103, 17981,  1106,  1138,  2964, 23215,  2162,  1477, 18570,\n",
      "          119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: squaredL2 norm. Specifically, J(w) = MSE +λw w, (5.18) train  where λ is a value chosen ahead of\n",
      "content_token: tensor([  101, 23215,  2162,  1477, 18570,   119, 21325,   117,   147,   113,\n",
      "          192,   114,   134, 10978,  2036,   116,   428,  2246,   192,   117,\n",
      "          113,   126,   119,  1407,   114,  2669,  1187,   428,  1110,   170,\n",
      "         2860,  3468,  3075,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: chosen ahead of time that controls the strength of our preference for smaller weights. When λ = 0,\n",
      "content_token: tensor([  101,  3468,  3075,  1104,  1159,  1115,  7451,  1103,  3220,  1104,\n",
      "         1412, 12629,  1111,  2964, 17981,   119,  1332,   428,   134,   121,\n",
      "          117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: When λ = 0, we impose no preference, and larger λ forces the weights to become smaller. Minimizing\n",
      "content_token: tensor([  101,  1332,   428,   134,   121,   117,  1195, 19103,  1185, 12629,\n",
      "          117,  1105,  2610,   428,  2088,  1103, 17981,  1106,  1561,  2964,\n",
      "          119, 14393, 25596,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: smaller. Minimizing J(w) results in a choice of weights that make a tradeoff between fitting the\n",
      "content_token: tensor([  101,  2964,   119, 14393, 25596,   147,   113,   192,   114,  2686,\n",
      "         1107,   170,  3026,  1104, 17981,  1115,  1294,   170,  2597,  5792,\n",
      "         1206, 11732,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: between fitting the training data and being small. This gives us solutions that have a smaller\n",
      "content_token: tensor([  101,  1206, 11732,  1103,  2013,  2233,  1105,  1217,  1353,   119,\n",
      "         1188,  3114,  1366,  7995,  1115,  1138,   170,  2964,   102])\n",
      "entity_list: ['training data']\n",
      "entity_token: [tensor([2013, 2233])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that have a smaller slope, or put weight on fewer of the features. As an example of how we can\n",
      "content_token: tensor([ 101, 1115, 1138,  170, 2964, 9877,  117, 1137, 1508, 2841, 1113, 8307,\n",
      "        1104, 1103, 1956,  119, 1249, 1126, 1859, 1104, 1293, 1195, 1169,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of how we can control a model’s tendency to overfit or underfit via weight decay, we can train a\n",
      "content_token: tensor([  101,  1104,  1293,  1195,  1169,  1654,   170,  2235,   787,   188,\n",
      "        12034,  1106,  1166, 14067,  1137,  1223, 14067,  2258,  2841, 14352,\n",
      "          117,  1195,  1169,  2669,   170,   102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we can train a high-degree polynomial regression model with different values of λ. See figure 5.5\n",
      "content_token: tensor([  101,  1195,  1169,  2669,   170,  1344,   118,  2178, 19068,  1231,\n",
      "        24032,  2235,  1114,  1472,  4718,  1104,   428,   119,  3969,  2482,\n",
      "          126,   119,   126,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: λ. See figure 5.5 for the results.    \n",
      "content_token: tensor([ 101,  428,  119, 3969, 2482,  126,  119,  126, 1111, 1103, 2686,  119,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content:            Figure 5.5: We fit a high-degree polynomial regression\n",
      "content_token: tensor([  101, 15982,   126,   119,   126,   131,  1284,  4218,   170,  1344,\n",
      "          118,  2178, 19068,  1231, 24032,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regression model to our example training set from figure 5.2. The true function is quadratic, but\n",
      "content_token: tensor([  101,  1231, 24032,  2235,  1106,  1412,  1859,  2013,  1383,  1121,\n",
      "         2482,   126,   119,   123,   119,  1109,  2276,  3053,  1110,   186,\n",
      "        18413, 21961,   117,  1133,   102])\n",
      "entity_list: ['output: regression model']\n",
      "entity_token: [tensor([ 5964,   131,  1231, 24032,  2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is quadratic, but here we use only models with degree 9. We vary the amount of weight decay to\n",
      "content_token: tensor([  101,  1110,   186, 18413, 21961,   117,  1133,  1303,  1195,  1329,\n",
      "         1178,  3584,  1114,  2178,   130,   119,  1284,  7907,  1103,  2971,\n",
      "         1104,  2841, 14352,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of weight decay to prevent these high-degree models from overfitting. (Left)With very large λ, we\n",
      "content_token: tensor([  101,  1104,  2841, 14352,  1106,  3843,  1292,  1344,   118,  2178,\n",
      "         3584,  1121,  1166, 14067,  1916,   119,   113,  8123,   114,  1556,\n",
      "         1304,  1415,   428,   117,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: very large λ, we can force the model to learn a function with no slope at all. This underfits\n",
      "content_token: tensor([  101,  1304,  1415,   428,   117,  1195,  1169,  2049,  1103,  2235,\n",
      "         1106,  3858,   170,  3053,  1114,  1185,  9877,  1120,  1155,   119,\n",
      "         1188,  1223, 14067,  1116,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: all. This underfits because it can only represent a constant function. (Center)With a medium value\n",
      "content_token: tensor([  101,  1155,   119,  1188,  1223, 14067,  1116,  1272,  1122,  1169,\n",
      "         1178,  4248,   170,  4836,  3053,   119,   113,  1945,   114,  1556,\n",
      "          170,  5143,  2860,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a medium value of λ, the learning algorithm recovers a curve with the right general shape. Even\n",
      "content_token: tensor([ 101,  170, 5143, 2860, 1104,  428,  117, 1103, 3776, 9932, 8680, 1116,\n",
      "         170, 7660, 1114, 1103, 1268, 1704, 3571,  119, 2431,  102])\n",
      "entity_list: ['learning algorithm']\n",
      "entity_token: [tensor([3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: general shape. Even though the model is capable of representing functions with much more\n",
      "content_token: tensor([ 101, 1704, 3571,  119, 2431, 1463, 1103, 2235, 1110, 4451, 1104, 4311,\n",
      "        4226, 1114, 1277, 1167,  102])\n",
      "entity_list: ['model']\n",
      "entity_token: [tensor([2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with much more complicated shape, weight decay has encouraged it to use a simpler function\n",
      "content_token: tensor([  101,  1114,  1277,  1167,  8277,  3571,   117,  2841, 14352,  1144,\n",
      "         6182,  1122,  1106,  1329,   170, 17633,  3053,   102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a simpler function described by smaller coefficients. (Right)With weight decay approaching zero\n",
      "content_token: tensor([  101,   170, 17633,  3053,  1758,  1118,  2964, 23795,   119,   113,\n",
      "         4114,   114,  1556,  2841, 14352,  8320,  6756,   102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: approaching zero (i.e., using the Moore-Penrose pseudoinverse to solve the underdetermined problem\n",
      "content_token: tensor([  101,  8320,  6756,   113,   178,   119,   174,   119,   117,  1606,\n",
      "         1103,  4673,   118, 23544, 10127, 23563,  1394, 10840,  1106,  9474,\n",
      "         1103,  1223, 26514, 26486,  2463,   102])\n",
      "entity_list: ['Moore-Penrose pseudoinverse', 'underdetermined problem']\n",
      "entity_token: [tensor([ 4673,   118, 23544, 10127, 23563,  1394, 10840]), tensor([ 1223, 26514, 26486,  2463])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1,\n",
      "        1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: problem with minimal regularization), the degree-9 polynomial overfits significantly, as we saw in\n",
      "content_token: tensor([  101,  2463,  1114, 10298,  2366,  2734,   114,   117,  1103,  2178,\n",
      "          118,   130, 19068,  1166, 14067,  1116,  5409,   117,  1112,  1195,\n",
      "         1486,  1107,   102])\n",
      "entity_list: ['regularization']\n",
      "entity_token: [tensor([2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as we saw in figure 5.2. 119   \n",
      "content_token: tensor([  101,  1112,  1195,  1486,  1107,  2482,   126,   119,   123,   119,\n",
      "        13606,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 7. REGULARIZATION FOR DEEP LEARNING 7.1 Parameter Norm Penalties\n",
      "content_token: tensor([  101,  8203,   128,   119,   155, 17020,  2591, 10783, 20595,  5301,\n",
      "        13821, 24805,   143,  9565, 18581, 16668,   149, 12420,  2069, 27451,\n",
      "        11780,   128,   119,   122, 23994, 19401, 16162,  1306, 23544,  1348,\n",
      "         4338,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Norm Penalties Regularizationhasbeenusedfordecadespriortotheadventofdeeplearning. Linear\n",
      "content_token: tensor([  101, 16162,  1306, 23544,  1348,  4338, 14381,  2734, 16481, 11891,\n",
      "         9299,  1174,  2821, 25534,  4704,  1643,  8558,  3740, 12858, 12393,\n",
      "        14850, 10008,  2007,  8043, 19094,  4558,  1158,   119,  2800,  1813,\n",
      "          102])\n",
      "entity_list: ['Regularization', 'deep learning']\n",
      "entity_token: [tensor([14381,  2734]), tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Linear modelssuchaslinear regressionand logisticregressionallowsimple, straightforward, and\n",
      "content_token: tensor([  101,  2800,  1813,  3584,  6385,  7147,  1116, 24984,  1231, 24032,\n",
      "         5709,  9366,  5562,  1874, 24032, 20797, 10732,  4060,  7136,   117,\n",
      "        21546,   117,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and effective regularization strategies. Many regularization approaches are based on limiting the\n",
      "content_token: tensor([  101,  1105,  3903,  2366,  2734, 10700,   119,  2408,  2366,  2734,\n",
      "         8015,  1132,  1359,  1113, 15816,  1103,   102])\n",
      "entity_list: ['regularization strategies']\n",
      "entity_token: [tensor([ 2366,  2734, 10700])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on limiting the capacity of models, such as neural networks, linear regression, or logistic\n",
      "content_token: tensor([  101,  1113, 15816,  1103,  3211,  1104,  3584,   117,  1216,  1112,\n",
      "        18250,  6379,   117,  7378,  1231, 24032,   117,  1137,  9366,  5562,\n",
      "          102])\n",
      "entity_list: ['neural networks']\n",
      "entity_token: [tensor([18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: or logistic regression, by adding a pa- rameter norm penalty Ω(θ) to the objective function J. We\n",
      "content_token: tensor([  101,  1137,  9366,  5562,  1231, 24032,   117,  1118,  5321,   170,\n",
      "          185,  1161,   118, 26084, 24951, 18570,  6180,   413,   113,   425,\n",
      "          114,  1106,  1103,  7649,  3053,   147,   119,  1284,   102])\n",
      "entity_list: ['logistic regression', 'parameter norm penalty', 'objective function']\n",
      "entity_token: [tensor([ 9366,  5562,  1231, 24032]), tensor([17816, 18570,  6180]), tensor([7649, 3053])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: function J. We denote the regularized ˜ objective function by J: ˜ J(θ;X,y) = J(θ;X,y)+αΩ(θ) (7.1)\n",
      "content_token: tensor([  101,  3053,   147,   119,  1284, 21185,  1103,  2366,  2200,   100,\n",
      "         7649,  3053,  1118,   147,   131,   100,   147,   113,   425,   132,\n",
      "          161,   117,   194,   114,   134,   147,   113,   425,   132,   161,\n",
      "          117,   194,   114,   116,   418, 28334,   113,   425,   114,   113,\n",
      "          128,   119,   122,   114,   102])\n",
      "entity_list: ['objective function', 'regularized objective function']\n",
      "entity_token: [tensor([7649, 3053]), tensor([2366, 2200, 7649, 3053])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (7.1) where α [0, ) is a hyperparameter that weights the relative contribution of the ∈ ∞ norm\n",
      "content_token: tensor([  101,   113,   128,   119,   122,   114,  1187,   418,   164,   121,\n",
      "          117,   114,  1110,   170,   177, 24312, 17482, 16470,  2083,  1115,\n",
      "        17981,  1103,  5236,  6436,  1104,  1103,   850,   855, 18570,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the ∈ ∞ norm penalty term, Ω, relative to the standard objective function J. Setting α to 0\n",
      "content_token: tensor([  101,  1104,  1103,   850,   855, 18570,  6180,  1858,   117,   413,\n",
      "          117,  5236,  1106,  1103,  2530,  7649,  3053,   147,   119,  9617,\n",
      "         1916,   418,  1106,   121,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: J. Setting α to 0 results in no regularization. Larger values of α correspond to more\n",
      "content_token: tensor([  101,   147,   119,  9617,  1916,   418,  1106,   121,  2686,  1107,\n",
      "         1185,  2366,  2734,   119, 10236,  1197,  4718,  1104,   418, 18420,\n",
      "         1106,  1167,   102])\n",
      "entity_list: ['regularization']\n",
      "entity_token: [tensor([2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: correspond to more regularization. ˜ When our training algorithm minimizes the regularized\n",
      "content_token: tensor([  101, 18420,  1106,  1167,  2366,  2734,   119,   100,  1332,  1412,\n",
      "         2013,  9932, 20220,  1116,  1103,  2366,  2200,   102])\n",
      "entity_list: ['regularization']\n",
      "entity_token: [tensor([2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the regularized objective function J it will decrease both the original objective J on the training\n",
      "content_token: tensor([ 101, 1103, 2366, 2200, 7649, 3053,  147, 1122, 1209, 9711, 1241, 1103,\n",
      "        1560, 7649,  147, 1113, 1103, 2013,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: J on the training data and some measure of the size of the parameters θ (or some subset of the\n",
      "content_token: tensor([  101,   147,  1113,  1103,  2013,  2233,  1105,  1199,  4929,  1104,\n",
      "         1103,  2060,  1104,  1103, 11934,   425,   113,  1137,  1199, 18005,\n",
      "         1104,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: some subset of the parameters). Different choices for the parameter norm Ω can result in different\n",
      "content_token: tensor([  101,  1199, 18005,  1104,  1103, 11934,   114,   119, 14380,  9940,\n",
      "         1111,  1103, 17816, 18570,   413,  1169,  1871,  1107,  1472,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: result in different solutions being preferred. In this section, we discuss the effects of the\n",
      "content_token: tensor([ 101, 1871, 1107, 1472, 7995, 1217, 6349,  119, 1130, 1142, 2237,  117,\n",
      "        1195, 6265, 1103, 3154, 1104, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the effects of the various norms when used as penalties on the model parameters. Before delving\n",
      "content_token: tensor([  101,  1103,  3154,  1104,  1103,  1672, 19600,  1165,  1215,  1112,\n",
      "        13095,  1113,  1103,  2235, 11934,   119,  2577,  3687,  3970,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Before delving into the regularization behavior of different norms, we note that for neural\n",
      "content_token: tensor([  101,  2577,  3687,  3970,  1154,  1103,  2366,  2734,  4658,  1104,\n",
      "         1472, 19600,   117,  1195,  3805,  1115,  1111, 18250,   102])\n",
      "entity_list: ['regularization']\n",
      "entity_token: [tensor([2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that for neural networks, we typically choose to use a parameter norm penalty Ω that penalizes only\n",
      "content_token: tensor([  101,  1115,  1111, 18250,  6379,   117,  1195,  3417,  4835,  1106,\n",
      "         1329,   170, 17816, 18570,  6180,   413,  1115,  8228, 10584, 11846,\n",
      "         1178,   102])\n",
      "entity_list: ['neural networks']\n",
      "entity_token: [tensor([18250,  6379])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that penalizes only the weights of the affine transformation at each layer and leaves the biases\n",
      "content_token: tensor([  101,  1115,  8228, 10584, 11846,  1178,  1103, 17981,  1104,  1103,\n",
      "          170, 16274,  1162,  9047,  1120,  1296,  6440,  1105,  2972,  1103,\n",
      "        15069,  1279,   102])\n",
      "entity_list: ['layer', 'biases']\n",
      "entity_token: [tensor([6440]), tensor([15069,  1279])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: leaves the biases unregularized. The biases typically require less data to fit accurately than the\n",
      "content_token: tensor([  101,  2972,  1103, 15069,  1279,  8362,  1874, 13830,  5815,  2200,\n",
      "          119,  1109, 15069,  1279,  3417,  4752,  1750,  2233,  1106,  4218,\n",
      "        14702,  1190,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: accurately than the weights. Each weight specifies how two variables interact. Fitting the weight\n",
      "content_token: tensor([  101, 14702,  1190,  1103, 17981,   119,  2994,  2841,   188, 25392,\n",
      "         9387,  1293,  1160, 10986, 12254,   119, 17355, 19162,  1103,  2841,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Fitting the weight well requires observing both variables in a variety of conditions. Each bias\n",
      "content_token: tensor([  101, 17355, 19162,  1103,  2841,  1218,  5315, 15639,  1241, 10986,\n",
      "         1107,   170,  2783,  1104,  2975,   119,  2994, 15069,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Each bias controls only a single variable. This means that we do not induce too much variance by\n",
      "content_token: tensor([  101,  2994, 15069,  7451,  1178,   170,  1423,  7898,   119,  1188,\n",
      "         2086,  1115,  1195,  1202,  1136, 21497,  1315,  1277, 26717,  1118,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: much variance by leaving the biases unregularized. Also, regularizing the bias parameters can\n",
      "content_token: tensor([  101,  1277, 26717,  1118,  2128,  1103, 15069,  1279,  8362,  1874,\n",
      "        13830,  5815,  2200,   119,  2907,   117,  2366,  4404,  1103, 15069,\n",
      "        11934,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: bias parameters can introduce a significant amount of underfitting. We therefore use the vector w\n",
      "content_token: tensor([  101, 15069, 11934,  1169,  8698,   170,  2418,  2971,  1104,  1223,\n",
      "        14067,  1916,   119,  1284,  3335,  1329,  1103,  9479,   192,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: use the vector w to indicate all of the weights that should be affected by a norm penalty, while\n",
      "content_token: tensor([  101,  1329,  1103,  9479,   192,  1106,  5057,  1155,  1104,  1103,\n",
      "        17981,  1115,  1431,  1129,  4634,  1118,   170, 18570,  6180,   117,\n",
      "         1229,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: norm penalty, while the vector θ denotes all of the parameters, including both w and the\n",
      "content_token: tensor([  101, 18570,  6180,   117,  1229,  1103,  9479,   425, 16699,  1155,\n",
      "         1104,  1103, 11934,   117,  1259,  1241,   192,  1105,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: both w and the unregularized parameters. In the context of neural networks, it is sometimes\n",
      "content_token: tensor([  101,  1241,   192,  1105,  1103,  8362,  1874, 13830,  5815,  2200,\n",
      "        11934,   119,  1130,  1103,  5618,  1104, 18250,  6379,   117,  1122,\n",
      "         1110,  2121,   102])\n",
      "entity_list: [\"'neural networks'\"]\n",
      "entity_token: [tensor([  112, 18250,  6379,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it is sometimes desirable to use a separate penalty with a different α coefficient for each layer\n",
      "content_token: tensor([  101,  1122,  1110,  2121, 17483,  1106,  1329,   170,  2767,  6180,\n",
      "         1114,   170,  1472,   418, 21130,  1111,  1296,  6440,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for each layer of the network. Because it can be expensive to search for the correct value of\n",
      "content_token: tensor([ 101, 1111, 1296, 6440, 1104, 1103, 2443,  119, 2279, 1122, 1169, 1129,\n",
      "        5865, 1106, 3403, 1111, 1103, 5663, 2860, 1104,  102])\n",
      "entity_list: [\"'network'\"]\n",
      "entity_token: [tensor([ 112, 2443,  112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: correct value of multiple hyperparameters, it is still reasonable to use the same weight decay at\n",
      "content_token: tensor([  101,  5663,  2860,  1104,  2967,   177, 24312, 17482, 16470,  5759,\n",
      "          117,  1122,  1110,  1253,  9483,  1106,  1329,  1103,  1269,  2841,\n",
      "        14352,  1120,   102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: weight decay at all layers just to reduce the size of search space. 230 CHAPTER 7. REGULARIZATION\n",
      "content_token: tensor([  101,  2841, 14352,  1120,  1155,  8798,  1198,  1106,  4851,  1103,\n",
      "         2060,  1104,  3403,  2000,   119, 11866,  8203,   128,   119,   155,\n",
      "        17020,  2591, 10783, 20595,  5301, 13821, 24805,   102])\n",
      "entity_list: ['weight decay', 'REGULARIZATION']\n",
      "entity_token: [tensor([ 2841, 14352]), tensor([  155, 17020,  2591, 10783, 20595,  5301, 13821, 24805])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 7. REGULARIZATION FOR DEEP LEARNING 7.1.1 L2 Parameter Regularization We have already seen, in\n",
      "content_token: tensor([  101,   128,   119,   155, 17020,  2591, 10783, 20595,  5301, 13821,\n",
      "        24805,   143,  9565, 18581, 16668,   149, 12420,  2069, 27451, 11780,\n",
      "          128,   119,   122,   119,   122,   149,  1477, 23994, 19401, 14381,\n",
      "         2734,  1284,  1138,  1640,  1562,   117,  1107,   102])\n",
      "entity_list: ['L2 Parameter Regularization', 'DEEP LEARNING']\n",
      "entity_token: [tensor([  149,  1477, 23994, 19401, 14381,  2734]), tensor([18581, 16668,   149, 12420,  2069, 27451, 11780])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: already seen, in section 5.2.2, one of the simplest and most common kinds of parameter norm\n",
      "content_token: tensor([  101,  1640,  1562,   117,  1107,  2237,   126,   119,   123,   119,\n",
      "          123,   117,  1141,  1104,  1103, 23565,  1105,  1211,  1887,  7553,\n",
      "         1104, 17816, 18570,   102])\n",
      "entity_list: ['parameter norm']\n",
      "entity_token: [tensor([17816, 18570])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of parameter norm penalty: the L2 parameter norm penalty commonly known as weight decay. This\n",
      "content_token: tensor([  101,  1104, 17816, 18570,  6180,   131,  1103,   149,  1477, 17816,\n",
      "        18570,  6180,  3337,  1227,  1112,  2841, 14352,   119,  1188,   102])\n",
      "entity_list: ['L2 parameter norm penalty', 'weight decay']\n",
      "entity_token: [tensor([  149,  1477, 17816, 18570,  6180]), tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: weight decay. This regularization strategy drives the weights closer to the origin1 by adding a\n",
      "content_token: tensor([  101,  2841, 14352,   119,  1188,  2366,  2734,  5564,  9307,  1103,\n",
      "        17981,  2739,  1106,  1103,  4247,  1475,  1118,  5321,   170,   102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: origin1 by adding a regularization term Ω(θ) = 1 w 2 to the objective function. In other 2 2\n",
      "content_token: tensor([ 101, 4247, 1475, 1118, 5321,  170, 2366, 2734, 1858,  413,  113,  425,\n",
      "         114,  134,  122,  192,  123, 1106, 1103, 7649, 3053,  119, 1130, 1168,\n",
      "         123,  123,  102])\n",
      "entity_list: ['regularization term']\n",
      "entity_token: [tensor([2366, 2734, 1858])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In other 2 2 academic communities, L2 regularization is also known as ridge regression or\n",
      "content_token: tensor([  101,  1130,  1168,   123,   123,  3397,  3611,   117,   149,  1477,\n",
      "         2366,  2734,  1110,  1145,  1227,  1112,  8699,  1231, 24032,  1137,\n",
      "          102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ridge regression or Tikhonov regularization. We can gain some insight into the behavior of weight\n",
      "content_token: tensor([  101,  8699,  1231, 24032,  1137,   157,  4847,  8613,  3292,  2366,\n",
      "         2734,   119,  1284,  1169,  4361,  1199, 14222,  1154,  1103,  4658,\n",
      "         1104,  2841,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: behavior of weight decay regularization by studying the gradient of the regularized objective\n",
      "content_token: tensor([  101,  4658,  1104,  2841, 14352,  2366,  2734,  1118,  5076,  1103,\n",
      "        19848,  1104,  1103,  2366,  2200,  7649,   102])\n",
      "entity_list: ['weight decay regularization']\n",
      "entity_token: [tensor([ 2841, 14352,  2366,  2734])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: objective function. To simplify the presentation, we assume no bias parameter, so θ is just w. Such\n",
      "content_token: tensor([  101,  7649,  3053,   119,  1706, 27466,  8223, 22881,  1103,  8685,\n",
      "          117,  1195,  7568,  1185, 15069, 17816,   117,  1177,   425,  1110,\n",
      "         1198,   192,   119,  5723,   102])\n",
      "entity_list: ['objective function']\n",
      "entity_token: [tensor([7649, 3053])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: θ is just w. Such a model has the following total objective function: α ˜ J(w;X,y) = w w+J(w;X,y),\n",
      "content_token: tensor([ 101,  425, 1110, 1198,  192,  119, 5723,  170, 2235, 1144, 1103, 1378,\n",
      "        1703, 7649, 3053,  131,  418,  100,  147,  113,  192,  132,  161,  117,\n",
      "         194,  114,  134,  192,  192,  116,  147,  113,  192,  132,  161,  117,\n",
      "         194,  114,  117,  102])\n",
      "entity_list: ['model']\n",
      "entity_token: [tensor([2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: = w w+J(w;X,y), (7.2)  2 with the corresponding parameter gradient ˜ J(w;X,y) = αw + J(w;X,y).\n",
      "content_token: tensor([  101,   134,   192,   192,   116,   147,   113,   192,   132,   161,\n",
      "          117,   194,   114,   117,   113,   128,   119,   123,   114,   123,\n",
      "         1114,  1103,  7671, 17816, 19848,   100,   147,   113,   192,   132,\n",
      "          161,   117,   194,   114,   134,   418,  2246,   116,   147,   113,\n",
      "          192,   132,   161,   117,   194,   114,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: = αw + J(w;X,y). (7.3) w w ∇ ∇ To take a single gradient step to update the weights, we perform\n",
      "content_token: tensor([  101,   134,   418,  2246,   116,   147,   113,   192,   132,   161,\n",
      "          117,   194,   114,   119,   113,   128,   119,   124,   114,   192,\n",
      "          192,   100,   100,  1706,  1321,   170,  1423, 19848,  2585,  1106,\n",
      "        11984,  1103, 17981,   117,  1195,  3870,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: weights, we perform this update: w w (αw + J(w;X,y)). (7.4) w ← − ∇ Written another way, the\n",
      "content_token: tensor([  101, 17981,   117,  1195,  3870,  1142, 11984,   131,   192,   192,\n",
      "          113,   418,  2246,   116,   147,   113,   192,   132,   161,   117,\n",
      "          194,   114,   114,   119,   113,   128,   119,   125,   114,   192,\n",
      "          843,   851,   100, 13404,  1330,  1236,   117,  1103,   102])\n",
      "entity_list: ['weights']\n",
      "entity_token: [tensor([17981])]\n",
      "label: tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: another way, the update is: w (1 α)w  J(w;X,y). (7.5) w ← − − ∇ We can see that the addition of\n",
      "content_token: tensor([  101,  1330,  1236,   117,  1103, 11984,  1110,   131,   192,   113,\n",
      "          122,   418,   114,   192,   147,   113,   192,   132,   161,   117,\n",
      "          194,   114,   119,   113,   128,   119,   126,   114,   192,   843,\n",
      "          851,   851,   100,  1284,  1169,  1267,  1115,  1103,  1901,  1104,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the addition of the weight decay term has modified the learning rule to multiplicatively shrink the\n",
      "content_token: tensor([  101,  1103,  1901,  1104,  1103,  2841, 14352,  1858,  1144,  5847,\n",
      "         1103,  3776,  3013,  1106,  4321,  1643,  9538,  3946,  1193, 26406,\n",
      "         1103,   102])\n",
      "entity_list: ['weight decay term']\n",
      "entity_token: [tensor([ 2841, 14352,  1858])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: shrink the weight vector by a constant factor on each step, just before performing the usual\n",
      "content_token: tensor([  101, 26406,  1103,  2841,  9479,  1118,   170,  4836,  5318,  1113,\n",
      "         1296,  2585,   117,  1198,  1196,  4072,  1103,  4400,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the usual gradient update. This describes what happens in a single step. But what happens over the\n",
      "content_token: tensor([  101,  1103,  4400, 19848, 11984,   119,  1188,  4856,  1184,  5940,\n",
      "         1107,   170,  1423,  2585,   119,  1252,  1184,  5940,  1166,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: happens over the entire course of training? We will further simplify the analysis by making a\n",
      "content_token: tensor([  101,  5940,  1166,  1103,  2072,  1736,  1104,  2013,   136,  1284,\n",
      "         1209,  1748, 27466,  8223, 22881,  1103,  3622,  1118,  1543,   170,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by making a quadratic approximation to the objective function in the neighborhood of the value of\n",
      "content_token: tensor([  101,  1118,  1543,   170,   186, 18413, 21961, 22519,  1106,  1103,\n",
      "         7649,  3053,  1107,  1103,  4532,  1104,  1103,  2860,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the value of the weights that obtains minimal unregularized training cost, w = argmin J(w). If\n",
      "content_token: tensor([  101,  1104,  1103,  2860,  1104,  1103, 17981,  1115,  6268,  1116,\n",
      "        10298,  8362,  1874, 13830,  5815,  2200,  2013,  2616,   117,   192,\n",
      "          134,   170, 10805,  7937,   147,   113,   192,   114,   119,  1409,\n",
      "          102])\n",
      "entity_list: ['training cost']\n",
      "entity_token: [tensor([2013, 2616])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: w = argmin J(w). If the objective ∗ w function is truly quadratic, as in the case of fitting a\n",
      "content_token: tensor([  101,   192,   134,   170, 10805,  7937,   147,   113,   192,   114,\n",
      "          119,  1409,  1103,  7649,   852,   192,  3053,  1110,  5098,   186,\n",
      "        18413, 21961,   117,  1112,  1107,  1103,  1692,  1104, 11732,   170,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: case of fitting a linear regression model with 1More generally, we could regularize the parameters\n",
      "content_token: tensor([  101,  1692,  1104, 11732,   170,  7378,  1231, 24032,  2235,  1114,\n",
      "          122,  2107,  4474,  2412,   117,  1195,  1180,  2366,  3708,  1103,\n",
      "        11934,   102])\n",
      "entity_list: ['linear regression model']\n",
      "entity_token: [tensor([ 7378,  1231, 24032,  2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the parameters to be near any specific point in space and, surprisingly, still get a regularization\n",
      "content_token: tensor([  101,  1103, 11934,  1106,  1129,  1485,  1251,  2747,  1553,  1107,\n",
      "         2000,  1105,   117, 12283,   117,  1253,  1243,   170,  2366,  2734,\n",
      "          102])\n",
      "entity_list: ['regularization']\n",
      "entity_token: [tensor([2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a regularization effect, but better results will be obtained for a value closer to the true one,\n",
      "content_token: tensor([ 101,  170, 2366, 2734, 2629,  117, 1133, 1618, 2686, 1209, 1129, 3836,\n",
      "        1111,  170, 2860, 2739, 1106, 1103, 2276, 1141,  117,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to the true one, with zero being a default value that makes sense when we do not know if the\n",
      "content_token: tensor([  101,  1106,  1103,  2276,  1141,   117,  1114,  6756,  1217,   170,\n",
      "        12973,  2860,  1115,  2228,  2305,  1165,  1195,  1202,  1136,  1221,\n",
      "         1191,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: do not know if the correct value should be positive or negative. Since it is far more common to\n",
      "content_token: tensor([ 101, 1202, 1136, 1221, 1191, 1103, 5663, 2860, 1431, 1129, 3112, 1137,\n",
      "        4366,  119, 1967, 1122, 1110, 1677, 1167, 1887, 1106,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: far more common to regularize the model parameters towards zero, we will focus on this special case\n",
      "content_token: tensor([  101,  1677,  1167,  1887,  1106,  2366,  3708,  1103,  2235, 11934,\n",
      "         2019,  6756,   117,  1195,  1209,  2817,  1113,  1142,  1957,  1692,\n",
      "          102])\n",
      "entity_list: ['model parameters']\n",
      "entity_token: [tensor([ 2235, 11934])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this special case in our exposition. 231 CHAPTER 7. REGULARIZATION FOR DEEP LEARNING ˆ mean squared\n",
      "content_token: tensor([  101,  1142,  1957,  1692,  1107,  1412, 27634,   119, 22154,  8203,\n",
      "          128,   119,   155, 17020,  2591, 10783, 20595,  5301, 13821, 24805,\n",
      "          143,  9565, 18581, 16668,   149, 12420,  2069, 27451, 11780,   100,\n",
      "         1928, 23215,   102])\n",
      "entity_list: ['REGULARIZATION FOR DEEP LEARNING']\n",
      "entity_token: [tensor([  155, 17020,  2591, 10783, 20595,  5301, 13821, 24805,   143,  9565,\n",
      "        18581, 16668,   149, 12420,  2069, 27451, 11780])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ˆ mean squared error, then the approximation is perfect. The approximation J is given by 1 ˆ J(θ) =\n",
      "content_token: tensor([  101,   100,  1928, 23215,  7353,   117,  1173,  1103, 22519,  1110,\n",
      "         3264,   119,  1109, 22519,   147,  1110,  1549,  1118,   122,   100,\n",
      "          147,   113,   425,   114,   134,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: given by 1 ˆ J(θ) = J(w )+ (w w ) H(w w ), (7.6) ∗ ∗  ∗ 2 − − where H is the Hessian matrix of J\n",
      "content_token: tensor([  101,  1549,  1118,   122,   100,   147,   113,   425,   114,   134,\n",
      "          147,   113,   192,   114,   116,   113,   192,   192,   114,   145,\n",
      "          113,   192,   192,   114,   117,   113,   128,   119,   127,   114,\n",
      "          852,   852,   852,   123,   851,   851,  1187,   145,  1110,  1103,\n",
      "        26349,  1811,  8952,  1104,   147,   102])\n",
      "entity_list: ['Hessian matrix']\n",
      "entity_token: [tensor([26349,  1811,  8952])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Hessian matrix of J with respect to w evaluated at w . There is ∗ no first-order term in this\n",
      "content_token: tensor([  101, 26349,  1811,  8952,  1104,   147,  1114,  4161,  1106,   192,\n",
      "        17428,  1120,   192,   119,  1247,  1110,   852,  1185,  1148,   118,\n",
      "         1546,  1858,  1107,  1142,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: term in this quadratic approximation, because w is defined to be a ∗ minimum, where the gradient\n",
      "content_token: tensor([  101,  1858,  1107,  1142,   186, 18413, 21961, 22519,   117,  1272,\n",
      "          192,  1110,  3393,  1106,  1129,   170,   852,  5867,   117,  1187,\n",
      "         1103, 19848,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: where the gradient vanishes. Likewise, because w is the location of a ∗ minimum of J, we can\n",
      "content_token: tensor([  101,  1187,  1103, 19848,  3498, 19033,   119, 18872,   117,  1272,\n",
      "          192,  1110,  1103,  2450,  1104,   170,   852,  5867,  1104,   147,\n",
      "          117,  1195,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of J, we can conclude that H is positive semidefinite. ˆ The minimum of J occurs where its gradient\n",
      "content_token: tensor([  101,  1104,   147,   117,  1195,  1169, 17581,  1115,   145,  1110,\n",
      "         3112,  3533, 28090,   119,   100,  1109,  5867,  1104,   147,  4365,\n",
      "         1187,  1157, 19848,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: where its gradient ˆ J(w) = H(w w ) (7.7) w ∗ ∇ − is equal to 0. To study the effect of weight\n",
      "content_token: tensor([  101,  1187,  1157, 19848,   100,   147,   113,   192,   114,   134,\n",
      "          145,   113,   192,   192,   114,   113,   128,   119,   128,   114,\n",
      "          192,   852,   100,   851,  1110,  4463,  1106,   121,   119,  1706,\n",
      "         2025,  1103,  2629,  1104,  2841,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: effect of weight decay, we modify equation 7.7 by adding the weight decay gradient. We can now\n",
      "content_token: tensor([  101,  2629,  1104,  2841, 14352,   117,  1195, 22015,  8381,   128,\n",
      "          119,   128,  1118,  5321,  1103,  2841, 14352, 19848,   119,  1284,\n",
      "         1169,  1208,   102])\n",
      "entity_list: ['weight decay gradient']\n",
      "entity_token: [tensor([ 2841, 14352, 19848])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: We can now solve for the minimum of the regularized ˆ version of J. We use the variable w˜ to\n",
      "content_token: tensor([ 101, 1284, 1169, 1208, 9474, 1111, 1103, 5867, 1104, 1103, 2366, 2200,\n",
      "         100, 1683, 1104,  147,  119, 1284, 1329, 1103, 7898,  100, 1106,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the variable w˜ to represent the location of the minimum. αw˜ +H(w˜ w ) = 0 (7.8) ∗ − (H +αI)w˜ =\n",
      "content_token: tensor([ 101, 1103, 7898,  100, 1106, 4248, 1103, 2450, 1104, 1103, 5867,  119,\n",
      "         100,  116,  145,  113,  100,  192,  114,  134,  121,  113,  128,  119,\n",
      "         129,  114,  852,  851,  113,  145,  116,  418, 2240,  114,  100,  134,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∗ − (H +αI)w˜ = Hw (7.9) ∗ w˜ = (H +αI) 1 Hw . (7.10) − ∗ As α approaches 0, the regularized\n",
      "content_token: tensor([ 101,  852,  851,  113,  145,  116,  418, 2240,  114,  100,  134,  145,\n",
      "        2246,  113,  128,  119,  130,  114,  852,  100,  134,  113,  145,  116,\n",
      "         418, 2240,  114,  122,  145, 2246,  119,  113,  128,  119, 1275,  114,\n",
      "         851,  852, 1249,  418, 8015,  121,  117, 1103, 2366, 2200,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 0, the regularized solution w˜ approaches w . But what ∗ happens as α grows? Because H is real and\n",
      "content_token: tensor([ 101,  121,  117, 1103, 2366, 2200, 5072,  100, 8015,  192,  119, 1252,\n",
      "        1184,  852, 5940, 1112,  418, 7096,  136, 2279,  145, 1110, 1842, 1105,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: H is real and symmetric, we can decompose it into a diagonal matrix Λ and an orthonormal basis of\n",
      "content_token: tensor([  101,   145,  1110,  1842,  1105, 21852,   117,  1195,  1169,  1260,\n",
      "         8178, 14811,  1122,  1154,   170, 22698,  8952,   403,  1105,  1126,\n",
      "         1137,  1582, 23038, 25635,  3142,  1104,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: basis of eigenvectors, Q, such that H = QΛQ . Applying the decomposition to equation 7.10, we\n",
      "content_token: tensor([  101,  3142,  1104,   174, 13417,  1179,  2707, 22711,   117,   154,\n",
      "          117,  1216,  1115,   145,   134,   154, 28324,  4880,   119,   138,\n",
      "         8661, 15318,  1103, 25898,  1106,  8381,   128,   119,  1275,   117,\n",
      "         1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: equation 7.10, we obtain:  w˜ = (QΛQ +αI) 1QΛQ w (7.11)  −  ∗ 1 = Q(Λ+αI)Q − QΛQ w (7.12)   ∗\n",
      "content_token: tensor([  101,  8381,   128,   119,  1275,   117,  1195,  6268,   131,   100,\n",
      "          134,   113,   154, 28324,  4880,   116,   418,  2240,   114,   122,\n",
      "         4880, 28324,  4880,   192,   113,   128,   119,  1429,   114,   851,\n",
      "          852,   122,   134,   154,   113,   403,   116,   418,  2240,   114,\n",
      "          154,   851,   154, 28324,  4880,   192,   113,   128,   119,  1367,\n",
      "          114,   852,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: QΛQ w (7.12)   ∗  1  = Q(Λ+αI) ΛQ w . (7.13) −  ∗ We see that the effect of weight decay is to\n",
      "content_token: tensor([  101,   154, 28324,  4880,   192,   113,   128,   119,  1367,   114,\n",
      "          852,   122,   134,   154,   113,   403,   116,   418,  2240,   114,\n",
      "          403,  4880,   192,   119,   113,   128,   119,  1492,   114,   851,\n",
      "          852,  1284,  1267,  1115,  1103,  2629,  1104,  2841, 14352,  1110,\n",
      "         1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: weight decay is to rescale w along the axes defined by ∗ the eigenvectors of H. Specifically, the\n",
      "content_token: tensor([  101,  2841, 14352,  1110,  1106,  1231, 26996,  1513,   192,  1373,\n",
      "         1103, 21682,  3393,  1118,   852,  1103,   174, 13417,  1179,  2707,\n",
      "        22711,  1104,   145,   119, 21325,   117,  1103,   102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Specifically, the component of w that is aligned with the ∗ i-th eigenvector of H is rescaled by a\n",
      "content_token: tensor([  101, 21325,   117,  1103,  6552,  1104,   192,  1115,  1110, 14006,\n",
      "         1114,  1103,   852,   178,   118, 24438,   174, 13417,  1179,  2707,\n",
      "         9363,  1104,   145,  1110,  1231, 26996,  2433,  1118,   170,   102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: H is rescaled by a factor of λ i . (You may wish to review λ +α i how this kind of scaling works,\n",
      "content_token: tensor([  101,   145,  1110,  1231, 26996,  2433,  1118,   170,  5318,  1104,\n",
      "          428,   178,   119,   113,  1192,  1336,  3683,  1106,  3189,   428,\n",
      "          116,   418,   178,  1293,  1142,  1912,  1104,   188,  7867,  1158,\n",
      "         1759,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of scaling works, first explained in figure 2.3). Alongthe directions wherethe eigenvalues of H\n",
      "content_token: tensor([  101,  1104,   188,  7867,  1158,  1759,   117,  1148,  3716,  1107,\n",
      "         2482,   123,   119,   124,   114,   119,  6364, 10681,  7768,  1187,\n",
      "        10681,   174, 13417,  1179,  7501, 10589,  1104,   145,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: eigenvalues of H arerelativelylarge, for example, where λ α, the effect of regularization is\n",
      "content_token: tensor([  101,   174, 13417,  1179,  7501, 10589,  1104,   145,  1132,  9261,\n",
      "         5838,  1193,  5815,  2176,   117,  1111,  1859,   117,  1187,   428,\n",
      "          418,   117,  1103,  2629,  1104,  2366,  2734,  1110,   102])\n",
      "entity_list: ['regularization']\n",
      "entity_token: [tensor([2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regularization is relatively small. However, components i  with λ α will be shrunk to have nearly\n",
      "content_token: tensor([  101,  2366,  2734,  1110,  3860,  1353,   119,  1438,   117,  5644,\n",
      "          178,  1114,   428,   418,  1209,  1129,   188,  8167, 12660,  1106,\n",
      "         1138,  2212,   102])\n",
      "entity_list: ['regularization']\n",
      "entity_token: [tensor([2366, 2734])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to have nearly zero magnitude. This effect is illustrated i  in figure 7.1. 232 CHAPTER 7.\n",
      "content_token: tensor([  101,  1106,  1138,  2212,  6756, 10094,   119,  1188,  2629,  1110,\n",
      "         8292,   178,  1107,  2482,   128,   119,   122,   119, 22637,  8203,\n",
      "          128,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 7.1. 232 CHAPTER 7. REGULARIZATION FOR DEEP LEARNING w ∗ w˜ w 1 Figure 7.1: An illustration of the\n",
      "content_token: tensor([  101,   128,   119,   122,   119, 22637,  8203,   128,   119,   155,\n",
      "        17020,  2591, 10783, 20595,  5301, 13821, 24805,   143,  9565, 18581,\n",
      "        16668,   149, 12420,  2069, 27451, 11780,   192,   852,   100,   192,\n",
      "          122, 15982,   128,   119,   122,   131,  1760, 17011,  1104,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: illustration of the effect of L2 (or weight decay) regularization on the value of the optimal w.\n",
      "content_token: tensor([  101, 17011,  1104,  1103,  2629,  1104,   149,  1477,   113,  1137,\n",
      "         2841, 14352,   114,  2366,  2734,  1113,  1103,  2860,  1104,  1103,\n",
      "        17307,   192,   119,   102])\n",
      "entity_list: ['L2 regularization', 'weight decay']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734]), tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the optimal w. The solid ellipses represent contours of equal value of the unregularized\n",
      "content_token: tensor([  101,  1104,  1103, 17307,   192,   119,  1109,  4600,  8468, 10913,\n",
      "         8830,  4248, 14255, 18834,  1116,  1104,  4463,  2860,  1104,  1103,\n",
      "         8362,  1874, 13830,  5815,  2200,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the unregularized objective. The dotted circles represent contours of equal value of the L2\n",
      "content_token: tensor([  101,  1103,  8362,  1874, 13830,  5815,  2200,  7649,   119,  1109,\n",
      "        22641,  7839,  4248, 14255, 18834,  1116,  1104,  4463,  2860,  1104,\n",
      "         1103,   149,  1477,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: value of the L2 regularizer. At the point w˜, these competing objectives reach an equilibrium. In\n",
      "content_token: tensor([  101,  2860,  1104,  1103,   149,  1477,  2366, 17260,   119,  1335,\n",
      "         1103,  1553,   100,   117,  1292,  6259, 11350,  2519,  1126, 15784,\n",
      "          119,  1130,   102])\n",
      "entity_list: ['L2 regularizer']\n",
      "entity_token: [tensor([  149,  1477,  2366, 17260])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: an equilibrium. In the first dimension, the eigenvalue of the Hessian of J is small. The objective\n",
      "content_token: tensor([  101,  1126, 15784,   119,  1130,  1103,  1148, 11025,   117,  1103,\n",
      "          174, 13417,  1179,  7501,  4175,  1104,  1103, 26349,  1811,  1104,\n",
      "          147,  1110,  1353,   119,  1109,  7649,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The objective function does not increase much when moving horizontally away from w . Because the\n",
      "content_token: tensor([  101,  1109,  7649,  3053,  1674,  1136,  2773,  1277,  1165,  2232,\n",
      "        26126,  1283,  1121,   192,   119,  2279,  1103,   102])\n",
      "entity_list: ['objective function']\n",
      "entity_token: [tensor([7649, 3053])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: w . Because the objective function does not express ∗ a strong preference along this direction, the\n",
      "content_token: tensor([  101,   192,   119,  2279,  1103,  7649,  3053,  1674,  1136,  6848,\n",
      "          852,   170,  2012, 12629,  1373,  1142,  2447,   117,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: this direction, the regularizer has a strong effect on this axis. The regularizer pulls w close to\n",
      "content_token: tensor([  101,  1142,  2447,   117,  1103,  2366, 17260,  1144,   170,  2012,\n",
      "         2629,  1113,  1142,  9840,   119,  1109,  2366, 17260,  7561,   192,\n",
      "         1601,  1106,   102])\n",
      "entity_list: ['regularizer\\n\\nregularizer 是一个深度学习领域的实体']\n",
      "entity_token: [tensor([ 2366, 17260,  2366, 17260,   100,   976,   100,   100,   100,   100,\n",
      "          100,   100,   100,   100,   100,   100])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: pulls w close to zero. In the second dimension, the objective function 1 is very sensitive to\n",
      "content_token: tensor([  101,  7561,   192,  1601,  1106,  6756,   119,  1130,  1103,  1248,\n",
      "        11025,   117,  1103,  7649,  3053,   122,  1110,  1304,  7246,  1106,\n",
      "          102])\n",
      "entity_list: ['objective function\\n\\nobjective function 是一个深度学习领域的实体']\n",
      "entity_token: [tensor([7649, 3053, 7649, 3053,  100,  976,  100,  100,  100,  100,  100,  100,\n",
      "         100,  100,  100,  100])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: very sensitive to movements away from w . The corresponding eigenvalue is large, ∗ indicating high\n",
      "content_token: tensor([  101,  1304,  7246,  1106,  5172,  1283,  1121,   192,   119,  1109,\n",
      "         7671,   174, 13417,  1179,  7501,  4175,  1110,  1415,   117,   852,\n",
      "         7713,  1344,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∗ indicating high curvature. As a result, weight decay affects the position of w relatively 2\n",
      "content_token: tensor([  101,   852,  7713,  1344, 16408, 13461,  5332,   119,  1249,   170,\n",
      "         1871,   117,  2841, 14352, 13974,  1103,  1700,  1104,   192,  3860,\n",
      "          123,   102])\n",
      "entity_list: ['weight decay\\n\\nweight decay 是一个深度学习领域的实体']\n",
      "entity_token: [tensor([ 2841, 14352,  2841, 14352,   100,   976,   100,   100,   100,   100,\n",
      "          100,   100,   100,   100,   100,   100])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of w relatively 2 little. Only directions along which the parameters contribute significantly to\n",
      "content_token: tensor([  101,  1104,   192,  3860,   123,  1376,   119,  2809,  7768,  1373,\n",
      "         1134,  1103, 11934,  8681,  5409,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: significantly to reducing the objective function are preserved relatively intact. In directions\n",
      "content_token: tensor([ 101, 5409, 1106, 7914, 1103, 7649, 3053, 1132, 6018, 3860, 9964,  119,\n",
      "        1130, 7768,  102])\n",
      "entity_list: ['objective function']\n",
      "entity_token: [tensor([7649, 3053])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In directions that do not contribute to reducing the objective function, a small eigenvalue of the\n",
      "content_token: tensor([  101,  1130,  7768,  1115,  1202,  1136,  8681,  1106,  7914,  1103,\n",
      "         7649,  3053,   117,   170,  1353,   174, 13417,  1179,  7501,  4175,\n",
      "         1104,  1103,   102])\n",
      "entity_list: ['objective function']\n",
      "entity_token: [tensor([7649, 3053])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: eigenvalue of the Hessian tells us that movement in this direction will not significantly increase\n",
      "content_token: tensor([  101,   174, 13417,  1179,  7501,  4175,  1104,  1103, 26349,  1811,\n",
      "         3301,  1366,  1115,  2230,  1107,  1142,  2447,  1209,  1136,  5409,\n",
      "         2773,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: increase the gradient. Components of the weight vector corresponding to such unimportant directions\n",
      "content_token: tensor([  101,  2773,  1103, 19848,   119,  3291, 24729, 21222,  1116,  1104,\n",
      "         1103,  2841,  9479,  7671,  1106,  1216,  8362,  4060,  4342,  2861,\n",
      "         7768,   102])\n",
      "entity_list: ['weight vector']\n",
      "entity_token: [tensor([2841, 9479])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: directions are decayed away through the use of the regularization throughout training. So far we\n",
      "content_token: tensor([  101,  7768,  1132, 14352,  1174,  1283,  1194,  1103,  1329,  1104,\n",
      "         1103,  2366,  2734,  2032,  2013,   119,  1573,  1677,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: training. So far we have discussed weight decay in terms of its effect on the optimization of an\n",
      "content_token: tensor([  101,  2013,   119,  1573,  1677,  1195,  1138,  6352,  2841, 14352,\n",
      "         1107,  2538,  1104,  1157,  2629,  1113,  1103, 25161,  1104,  1126,\n",
      "          102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: optimization of an abstract, general, quadratic cost function. How do these effects relate to\n",
      "content_token: tensor([  101, 25161,  1104,  1126, 11108,   117,  1704,   117,   186, 18413,\n",
      "        21961,  2616,  3053,   119,  1731,  1202,  1292,  3154, 15123,  1106,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: effects relate to machine learning in particular? We can find out by studying linear regression, a\n",
      "content_token: tensor([  101,  3154, 15123,  1106,  3395,  3776,  1107,  2440,   136,  1284,\n",
      "         1169,  1525,  1149,  1118,  5076,  7378,  1231, 24032,   117,   170,\n",
      "          102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regression, a model for which the true cost function is quadratic and therefore amenable to the\n",
      "content_token: tensor([  101,  1231, 24032,   117,   170,  2235,  1111,  1134,  1103,  2276,\n",
      "         2616,  3053,  1110,   186, 18413, 21961,  1105,  3335,  1821,  7076,\n",
      "         2165,  1106,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: amenable to the same kind of analysis we have used so far. Applying the analysis again, we will be\n",
      "content_token: tensor([  101,  1821,  7076,  2165,  1106,  1103,  1269,  1912,  1104,  3622,\n",
      "         1195,  1138,  1215,  1177,  1677,   119,   138,  8661, 15318,  1103,\n",
      "         3622,  1254,   117,  1195,  1209,  1129,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: again, we will be able to obtain a special case of the same results, but with the solution now\n",
      "content_token: tensor([ 101, 1254,  117, 1195, 1209, 1129, 1682, 1106, 6268,  170, 1957, 1692,\n",
      "        1104, 1103, 1269, 2686,  117, 1133, 1114, 1103, 5072, 1208,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the solution now phrased in terms of the training data. For linear regression, the cost function is\n",
      "content_token: tensor([  101,  1103,  5072,  1208,  7224,  1181,  1107,  2538,  1104,  1103,\n",
      "         2013,  2233,   119,  1370,  7378,  1231, 24032,   117,  1103,  2616,\n",
      "         3053,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: cost function is 233 2w CHAPTER 7. REGULARIZATION FOR DEEP LEARNING the sum of squared errors: (Xw\n",
      "content_token: tensor([  101,  2616,  3053,  1110, 24482,   123,  2246,  8203,   128,   119,\n",
      "          155, 17020,  2591, 10783, 20595,  5301, 13821, 24805,   143,  9565,\n",
      "        18581, 16668,   149, 12420,  2069, 27451, 11780,  1103,  7584,  1104,\n",
      "        23215, 11122,   131,   113,   161,  2246,   102])\n",
      "entity_list: ['REGULARIZATION FOR DEEP LEARNING']\n",
      "entity_token: [tensor([  155, 17020,  2591, 10783, 20595,  5301, 13821, 24805,   143,  9565,\n",
      "        18581, 16668,   149, 12420,  2069, 27451, 11780])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: squared errors: (Xw y) (Xw y). (7.14)  − − When we add L2 regularization, the objective function\n",
      "content_token: tensor([  101, 23215, 11122,   131,   113,   161,  2246,   194,   114,   113,\n",
      "          161,  2246,   194,   114,   119,   113,   128,   119,  1489,   114,\n",
      "          851,   851,  1332,  1195,  5194,   149,  1477,  2366,  2734,   117,\n",
      "         1103,  7649,  3053,   102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 2, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: objective function changes to 1 (Xw y) (Xw y)+ αw w. (7.15)   − − 2 This changes the normal\n",
      "content_token: tensor([ 101, 7649, 3053, 2607, 1106,  122,  113,  161, 2246,  194,  114,  113,\n",
      "         161, 2246,  194,  114,  116,  418, 2246,  192,  119,  113,  128,  119,\n",
      "        1405,  114,  851,  851,  123, 1188, 2607, 1103, 2999,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: changes the normal equations for the solution from w = (X X) 1X y (7.16)  −  to w = (X X +αI) 1X\n",
      "content_token: tensor([  101,  2607,  1103,  2999, 11838,  1111,  1103,  5072,  1121,   192,\n",
      "          134,   113,   161,   161,   114,   122,  3190,   194,   113,   128,\n",
      "          119,  1479,   114,   851,  1106,   192,   134,   113,   161,   161,\n",
      "          116,   418,  2240,   114,   122,  3190,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to w = (X X +αI) 1X y. (7.17)  −  Thematrix X X inequation7.16is proportionaltothecovariance\n",
      "content_token: tensor([  101,  1106,   192,   134,   113,   161,   161,   116,   418,  2240,\n",
      "          114,   122,  3190,   194,   119,   113,   128,   119,  1542,   114,\n",
      "          851, 23420,  2980, 14799,   161,   161,  1107,  1162, 13284,  2116,\n",
      "         1559,   119,  1479,  1548, 15122,  2430, 10681,  2528,  8997, 19425,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: matrix 1 X X.  m  Using L2 regularization replaces this matrix with X X +αI −1 in equation 7.17.\n",
      "content_token: tensor([  101,  8952,   122,   161,   161,   119,   182,  7993,   149,  1477,\n",
      "         2366,  2734, 22974,  1142,  8952,  1114,   161,   161,   116,   418,\n",
      "         2240,   851,  1475,  1107,  8381,   128,   119,  1542,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in equation 7.17. The new matrix is the same as the original one, but with the addition of α to the\n",
      "content_token: tensor([ 101, 1107, 8381,  128,  119, 1542,  119, 1109, 1207, 8952, 1110, 1103,\n",
      "        1269, 1112, 1103, 1560, 1141,  117, 1133, 1114, 1103, 1901, 1104,  418,\n",
      "        1106, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of α to the   diagonal. The diagonal entries of this matrix correspond to the variance of each\n",
      "content_token: tensor([  101,  1104,   418,  1106,  1103, 22698,   119,  1109, 22698, 10813,\n",
      "         1104,  1142,  8952, 18420,  1106,  1103, 26717,  1104,  1296,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: variance of each input feature. We can see that L2 regularization causes the learning algorithm to\n",
      "content_token: tensor([  101, 26717,  1104,  1296,  7758,  2672,   119,  1284,  1169,  1267,\n",
      "         1115,   149,  1477,  2366,  2734,  4680,  1103,  3776,  9932,  1106,\n",
      "          102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithm to “perceive” the input X as having higher variance, which makes it shrink the weights on\n",
      "content_token: tensor([  101,  9932,  1106,   789, 25784,   790,  1103,  7758,   161,  1112,\n",
      "         1515,  2299, 26717,   117,  1134,  2228,  1122, 26406,  1103, 17981,\n",
      "         1113,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the weights on features whose covariance with the output target is low compared to this added\n",
      "content_token: tensor([  101,  1103, 17981,  1113,  1956,  2133,  1884,  8997, 19425,  1114,\n",
      "         1103,  5964,  4010,  1110,  1822,  3402,  1106,  1142,  1896,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to this added variance. 7.1.2 L1 Regularization While L2 weight decay is the most common form of\n",
      "content_token: tensor([  101,  1106,  1142,  1896, 26717,   119,   128,   119,   122,   119,\n",
      "          123,   149,  1475, 14381,  2734,  1799,   149,  1477,  2841, 14352,\n",
      "         1110,  1103,  1211,  1887,  1532,  1104,   102])\n",
      "entity_list: ['L1 Regularization', 'L2 weight decay']\n",
      "entity_token: [tensor([  149,  1475, 14381,  2734]), tensor([  149,  1477,  2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: most common form of weight decay, there are other ways to penalize the size of the model\n",
      "content_token: tensor([  101,  1211,  1887,  1532,  1104,  2841, 14352,   117,  1175,  1132,\n",
      "         1168,  3242,  1106,  8228, 10584,  3171,  1103,  2060,  1104,  1103,\n",
      "         2235,   102])\n",
      "entity_list: ['weight decay', 'model']\n",
      "entity_token: [tensor([ 2841, 14352]), tensor([2235])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: size of the model parameters. Another option is to use L1 regularization. Formally, L1\n",
      "content_token: tensor([  101,  2060,  1104,  1103,  2235, 11934,   119,  2543,  5146,  1110,\n",
      "         1106,  1329,   149,  1475,  2366,  2734,   119, 15075,  2716,   117,\n",
      "          149,  1475,   102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Formally, L1 regularization on the model parameter w is defined as: Ω(θ) = w = w , (7.18) 1 i || ||\n",
      "content_token: tensor([  101, 15075,  2716,   117,   149,  1475,  2366,  2734,  1113,  1103,\n",
      "         2235, 17816,   192,  1110,  3393,  1112,   131,   413,   113,   425,\n",
      "          114,   134,   192,   134,   192,   117,   113,   128,   119,  1407,\n",
      "          114,   122,   178,   197,   197,   197,   197,   102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: , (7.18) 1 i || || | | i  that is, as the sum of absolute values of the individual parameters.2 We\n",
      "content_token: tensor([  101,   117,   113,   128,   119,  1407,   114,   122,   178,   197,\n",
      "          197,   197,   197,   197,   197,   178,  1115,  1110,   117,  1112,\n",
      "         1103,  7584,  1104,  7846,  4718,  1104,  1103,  2510, 11934,   119,\n",
      "          123,  1284,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parameters.2 We will now discuss the effect of L1 regularization on the simple linear regression\n",
      "content_token: tensor([  101, 11934,   119,   123,  1284,  1209,  1208,  6265,  1103,  2629,\n",
      "         1104,   149,  1475,  2366,  2734,  1113,  1103,  3014,  7378,  1231,\n",
      "        24032,   102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: linear regression model, with no bias parameter, that we studied in our analysis of L2\n",
      "content_token: tensor([  101,  7378,  1231, 24032,  2235,   117,  1114,  1185, 15069, 17816,\n",
      "          117,  1115,  1195,  2376,  1107,  1412,  3622,  1104,   149,  1477,\n",
      "          102])\n",
      "entity_list: ['linear regression model', 'bias parameter', 'L2']\n",
      "entity_token: [tensor([ 7378,  1231, 24032,  2235]), tensor([15069, 17816]), tensor([ 149, 1477])]\n",
      "label: tensor([0, 2, 1, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: our analysis of L2 regularization. In particular, we are interested in delineating the differences\n",
      "content_token: tensor([ 101, 1412, 3622, 1104,  149, 1477, 2366, 2734,  119, 1130, 2440,  117,\n",
      "        1195, 1132, 3888, 1107, 3687, 2042, 3798, 1103, 5408,  102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the differences between L1 and L2 forms 2As with L2 regularization, we could regularize the\n",
      "content_token: tensor([  101,  1103,  5408,  1206,   149,  1475,  1105,   149,  1477,  2769,\n",
      "          123, 23390,  1114,   149,  1477,  2366,  2734,   117,  1195,  1180,\n",
      "         2366,  3708,  1103,   102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regularize the parameters towards a value that is not zero, but instead towards some parameter\n",
      "content_token: tensor([  101,  2366,  3708,  1103, 11934,  2019,   170,  2860,  1115,  1110,\n",
      "         1136,  6756,   117,  1133,  1939,  2019,  1199, 17816,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: some parameter value w(o). In that case the L1 regularization would introduce the term Ω(θ) = w\n",
      "content_token: tensor([  101,  1199, 17816,  2860,   192,   113,   184,   114,   119,  1130,\n",
      "         1115,  1692,  1103,   149,  1475,  2366,  2734,  1156,  8698,  1103,\n",
      "         1858,   413,   113,   425,   114,   134,   192,   102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the term Ω(θ) = w w(o) =  w w(o) . || − ||1 i | i − i | 234 CHAPTER 7. REGULARIZATION FOR DEEP\n",
      "content_token: tensor([  101,  1103,  1858,   413,   113,   425,   114,   134,   192,   192,\n",
      "          113,   184,   114,   134,   192,   192,   113,   184,   114,   119,\n",
      "          197,   197,   851,   197,   197,   122,   178,   197,   178,   851,\n",
      "          178,   197, 24354,  8203,   128,   119,   155, 17020,  2591, 10783,\n",
      "        20595,  5301, 13821, 24805,   143,  9565, 18581, 16668,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: FOR DEEP LEARNING of regularization. As with L2 weight decay, L1 weight decay controls the strength\n",
      "content_token: tensor([  101,   143,  9565, 18581, 16668,   149, 12420,  2069, 27451, 11780,\n",
      "         1104,  2366,  2734,   119,  1249,  1114,   149,  1477,  2841, 14352,\n",
      "          117,   149,  1475,  2841, 14352,  7451,  1103,  3220,   102])\n",
      "entity_list: ['L2 weight decay']\n",
      "entity_token: [tensor([  149,  1477,  2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the strength of the regularization by scaling the penalty Ω using a positive hyperparameter α. ˜\n",
      "content_token: tensor([  101,  1103,  3220,  1104,  1103,  2366,  2734,  1118,   188,  7867,\n",
      "         1158,  1103,  6180,   413,  1606,   170,  3112,   177, 24312, 17482,\n",
      "        16470,  2083,   418,   119,   100,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: hyperparameter α. ˜ Thus, the regularized objective function J(w;X,y) is given by J˜(w;X,y) = α w\n",
      "content_token: tensor([  101,   177, 24312, 17482, 16470,  2083,   418,   119,   100,  4516,\n",
      "          117,  1103,  2366,  2200,  7649,  3053,   147,   113,   192,   132,\n",
      "          161,   117,   194,   114,  1110,  1549,  1118,   100,   113,   192,\n",
      "          132,   161,   117,   194,   114,   134,   418,   192,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by J˜(w;X,y) = α w +J(w;X,y), (7.19) 1 || || with the corresponding gradient (actually,\n",
      "content_token: tensor([  101,  1118,   100,   113,   192,   132,   161,   117,   194,   114,\n",
      "          134,   418,   192,   116,   147,   113,   192,   132,   161,   117,\n",
      "          194,   114,   117,   113,   128,   119,  1627,   114,   122,   197,\n",
      "          197,   197,   197,  1114,  1103,  7671, 19848,   113,  2140,   117,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: gradient (actually, sub-gradient): ˜ J(w;X,y) = αsign(w)+ J(X,y;w) (7.20) w w ∇ ∇ where sign(w) is\n",
      "content_token: tensor([  101, 19848,   113,  2140,   117,  4841,   118, 19848,   114,   131,\n",
      "          100,   147,   113,   192,   132,   161,   117,   194,   114,   134,\n",
      "          418, 19638,   113,   192,   114,   116,   147,   113,   161,   117,\n",
      "          194,   132,   192,   114,   113,   128,   119,  1406,   114,   192,\n",
      "          192,   100,   100,  1187,  2951,   113,   192,   114,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∇ where sign(w) is simply the sign of w applied element-wise. By inspecting equation 7.20, we can\n",
      "content_token: tensor([  101,   100,  1187,  2951,   113,   192,   114,  1110,  2566,  1103,\n",
      "         2951,  1104,   192,  3666,  5290,   118, 10228,   119,  1650, 25151,\n",
      "         1158,  8381,   128,   119,  1406,   117,  1195,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 7.20, we can see immediately that the effect of L1 regularization is quite different from that of\n",
      "content_token: tensor([ 101,  128,  119, 1406,  117, 1195, 1169, 1267, 2411, 1115, 1103, 2629,\n",
      "        1104,  149, 1475, 2366, 2734, 1110, 2385, 1472, 1121, 1115, 1104,  102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from that of L2 regularization. Specifically, we can see that the regularization contribution to\n",
      "content_token: tensor([  101,  1121,  1115,  1104,   149,  1477,  2366,  2734,   119, 21325,\n",
      "          117,  1195,  1169,  1267,  1115,  1103,  2366,  2734,  6436,  1106,\n",
      "          102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: contribution to the gradient no longer scales linearly with each w; instead it is a constant factor\n",
      "content_token: tensor([  101,  6436,  1106,  1103, 19848,  1185,  2039,  9777,  7378,  1193,\n",
      "         1114,  1296,   192,   132,  1939,  1122,  1110,   170,  4836,  5318,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a constant factor with a sign equal to sign(w ). One i i consequence of this form of the gradient\n",
      "content_token: tensor([  101,   170,  4836,  5318,  1114,   170,  2951,  4463,  1106,  2951,\n",
      "          113,   192,   114,   119,  1448,   178,   178,  9547,  1104,  1142,\n",
      "         1532,  1104,  1103, 19848,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the gradient is that we will not necessarily see clean algebraic solutions to quadratic\n",
      "content_token: tensor([  101,  1104,  1103, 19848,  1110,  1115,  1195,  1209,  1136,  9073,\n",
      "         1267,  4044, 19669,  7995,  1106,   186, 18413, 21961,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to quadratic approximations of J(X,y;w) as we did for L2 regularization. Our simple linear model\n",
      "content_token: tensor([  101,  1106,   186, 18413, 21961, 22519,  1116,  1104,   147,   113,\n",
      "          161,   117,   194,   132,   192,   114,  1112,  1195,  1225,  1111,\n",
      "          149,  1477,  2366,  2734,   119,  3458,  3014,  7378,  2235,   102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: simple linear model has a quadratic cost function that we can represent via its Taylor series.\n",
      "content_token: tensor([  101,  3014,  7378,  2235,  1144,   170,   186, 18413, 21961,  2616,\n",
      "         3053,  1115,  1195,  1169,  4248,  2258,  1157,  3357,  1326,   119,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: its Taylor series. Alternately, we could imagine that this is a truncated Taylor series\n",
      "content_token: tensor([  101,  1157,  3357,  1326,   119, 14234,  1193,   117,  1195,  1180,\n",
      "         5403,  1115,  1142,  1110,   170,   189, 28098,  3357,  1326,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Taylor series approximating the cost function of a more sophisticated model. The gradient in this\n",
      "content_token: tensor([  101,  3357,  1326, 26403, 27182,  1103,  2616,  3053,  1104,   170,\n",
      "         1167, 12580,  2235,   119,  1109, 19848,  1107,  1142,   102])\n",
      "entity_list: ['output: cost function', 'gradient']\n",
      "entity_token: [tensor([5964,  131, 2616, 3053]), tensor([19848])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: gradient in this setting is given by Jˆ(w) = H(w w ), (7.21) w ∗ ∇ − where, again, H is the Hessian\n",
      "content_token: tensor([  101, 19848,  1107,  1142,  3545,  1110,  1549,  1118,   100,   113,\n",
      "          192,   114,   134,   145,   113,   192,   192,   114,   117,   113,\n",
      "          128,   119,  1626,   114,   192,   852,   100,   851,  1187,   117,\n",
      "         1254,   117,   145,  1110,  1103, 26349,  1811,   102])\n",
      "entity_list: ['output: gradient', 'Hessian']\n",
      "entity_token: [tensor([ 5964,   131, 19848]), tensor([26349,  1811])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: H is the Hessian matrix of J with respect to w evaluated at w . ∗ Because the L1 penalty does not\n",
      "content_token: tensor([  101,   145,  1110,  1103, 26349,  1811,  8952,  1104,   147,  1114,\n",
      "         4161,  1106,   192, 17428,  1120,   192,   119,   852,  2279,  1103,\n",
      "          149,  1475,  6180,  1674,  1136,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: L1 penalty does not admit clean algebraic expressions in the case of a fully general Hessian, we\n",
      "content_token: tensor([  101,   149,  1475,  6180,  1674,  1136,  5890,  4044, 19669, 11792,\n",
      "         1107,  1103,  1692,  1104,   170,  3106,  1704, 26349,  1811,   117,\n",
      "         1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: general Hessian, we will also make the further simplifying assumption that the Hessian is diagonal,\n",
      "content_token: tensor([  101,  1704, 26349,  1811,   117,  1195,  1209,  1145,  1294,  1103,\n",
      "         1748, 27466,  8223, 22881,  1158, 13457,  1115,  1103, 26349,  1811,\n",
      "         1110, 22698,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is diagonal, H = diag([H ,...,H ]), where each H > 0. 1,1 n,n i,i This assumption holds if the data\n",
      "content_token: tensor([  101,  1110, 22698,   117,   145,   134,  4267,  8517,   113,   164,\n",
      "          145,   117,   119,   119,   119,   117,   145,   166,   114,   117,\n",
      "         1187,  1296,   145,   135,   121,   119,   122,   117,   122,   183,\n",
      "          117,   183,   178,   117,   178,  1188, 13457,  3486,  1191,  1103,\n",
      "         2233,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: holds if the data for the linear regression problem has been preprocessed to remove all correlation\n",
      "content_token: tensor([  101,  3486,  1191,  1103,  2233,  1111,  1103,  7378,  1231, 24032,\n",
      "         2463,  1144,  1151,  3073,  1643,  2180, 22371,  1174,  1106,  5782,\n",
      "         1155, 18741,   102])\n",
      "entity_list: ['linear regression problem']\n",
      "entity_token: [tensor([ 7378,  1231, 24032,  2463])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: all correlation between the input features, which may be accomplished using PCA. Our quadratic\n",
      "content_token: tensor([  101,  1155, 18741,  1206,  1103,  7758,  1956,   117,  1134,  1336,\n",
      "         1129,  8587,  1606,  7054,  1592,   119,  3458,   186, 18413, 21961,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: PCA. Our quadratic approximation of the L1 regularized objective function decom- poses into a sum\n",
      "content_token: tensor([  101,  7054,  1592,   119,  3458,   186, 18413, 21961, 22519,  1104,\n",
      "         1103,   149,  1475,  2366,  2200,  7649,  3053,  1260,  8178,   118,\n",
      "        25366,  1154,   170,  7584,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: poses into a sum over the parameters: 1 Jˆ (w;X,y) = J(w ;X,y)+ H (w w )2 +α w . (7.22) ∗ 2 i,i i −\n",
      "content_token: tensor([  101, 25366,  1154,   170,  7584,  1166,  1103, 11934,   131,   122,\n",
      "          100,   113,   192,   132,   161,   117,   194,   114,   134,   147,\n",
      "          113,   192,   132,   161,   117,   194,   114,   116,   145,   113,\n",
      "          192,   192,   114,   123,   116,   418,   192,   119,   113,   128,\n",
      "          119,  1659,   114,   852,   123,   178,   117,   178,   178,   851,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (7.22) ∗ 2 i,i i − ∗i | i | i    Theproblem ofminimizing thisapproximatecostfunction hasan\n",
      "content_token: tensor([  101,   113,   128,   119,  1659,   114,   852,   123,   178,   117,\n",
      "          178,   178,   851,   852,  1182,   197,   178,   197,   178,  1109,\n",
      "         1643,  2180,  2165,  1306,  1104, 25685, 25596,  1142, 11478,  1643,\n",
      "         2180,  8745,  7213, 13538,  1204, 26420,  1144,  1389,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: hasan analyticalsolution (for each dimension i), with the following form: α w = sign(w )max w ,0 .\n",
      "content_token: tensor([  101,  1144,  1389, 22828, 24313, 12964,   113,  1111,  1296, 11025,\n",
      "          178,   114,   117,  1114,  1103,  1378,  1532,   131,   418,   192,\n",
      "          134,  2951,   113,   192,   114, 12477,  1775,   192,   117,   121,\n",
      "          119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sign(w )max w ,0 . (7.23) i i∗ ∗i | |− H i,i   235 CHAPTER 7. REGULARIZATION FOR DEEP LEARNING\n",
      "content_token: tensor([  101,  2951,   113,   192,   114, 12477,  1775,   192,   117,   121,\n",
      "          119,   113,   128,   119,  1695,   114,   178,   178, 28746,   852,\n",
      "         1182,   197,   197,   851,   145,   178,   117,   178, 19152,  8203,\n",
      "          128,   119,   155, 17020,  2591, 10783, 20595,  5301, 13821, 24805,\n",
      "          143,  9565, 18581, 16668,   149, 12420,  2069, 27451, 11780,   102])\n",
      "entity_list: ['Regularization for deep learning']\n",
      "entity_token: [tensor([14381,  2734,  1111,  1996,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: FOR DEEP LEARNING Consider the situation where w > 0 for all i. There are two possible outcomes: ∗i\n",
      "content_token: tensor([  101,   143,  9565, 18581, 16668,   149, 12420,  2069, 27451, 11780,\n",
      "        25515,  1103,  2820,  1187,   192,   135,   121,  1111,  1155,   178,\n",
      "          119,  1247,  1132,  1160,  1936, 13950,   131,   852,  1182,   102])\n",
      "entity_list: ['DEEP LEARNING']\n",
      "entity_token: [tensor([18581, 16668,   149, 12420,  2069, 27451, 11780])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: outcomes: ∗i 1. The case wherew α . Here the optimal value of w under the regularized ∗i ≤ H i,i i\n",
      "content_token: tensor([  101, 13950,   131,   852,  1182,   122,   119,  1109,  1692,  1187,\n",
      "         2246,   418,   119,  3446,  1103, 17307,  2860,  1104,   192,  1223,\n",
      "         1103,  2366,  2200,   852,  1182,   863,   145,   178,   117,   178,\n",
      "          178,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∗i ≤ H i,i i objectiveissimplyw = 0. Thisoccursbecausethecontributionof J(w;X,y) i ˜ to the\n",
      "content_token: tensor([  101,   852,  1182,   863,   145,   178,   117,   178,   178,  7649,\n",
      "        14788,  4060,  1643,  1193,  2246,   134,   121,   119,  1188, 13335,\n",
      "        10182,  1733,  3962,  2599,  5613, 10681,  7235, 14970, 10008,   147,\n",
      "          113,   192,   132,   161,   117,   194,   114,   178,   100,  1106,\n",
      "         1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: J(w;X,y) i ˜ to the regularized objective J(w;X,y) is overwhelmed—in direction i—by the L1\n",
      "content_token: tensor([  101,   147,   113,   192,   132,   161,   117,   194,   114,   178,\n",
      "          100,  1106,  1103,  2366,  2200,  7649,   147,   113,   192,   132,\n",
      "          161,   117,   194,   114,  1110, 14127,   783,  1107,  2447,   178,\n",
      "          783,  1118,  1103,   149,  1475,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: i—by the L1 regularization which pushes the value of w to zero. i 2. The case wherew > α . In this\n",
      "content_token: tensor([  101,   178,   783,  1118,  1103,   149,  1475,  2366,  2734,  1134,\n",
      "        14505,  1103,  2860,  1104,   192,  1106,  6756,   119,   178,   123,\n",
      "          119,  1109,  1692,  1187,  2246,   135,   418,   119,  1130,  1142,\n",
      "          102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: > α . In this case, the regularization does not move the ∗i H i,i optimal value of w to zero but\n",
      "content_token: tensor([  101,   135,   418,   119,  1130,  1142,  1692,   117,  1103,  2366,\n",
      "         2734,  1674,  1136,  1815,  1103,   852,  1182,   145,   178,   117,\n",
      "          178, 17307,  2860,  1104,   192,  1106,  6756,  1133,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of w to zero but instead it just shifts it in that direction by a i distance equal to α . H i,i A\n",
      "content_token: tensor([  101,  1104,   192,  1106,  6756,  1133,  1939,  1122,  1198, 12644,\n",
      "         1122,  1107,  1115,  2447,  1118,   170,   178,  2462,  4463,  1106,\n",
      "          418,   119,   145,   178,   117,   178,   138,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to α . H i,i A similar process happens when w < 0, but with the L1 penalty making w less i∗ i\n",
      "content_token: tensor([  101,  1106,   418,   119,   145,   178,   117,   178,   138,  1861,\n",
      "         1965,  5940,  1165,   192,   133,   121,   117,  1133,  1114,  1103,\n",
      "          149,  1475,  6180,  1543,   192,  1750,   178, 28746,   178,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: making w less i∗ i negative by α , or 0. H i,i In comparison to L2 regularization, L1\n",
      "content_token: tensor([  101,  1543,   192,  1750,   178, 28746,   178,  4366,  1118,   418,\n",
      "          117,  1137,   121,   119,   145,   178,   117,   178,  1130,  7577,\n",
      "         1106,   149,  1477,  2366,  2734,   117,   149,  1475,   102])\n",
      "entity_list: ['L2 regularization', 'L1 regularization\\n\\nExplanation: The entities \"L2 regularization\" and \"L1 regularization\" are mentioned in the input sentence and both belong to the field of deep learning.']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734]), tensor([  149,  1475,  2366,  2734, 16409,  1643, 20592,  2116,   131,  1109,\n",
      "        11659,   107,   149,  1477,  2366,  2734,   107,  1105,   107,   149,\n",
      "         1475,  2366,  2734,   107,  1132,  3025,  1107,  1103,  7758,  5650,\n",
      "         1105,  1241,  6772,  1106,  1103,  1768,  1104,  1996,  3776,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1,\n",
      "        1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: regularization, L1 regularization results in a solution that is more sparse. Sparsity in this\n",
      "content_token: tensor([  101,  2366,  2734,   117,   149,  1475,  2366,  2734,  2686,  1107,\n",
      "          170,  5072,  1115,  1110,  1167, 22726,   119, 23665,  1733,  1785,\n",
      "         1107,  1142,   102])\n",
      "entity_list: ['L1 regularization\\n\\nExplanation: The entity \"L1 regularization\" is mentioned in the input sentence and belongs to the field of deep learning.']\n",
      "entity_token: [tensor([  149,  1475,  2366,  2734, 16409,  1643, 20592,  2116,   131,  1109,\n",
      "         9127,   107,   149,  1475,  2366,  2734,   107,  1110,  3025,  1107,\n",
      "         1103,  7758,  5650,  1105,  7017,  1106,  1103,  1768,  1104,  1996,\n",
      "         3776,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Sparsity in this context refers to the fact that some parameters have an optimal value of zero. The\n",
      "content_token: tensor([  101, 23665,  1733,  1785,  1107,  1142,  5618,  4431,  1106,  1103,\n",
      "         1864,  1115,  1199, 11934,  1138,  1126, 17307,  2860,  1104,  6756,\n",
      "          119,  1109,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: value of zero. The sparsity of L1 regularization is a qualitatively different behavior than arises\n",
      "content_token: tensor([  101,  2860,  1104,  6756,   119,  1109, 22620,  1733,  1785,  1104,\n",
      "          149,  1475,  2366,  2734,  1110,   170,   186,  4746, 24936,  1193,\n",
      "         1472,  4658,  1190, 20251,   102])\n",
      "entity_list: ['L1 regularization\\n\\nExplanation: The entity \"L1 regularization\" is mentioned in the input sentence and belongs to the field of deep learning.']\n",
      "entity_token: [tensor([  149,  1475,  2366,  2734, 16409,  1643, 20592,  2116,   131,  1109,\n",
      "         9127,   107,   149,  1475,  2366,  2734,   107,  1110,  3025,  1107,\n",
      "         1103,  7758,  5650,  1105,  7017,  1106,  1103,  1768,  1104,  1996,\n",
      "         3776,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: than arises with L2 regularization. Equation 7.13 gave the solution w˜ for L2 regularization. If we\n",
      "content_token: tensor([  101,  1190, 20251,  1114,   149,  1477,  2366,  2734,   119,   142,\n",
      "        13284,  2116,   128,   119,  1492,  1522,  1103,  5072,   100,  1111,\n",
      "          149,  1477,  2366,  2734,   119,  1409,  1195,   102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: If we revisit that equation using the assumption of a diagonal and positive definite Hessian H that\n",
      "content_token: tensor([  101,  1409,  1195,  1231,  9356,  2875,  1115,  8381,  1606,  1103,\n",
      "        13457,  1104,   170, 22698,  1105,  3112, 16428, 26349,  1811,   145,\n",
      "         1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Hessian H that we introduced for our analysis of L1 regularization, we find that w˜ = H i,i w . If\n",
      "content_token: tensor([  101, 26349,  1811,   145,  1115,  1195,  2234,  1111,  1412,  3622,\n",
      "         1104,   149,  1475,  2366,  2734,   117,  1195,  1525,  1115,   100,\n",
      "          134,   145,   178,   117,   178,   192,   119,  1409,   102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: w˜ = H i,i w . If w was nonzero, then w˜ remains i H +α ∗i ∗i i i,i nonzero. This demonstrates that\n",
      "content_token: tensor([  101,   100,   134,   145,   178,   117,   178,   192,   119,  1409,\n",
      "          192,  1108,  1664,  6198,  1186,   117,  1173,   100,  2606,   178,\n",
      "          145,   116,   418,   852,  1182,   852,  1182,   178,   178,   117,\n",
      "          178,  1664,  6198,  1186,   119,  1188, 17798,  1115,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: demonstrates that L2 regularization does not cause the parameters to become sparse, while L1\n",
      "content_token: tensor([  101, 17798,  1115,   149,  1477,  2366,  2734,  1674,  1136,  2612,\n",
      "         1103, 11934,  1106,  1561, 22726,   117,  1229,   149,  1475,   102])\n",
      "entity_list: ['L2 regularization', 'L1 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734]), tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sparse, while L1 regularization may do so for large enough α. The sparsity property induced by L1\n",
      "content_token: tensor([  101, 22726,   117,  1229,   149,  1475,  2366,  2734,  1336,  1202,\n",
      "         1177,  1111,  1415,  1536,   418,   119,  1109, 22620,  1733,  1785,\n",
      "         2400, 10645,  1118,   149,  1475,   102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: induced by L1 regularization has been used extensively as a feature selectionmechanism. Feature\n",
      "content_token: tensor([  101, 10645,  1118,   149,  1475,  2366,  2734,  1144,  1151,  1215,\n",
      "         7620,  1112,   170,  2672,  4557,  3263, 18546,  1863,   119, 14533,\n",
      "          102])\n",
      "entity_list: ['L1 regularization']\n",
      "entity_token: [tensor([ 149, 1475, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Feature selection simplifies a machine learning problem by choosing which subset of the available\n",
      "content_token: tensor([  101, 14533,  4557, 27466,  8223,  2646, 16847,   170,  3395,  3776,\n",
      "         2463,  1118, 11027,  1134, 18005,  1104,  1103,  1907,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the available features should be used. In particular, the well known LASSO (Tibshirani, 1995)\n",
      "content_token: tensor([  101,  1104,  1103,  1907,  1956,  1431,  1129,  1215,   119,  1130,\n",
      "         2440,   117,  1103,  1218,  1227, 10722, 12480,  2346,   113,   157,\n",
      "        13292,  5933, 23851,   117,  1876,   114,   102])\n",
      "entity_list: ['LASSO']\n",
      "entity_token: [tensor([10722, 12480,  2346])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (Tibshirani, 1995) (least absolute shrinkage and selection operator) model integrates an L1 penalty\n",
      "content_token: tensor([  101,   113,   157, 13292,  5933, 23851,   117,  1876,   114,   113,\n",
      "         1655,  7846, 26406,  2553,  1105,  4557,  6650,   114,  2235, 18831,\n",
      "         1116,  1126,   149,  1475,  6180,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: an L1 penalty with a linear model and a least squares cost function. The L1 penalty causes a subset\n",
      "content_token: tensor([  101,  1126,   149,  1475,  6180,  1114,   170,  7378,  2235,  1105,\n",
      "          170,  1655, 16004,  2616,  3053,   119,  1109,   149,  1475,  6180,\n",
      "         4680,   170, 18005,   102])\n",
      "entity_list: ['L1 penalty', 'least squares cost function']\n",
      "entity_token: [tensor([ 149, 1475, 6180]), tensor([ 1655, 16004,  2616,  3053])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: causes a subset of the weights to become zero, suggesting that the corresponding features may\n",
      "content_token: tensor([  101,  4680,   170, 18005,  1104,  1103, 17981,  1106,  1561,  6756,\n",
      "          117,  8783,  1115,  1103,  7671,  1956,  1336,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: features may safely be discarded. In section 5.6.1, we saw that many regularization strategies can\n",
      "content_token: tensor([  101,  1956,  1336,  9510,  1129, 16041,   119,  1130,  2237,   126,\n",
      "          119,   127,   119,   122,   117,  1195,  1486,  1115,  1242,  2366,\n",
      "         2734, 10700,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: strategies can be interpreted as MAP Bayesian inference, and that in particular, L2 regularization\n",
      "content_token: tensor([  101, 10700,  1169,  1129,  9829,  1112,  9960,  2101,  2410, 18766,\n",
      "         1389,  1107, 16792,   117,  1105,  1115,  1107,  2440,   117,   149,\n",
      "         1477,  2366,  2734,   102])\n",
      "entity_list: ['L2 regularization']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: L2 regularization is equivalent to MAP Bayesian inference with a Gaussian prior on the weights. For\n",
      "content_token: tensor([  101,   149,  1477,  2366,  2734,  1110,  4976,  1106,  9960,  2101,\n",
      "         2410, 18766,  1389,  1107, 16792,  1114,   170,   144, 25134, 11890,\n",
      "         2988,  1113,  1103, 17981,   119,  1370,   102])\n",
      "entity_list: ['L2 regularization', 'Gaussian prior']\n",
      "entity_token: [tensor([ 149, 1477, 2366, 2734]), tensor([  144, 25134, 11890,  2988])]\n",
      "label: tensor([0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on the weights. For L1 regu- larization, the penalty αΩ(w) = α w used to regularize a cost function\n",
      "content_token: tensor([  101,  1113,  1103, 17981,   119,  1370,   149,  1475,  1231, 13830,\n",
      "          118,  2495, 28021,  1891,   117,  1103,  6180,   418, 28334,   113,\n",
      "          192,   114,   134,   418,   192,  1215,  1106,  2366,  3708,   170,\n",
      "         2616,  3053,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a cost function is i | i | equivalent to the log-prior term that is maximized by MAP Bayesian\n",
      "content_token: tensor([  101,   170,  2616,  3053,  1110,   178,   197,   178,   197,  4976,\n",
      "         1106,  1103,  9366,   118,  2988,  1858,  1115,  1110, 12477,  8745,\n",
      "        26740,  1118,  9960,  2101,  2410, 18766,  1389,   102])\n",
      "entity_list: ['cost function']\n",
      "entity_token: [tensor([2616, 3053])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by MAP Bayesian inference  when the prior is an isotropic Laplace distribution (equation 3.26)\n",
      "content_token: tensor([  101,  1118,  9960,  2101,  2410, 18766,  1389,  1107, 16792,  1165,\n",
      "         1103,  2988,  1110,  1126,  1110,  3329, 27098,  2001, 11256,  3735,\n",
      "          113,  8381,   124,   119,  1744,   114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (equation 3.26) over w Rn: ∈ 1 logp(w) = logLaplace(w ;0, ) = α w +nlogα nlog2. (7.24) i 1 α − ||\n",
      "content_token: tensor([  101,   113,  8381,   124,   119,  1744,   114,  1166,   192,   155,\n",
      "         1179,   131,   850,   122,  9366,  1643,   113,   192,   114,   134,\n",
      "         9366,  2162, 11478, 17510,   113,   192,   132,   121,   117,   114,\n",
      "          134,   418,   192,   116,   183, 13791, 15561,   183, 13791,  1477,\n",
      "          119,   113,   128,   119,  1572,   114,   178,   122,   418,   851,\n",
      "          197,   197,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (7.24) i 1 α − || || − i  236\n",
      "content_token: tensor([  101,   113,   128,   119,  1572,   114,   178,   122,   418,   851,\n",
      "          197,   197,   197,   197,   851,   178, 26361,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 7. REGULARIZATION FOR DEEP LEARNING From the point of view of learning via maximization with\n",
      "content_token: tensor([  101,  8203,   128,   119,   155, 17020,  2591, 10783, 20595,  5301,\n",
      "        13821, 24805,   143,  9565, 18581, 16668,   149, 12420,  2069, 27451,\n",
      "        11780,  1622,  1103,  1553,  1104,  2458,  1104,  3776,  2258, 12477,\n",
      "         8745,  3080,  8569,  1114,   102])\n",
      "entity_list: ['REGULARIZATION FOR DEEP LEARNING']\n",
      "entity_token: [tensor([  155, 17020,  2591, 10783, 20595,  5301, 13821, 24805,   143,  9565,\n",
      "        18581, 16668,   149, 12420,  2069, 27451, 11780])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: maximization with respect to w, we can ignore the logα log2 terms because they do not depend on w.\n",
      "content_token: tensor([  101, 12477,  8745,  3080,  8569,  1114,  4161,  1106,   192,   117,\n",
      "         1195,  1169,  8429,  1103,  9366, 15561,  9366,  1477,  2538,  1272,\n",
      "         1152,  1202,  1136, 12864,  1113,   192,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: do not depend on w. − 7.2 Norm Penalties as Constrained Optimization Consider the cost function\n",
      "content_token: tensor([  101,  1202,  1136, 12864,  1113,   192,   119,   851,   128,   119,\n",
      "          123, 16162,  1306, 23544,  1348,  4338,  1112, 16752, 16468,  9044,\n",
      "         9126,  3121,  3080,  8569, 25515,  1103,  2616,  3053,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the cost function regularized by a parameter norm penalty: ˜ J(θ;X,y) = J(θ;X,y)+αΩ(θ). (7.25)\n",
      "content_token: tensor([  101,  1103,  2616,  3053,  2366,  2200,  1118,   170, 17816, 18570,\n",
      "         6180,   131,   100,   147,   113,   425,   132,   161,   117,   194,\n",
      "          114,   134,   147,   113,   425,   132,   161,   117,   194,   114,\n",
      "          116,   418, 28334,   113,   425,   114,   119,   113,   128,   119,\n",
      "         1512,   114,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (7.25) Recall from section 4.4 that we can minimize a function subject to constraints by\n",
      "content_token: tensor([  101,   113,   128,   119,  1512,   114, 11336,  7867,  1233,  1121,\n",
      "         2237,   125,   119,   125,  1115,  1195,  1169, 20220,   170,  3053,\n",
      "         2548,  1106, 15651,  1118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to constraints by constructing a generalized Lagrange function, consisting of the original\n",
      "content_token: tensor([  101,  1106, 15651,  1118, 17217,   170, 22214,  2001, 14867, 12377,\n",
      "         3053,   117,  4721,  1104,  1103,  1560,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the original objective function plus a set of penalties. Each penalty is a product between a\n",
      "content_token: tensor([  101,  1104,  1103,  1560,  7649,  3053,  4882,   170,  1383,  1104,\n",
      "        13095,   119,  2994,  6180,  1110,   170,  3317,  1206,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a product between a coefficient, called a Karush–Kuhn–Tucker (KKT) multiplier, and a function\n",
      "content_token: tensor([  101,   170,  3317,  1206,   170, 21130,   117,  1270,   170, 14812,\n",
      "        15432,   782, 23209,  7272,   782,  9594,   113,   148,  2428,  1942,\n",
      "          114,  4321,  1643,  9888,   117,  1105,   170,  3053,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and a function representing whether the constraint is satisfied. If we wanted to constrain Ω(θ) to\n",
      "content_token: tensor([  101,  1105,   170,  3053,  4311,  2480,  1103, 14255, 16468, 10879,\n",
      "         1110,  8723,   119,  1409,  1195,  1458,  1106, 14255, 16468,  1394,\n",
      "          413,   113,   425,   114,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: constrain Ω(θ) to be less than some constant k, we could construct a generalized Lagrange function\n",
      "content_token: tensor([  101, 14255, 16468,  1394,   413,   113,   425,   114,  1106,  1129,\n",
      "         1750,  1190,  1199,  4836,   180,   117,  1195,  1180,  9417,   170,\n",
      "        22214,  2001, 14867, 12377,  3053,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Lagrange function (θ,α;X,y) = J(θ;X,y)+α(Ω(θ) k). (7.26) L − The solution to the constrained\n",
      "content_token: tensor([  101,  2001, 14867, 12377,  3053,   113,   425,   117,   418,   132,\n",
      "          161,   117,   194,   114,   134,   147,   113,   425,   132,   161,\n",
      "          117,   194,   114,   116,   418,   113,   413,   113,   425,   114,\n",
      "          180,   114,   119,   113,   128,   119,  1744,   114,   149,   851,\n",
      "         1109,  5072,  1106,  1103, 14255, 16468,  9044,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to the constrained problem is given by θ = argmin max (θ,α). (7.27) ∗ θ α,α 0L ≥ As described in\n",
      "content_token: tensor([  101,  1106,  1103, 14255, 16468,  9044,  2463,  1110,  1549,  1118,\n",
      "          425,   134,   170, 10805,  7937, 12477,  1775,   113,   425,   117,\n",
      "          418,   114,   119,   113,   128,   119,  1765,   114,   852,   425,\n",
      "          418,   117,   418,   121,  2162,   864,  1249,  1758,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ≥ As described in section 4.4, solving this problem requires modifying both θ and α. Section 4.5\n",
      "content_token: tensor([  101,   864,  1249,  1758,  1107,  2237,   125,   119,   125,   117,\n",
      "        15097,  1142,  2463,  5315, 22015,  1158,  1241,   425,  1105,   418,\n",
      "          119,  6177,   125,   119,   126,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and α. Section 4.5 provides a worked example of linear regression with an L2 constraint. Many\n",
      "content_token: tensor([  101,  1105,   418,   119,  6177,   125,   119,   126,  2790,   170,\n",
      "         1589,  1859,  1104,  7378,  1231, 24032,  1114,  1126,   149,  1477,\n",
      "        14255, 16468, 10879,   119,  2408,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: L2 constraint. Many different procedures are possible—some may use gradient descent, while others\n",
      "content_token: tensor([  101,   149,  1477, 14255, 16468, 10879,   119,  2408,  1472,  8826,\n",
      "         1132,  1936,   783,  1199,  1336,  1329, 19848,  6585,   117,  1229,\n",
      "         1639,   102])\n",
      "entity_list: ['gradient descent']\n",
      "entity_token: [tensor([19848,  6585])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: while others may use analytical solutions for where the gradient is zero—but in all procedures α\n",
      "content_token: tensor([  101,  1229,  1639,  1336,  1329, 22828,  7995,  1111,  1187,  1103,\n",
      "        19848,  1110,  6756,   783,  1133,  1107,  1155,  8826,   418,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in all procedures α must increase whenever Ω(θ) > k and decrease whenever Ω(θ)< k. All positive α\n",
      "content_token: tensor([ 101, 1107, 1155, 8826,  418, 1538, 2773, 7747,  413,  113,  425,  114,\n",
      "         135,  180, 1105, 9711, 7747,  413,  113,  425,  114,  133,  180,  119,\n",
      "        1398, 3112,  418,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: k. All positive α encourage Ω(θ) to shrink. The optimal value α will encourage Ω(θ) ∗ to shrink,\n",
      "content_token: tensor([  101,   180,   119,  1398,  3112,   418,  8343,   413,   113,   425,\n",
      "          114,  1106, 26406,   119,  1109, 17307,  2860,   418,  1209,  8343,\n",
      "          413,   113,   425,   114,   852,  1106, 26406,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Ω(θ) ∗ to shrink, but not so strongly to make Ω(θ) become less than k. To gain some insight into\n",
      "content_token: tensor([  101,   413,   113,   425,   114,   852,  1106, 26406,   117,  1133,\n",
      "         1136,  1177,  5473,  1106,  1294,   413,   113,   425,   114,  1561,\n",
      "         1750,  1190,   180,   119,  1706,  4361,  1199, 14222,  1154,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: some insight into the effect of the constraint, we can fix α and view ∗ the problem as just a\n",
      "content_token: tensor([  101,  1199, 14222,  1154,  1103,  2629,  1104,  1103, 14255, 16468,\n",
      "        10879,   117,  1195,  1169,  8239,   418,  1105,  2458,   852,  1103,\n",
      "         2463,  1112,  1198,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: problem as just a function of θ: θ = argmin (θ,α ) = argminJ(θ;X,y)+α Ω(θ). (7.28) ∗ ∗ ∗ L θ θ ˜\n",
      "content_token: tensor([  101,  2463,  1112,  1198,   170,  3053,  1104,   425,   131,   425,\n",
      "          134,   170, 10805,  7937,   113,   425,   117,   418,   114,   134,\n",
      "          170, 10805,  7937,  4538,   113,   425,   132,   161,   117,   194,\n",
      "          114,   116,   418,   413,   113,   425,   114,   119,   113,   128,\n",
      "          119,  1743,   114,   852,   852,   852,   149,   425,   425,   100,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∗ ∗ ∗ L θ θ ˜ This is exactly the same as the regularized training problem of minimizing J. We can\n",
      "content_token: tensor([  101,   852,   852,   852,   149,   425,   425,   100,  1188,  1110,\n",
      "         2839,  1103,  1269,  1112,  1103,  2366,  2200,  2013,  2463,  1104,\n",
      "         8715, 25596,   147,   119,  1284,  1169,   102])\n",
      "entity_list: ['regularized training problem']\n",
      "entity_token: [tensor([2366, 2200, 2013, 2463])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: J. We can thus think of a parameter norm penalty as imposing a constraint on the weights. If Ω is\n",
      "content_token: tensor([  101,   147,   119,  1284,  1169,  2456,  1341,  1104,   170, 17816,\n",
      "        18570,  6180,  1112, 17884,   170, 14255, 16468, 10879,  1113,  1103,\n",
      "        17981,   119,  1409,   413,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: weights. If Ω is the L2 norm, then the weights are constrained to lie in an L2 ball. If Ω is the L1\n",
      "content_token: tensor([  101, 17981,   119,  1409,   413,  1110,  1103,   149,  1477, 18570,\n",
      "          117,  1173,  1103, 17981,  1132, 14255, 16468,  9044,  1106,  4277,\n",
      "         1107,  1126,   149,  1477,  3240,   119,  1409,   413,  1110,  1103,\n",
      "          149,  1475,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: If Ω is the L1 norm, then the weights are constrained to lie in a region of 237 CHAPTER 7.\n",
      "content_token: tensor([  101,  1409,   413,  1110,  1103,   149,  1475, 18570,   117,  1173,\n",
      "         1103, 17981,  1132, 14255, 16468,  9044,  1106,  4277,  1107,   170,\n",
      "         1805,  1104, 25950,  8203,   128,   119,   102])\n",
      "entity_list: ['L1 norm']\n",
      "entity_token: [tensor([  149,  1475, 18570])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of 237 CHAPTER 7. REGULARIZATION FOR DEEP LEARNING limited L1 norm. Usually we do not know the size\n",
      "content_token: tensor([  101,  1104, 25950,  8203,   128,   119,   155, 17020,  2591, 10783,\n",
      "        20595,  5301, 13821, 24805,   143,  9565, 18581, 16668,   149, 12420,\n",
      "         2069, 27451, 11780,  2609,   149,  1475, 18570,   119, 12378,  1195,\n",
      "         1202,  1136,  1221,  1103,  2060,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: not know the size of the constraint region that we impose by using weight decay with coefficient α\n",
      "content_token: tensor([  101,  1136,  1221,  1103,  2060,  1104,  1103, 14255, 16468, 10879,\n",
      "         1805,  1115,  1195, 19103,  1118,  1606,  2841, 14352,  1114, 21130,\n",
      "          418,   102])\n",
      "entity_list: ['weight decay']\n",
      "entity_token: [tensor([ 2841, 14352])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with coefficient α because the value of α does not ∗ ∗ directly tell us the value of k. In\n",
      "content_token: tensor([  101,  1114, 21130,   418,  1272,  1103,  2860,  1104,   418,  1674,\n",
      "         1136,   852,   852,  2626,  1587,  1366,  1103,  2860,  1104,   180,\n",
      "          119,  1130,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the value of k. In principle, one can solve for k, but the relationship between k and α depends on\n",
      "content_token: tensor([ 101, 1103, 2860, 1104,  180,  119, 1130, 6708,  117, 1141, 1169, 9474,\n",
      "        1111,  180,  117, 1133, 1103, 2398, 1206,  180, 1105,  418, 9113, 1113,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: k and α depends on the form of J. While we do not know the exact size ∗ of the constraint region,\n",
      "content_token: tensor([  101,   180,  1105,   418,  9113,  1113,  1103,  1532,  1104,   147,\n",
      "          119,  1799,  1195,  1202,  1136,  1221,  1103,  6129,  2060,   852,\n",
      "         1104,  1103, 14255, 16468, 10879,  1805,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: constraint region, we can control it roughly by increasing or decreasing α in order to grow or\n",
      "content_token: tensor([  101, 14255, 16468, 10879,  1805,   117,  1195,  1169,  1654,  1122,\n",
      "         4986,  1118,  4138,  1137, 18326,   418,  1107,  1546,  1106,  4328,\n",
      "         1137,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in order to grow or shrink the constraint region. Larger α will result in a smaller constraint\n",
      "content_token: tensor([  101,  1107,  1546,  1106,  4328,  1137, 26406,  1103, 14255, 16468,\n",
      "        10879,  1805,   119, 10236,  1197,   418,  1209,  1871,  1107,   170,\n",
      "         2964, 14255, 16468, 10879,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: smaller constraint region. Smaller α will result in a larger constraint region. Sometimes we may\n",
      "content_token: tensor([  101,  2964, 14255, 16468, 10879,  1805,   119,  6844,  1200,   418,\n",
      "         1209,  1871,  1107,   170,  2610, 14255, 16468, 10879,  1805,   119,\n",
      "         5875,  1195,  1336,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Sometimes we may wish to use explicit constraints rather than penalties. As described in section\n",
      "content_token: tensor([  101,  5875,  1195,  1336,  3683,  1106,  1329, 14077, 15651,  1897,\n",
      "         1190, 13095,   119,  1249,  1758,  1107,  2237,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in section 4.4, we can modify algorithms such as stochastic gradient descent to take a step\n",
      "content_token: tensor([  101,  1107,  2237,   125,   119,   125,   117,  1195,  1169, 22015,\n",
      "        14975,  1216,  1112,   188,  2430,  7147,  5668, 19848,  6585,  1106,\n",
      "         1321,   170,  2585,   102])\n",
      "entity_list: ['stochastic gradient descent']\n",
      "entity_token: [tensor([  188,  2430,  7147,  5668, 19848,  6585])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to take a step downhill on J(θ) and then project θ back to the nearest point that satisfies Ω(θ) <\n",
      "content_token: tensor([  101,  1106,  1321,   170,  2585, 23895,  1113,   147,   113,   425,\n",
      "          114,  1105,  1173,  1933,   425,  1171,  1106,  1103,  6830,  1553,\n",
      "         1115,  2068,  1548, 16847,   413,   113,   425,   114,   133,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: satisfies Ω(θ) < k. This can be useful if we have an idea of what value of k is appropriate and do\n",
      "content_token: tensor([  101,  2068,  1548, 16847,   413,   113,   425,   114,   133,   180,\n",
      "          119,  1188,  1169,  1129,  5616,  1191,  1195,  1138,  1126,  1911,\n",
      "         1104,  1184,  2860,  1104,   180,  1110,  5806,  1105,  1202,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: appropriate and do not want to spend time searching for the value of α that corresponds to this k.\n",
      "content_token: tensor([  101,  5806,  1105,  1202,  1136,  1328,  1106,  4511,  1159,  6205,\n",
      "         1111,  1103,  2860,  1104,   418,  1115, 15497,  1106,  1142,   180,\n",
      "          119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to this k. Anotherreason touse explicitconstraintsandreprojectionrather than enforcing constraints\n",
      "content_token: tensor([  101,  1106,  1142,   180,   119,  2543, 11811,  2142,  1106,  5613,\n",
      "        14077,  7235, 16468, 10879,  9995, 11114,  1643,  2180, 26857,  7625,\n",
      "         4679,  1190,  4035, 20586, 15651,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: constraints with penalties is that penalties can cause non-convex optimization procedures to get\n",
      "content_token: tensor([  101, 15651,  1114, 13095,  1110,  1115, 13095,  1169,  2612,  1664,\n",
      "          118, 20137, 25161,  8826,  1106,  1243,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: procedures to get stuck in local minima corresponding to small θ. When training neural networks,\n",
      "content_token: tensor([  101,  8826,  1106,  1243,  5342,  1107,  1469,  8715,  1918,  7671,\n",
      "         1106,  1353,   425,   119,  1332,  2013, 18250,  6379,   117,   102])\n",
      "entity_list: ['neural networks']\n",
      "entity_token: [tensor([18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: neural networks, this usually manifests as neural networks that train with several “dead units.”\n",
      "content_token: tensor([  101, 18250,  6379,   117,  1142,  1932, 23487,  1116,  1112, 18250,\n",
      "         6379,  1115,  2669,  1114,  1317,   789,  2044,  2338,   119,   790,\n",
      "          102])\n",
      "entity_list: ['neural networks']\n",
      "entity_token: [tensor([18250,  6379])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: “dead units.” These are units that do not contribute much to the behavior of the function learned\n",
      "content_token: tensor([ 101,  789, 2044, 2338,  119,  790, 1636, 1132, 2338, 1115, 1202, 1136,\n",
      "        8681, 1277, 1106, 1103, 4658, 1104, 1103, 3053, 3560,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: function learned by the network because the weights going into or out of them are all very small.\n",
      "content_token: tensor([  101,  3053,  3560,  1118,  1103,  2443,  1272,  1103, 17981,  1280,\n",
      "         1154,  1137,  1149,  1104,  1172,  1132,  1155,  1304,  1353,   119,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: are all very small. When training with a penalty on the norm of the weights, these configurations\n",
      "content_token: tensor([  101,  1132,  1155,  1304,  1353,   119,  1332,  2013,  1114,   170,\n",
      "         6180,  1113,  1103, 18570,  1104,  1103, 17981,   117,  1292, 25209,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: configurations can be locally optimal, even if it is possible to significantly reduce J by making\n",
      "content_token: tensor([  101, 25209,  1169,  1129,  6889, 17307,   117,  1256,  1191,  1122,\n",
      "         1110,  1936,  1106,  5409,  4851,   147,  1118,  1543,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: reduce J by making the weights larger. Explicit constraints implemented by re-projection can work\n",
      "content_token: tensor([  101,  4851,   147,  1118,  1543,  1103, 17981,  2610,   119, 16409,\n",
      "         1643, 18726, 15651,  7042,  1118,  1231,   118, 15178,  1169,  1250,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can work much better in these cases because they do not encourage the weights to approach the\n",
      "content_token: tensor([  101,  1169,  1250,  1277,  1618,  1107,  1292,  2740,  1272,  1152,\n",
      "         1202,  1136,  8343,  1103, 17981,  1106,  3136,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to approach the origin. Explicit constraints implemented by re-projection only have an effect when\n",
      "content_token: tensor([  101,  1106,  3136,  1103,  4247,   119, 16409,  1643, 18726, 15651,\n",
      "         7042,  1118,  1231,   118, 15178,  1178,  1138,  1126,  2629,  1165,\n",
      "          102])\n",
      "entity_list: ['re-projection']\n",
      "entity_token: [tensor([ 1231,   118, 15178])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: have an effect when the weights become large and attempt to leave the constraint region. Finally,\n",
      "content_token: tensor([  101,  1138,  1126,  2629,  1165,  1103, 17981,  1561,  1415,  1105,\n",
      "         2661,  1106,  1817,  1103, 14255, 16468, 10879,  1805,   119,  4428,\n",
      "          117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: region. Finally, explicit constraints with reprojection can be useful because they impose some\n",
      "content_token: tensor([  101,  1805,   119,  4428,   117, 14077, 15651,  1114,  1231,  1643,\n",
      "         2180, 26857,  1169,  1129,  5616,  1272,  1152, 19103,  1199,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: they impose some stability on the optimization procedure. When using high learning rates, it is\n",
      "content_token: tensor([  101,  1152, 19103,  1199,  9397,  1113,  1103, 25161,  7791,   119,\n",
      "         1332,  1606,  1344,  3776,  5600,   117,  1122,  1110,   102])\n",
      "entity_list: ['learning rates', 'optimization procedure']\n",
      "entity_token: [tensor([3776, 5600]), tensor([25161,  7791])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: rates, it is possible to encounter a positive feedback loop in which large weights induce large\n",
      "content_token: tensor([  101,  5600,   117,  1122,  1110,  1936,  1106,  8107,   170,  3112,\n",
      "        13032,  7812,  1107,  1134,  1415, 17981, 21497,  1415,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: induce large gradients which then induce a large update to the weights. If these updates\n",
      "content_token: tensor([  101, 21497,  1415, 19848,  1116,  1134,  1173, 21497,   170,  1415,\n",
      "        11984,  1106,  1103, 17981,   119,  1409,  1292, 15549,   102])\n",
      "entity_list: ['gradients', 'weights', 'updates']\n",
      "entity_token: [tensor([19848,  1116]), tensor([17981]), tensor([15549])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: If these updates consistently increase the size of the weights, then θ rapidly moves away from the\n",
      "content_token: tensor([  101,  1409,  1292, 15549, 10887,  2773,  1103,  2060,  1104,  1103,\n",
      "        17981,   117,  1173,   425,  5223,  5279,  1283,  1121,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: moves away from the origin until numerical overflow occurs. Explicit constraints with reprojection\n",
      "content_token: tensor([  101,  5279,  1283,  1121,  1103,  4247,  1235, 18294,  1166, 12712,\n",
      "         4365,   119, 16409,  1643, 18726, 15651,  1114,  1231,  1643,  2180,\n",
      "        26857,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with reprojection prevent this feedback loop from continuing to increase the magnitude of the\n",
      "content_token: tensor([  101,  1114,  1231,  1643,  2180, 26857,  3843,  1142, 13032,  7812,\n",
      "         1121,  5542,  1106,  2773,  1103, 10094,  1104,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: magnitude of the weights without bound. Hinton et al. (2012c) recommend using constraints combined\n",
      "content_token: tensor([  101, 10094,  1104,  1103, 17981,  1443,  4930,   119,  8790, 13124,\n",
      "         3084,  2393,   119,   113,  1368,  1665,   114, 18029,  1606, 15651,\n",
      "         3490,   102])\n",
      "entity_list: ['Hinton et al.']\n",
      "entity_token: [tensor([ 8790, 13124,  3084,  2393,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: combined with a high learning rate to allowrapid exploration ofparameter spacewhile maintaining\n",
      "content_token: tensor([  101,  3490,  1114,   170,  1344,  3776,  2603,  1106,  2621, 14543,\n",
      "         2386, 10016,  1104, 17482, 16470,  2083,  2000, 21053,  8338,   102])\n",
      "entity_list: ['learning rate']\n",
      "entity_token: [tensor([3776, 2603])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: maintaining some stability. In particular, Hinton et al. (2012c) recommend a strategy introduced by\n",
      "content_token: tensor([  101,  8338,  1199,  9397,   119,  1130,  2440,   117,  8790, 13124,\n",
      "         3084,  2393,   119,   113,  1368,  1665,   114, 18029,   170,  5564,\n",
      "         2234,  1118,   102])\n",
      "entity_list: ['Hinton et al.']\n",
      "entity_token: [tensor([ 8790, 13124,  3084,  2393,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: introduced by Srebro and Shraibman (2005): constraining the norm of each column of the weight\n",
      "content_token: tensor([  101,  2234,  1118,  8731, 15581,  2180,  1105,   156, 20955, 13292,\n",
      "         1399,   113,  1478,   114,   131, 14255, 16468, 16534,  1103, 18570,\n",
      "         1104,  1296,  5551,  1104,  1103,  2841,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the weight matrix 238\n",
      "content_token: tensor([  101,  1104,  1103,  2841,  8952, 24370,   102])\n",
      "entity_list: ['weight matrix']\n",
      "entity_token: [tensor([2841, 8952])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS the second derivatives of the cost function.\n",
      "content_token: tensor([  101,  8203,   129,   119,   152,  2101, 21669, 14038,  5301, 13821,\n",
      "        24805,   143,  9565,   157,  9664, 11607, 15740, 18581, 16668,   150,\n",
      "        15609, 21678,  1708,  1103,  1248, 18952,  1104,  1103,  2616,  3053,\n",
      "          119,   102])\n",
      "entity_list: ['training deep models']\n",
      "entity_token: [tensor([2013, 1996, 3584])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the cost function. Finally, we conclude with a review of several optimization strategies that are\n",
      "content_token: tensor([  101,  1103,  2616,  3053,   119,  4428,   117,  1195, 17581,  1114,\n",
      "          170,  3189,  1104,  1317, 25161, 10700,  1115,  1132,   102])\n",
      "entity_list: ['cost function', 'optimization strategies']\n",
      "entity_token: [tensor([2616, 3053]), tensor([25161, 10700])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: strategies that are formed by combining simple optimization algorithms into higher-level\n",
      "content_token: tensor([  101, 10700,  1115,  1132,  1824,  1118, 12459,  3014, 25161, 14975,\n",
      "         1154,  2299,   118,  1634,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: into higher-level procedures. 8.1 How Learning Differs from Pure Optimization Optimization\n",
      "content_token: tensor([  101,  1154,  2299,   118,  1634,  8826,   119,   129,   119,   122,\n",
      "         1731,  9681, 12120, 12929,  1116,  1121, 16665,  9126,  3121,  3080,\n",
      "         8569,  9126,  3121,  3080,  8569,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Optimization algorithms used for training of deep models differ from traditional optimization\n",
      "content_token: tensor([  101,  9126,  3121,  3080,  8569, 14975,  1215,  1111,  2013,  1104,\n",
      "         1996,  3584, 11271,  1121,  2361, 25161,   102])\n",
      "entity_list: ['deep models']\n",
      "entity_token: [tensor([1996, 3584])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: optimization algorithms in several ways. Machine learning usually acts indirectly. In most machine\n",
      "content_token: tensor([  101, 25161, 14975,  1107,  1317,  3242,   119,  7792,  3776,  1932,\n",
      "         4096, 18814,   119,  1130,  1211,  3395,   102])\n",
      "entity_list: ['output: optimization algorithms']\n",
      "entity_token: [tensor([ 5964,   131, 25161, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In most machine learning scenarios, we care about some performance measure P, that is defined with\n",
      "content_token: tensor([  101,  1130,  1211,  3395,  3776, 18414,   117,  1195,  1920,  1164,\n",
      "         1199,  2099,  4929,   153,   117,  1115,  1110,  3393,  1114,   102])\n",
      "entity_list: ['output: machine learning']\n",
      "entity_token: [tensor([5964,  131, 3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is defined with respect to the test set and may also be intractable. We therefore optimize P only\n",
      "content_token: tensor([  101,  1110,  3393,  1114,  4161,  1106,  1103,  2774,  1383,  1105,\n",
      "         1336,  1145,  1129,  1107, 15017,  1895,   119,  1284,  3335, 11769,\n",
      "         3121, 19092,   153,  1178,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: optimize P only indirectly. We reduce a different cost function J(θ) in the hope that doing so will\n",
      "content_token: tensor([  101, 11769,  3121, 19092,   153,  1178, 18814,   119,  1284,  4851,\n",
      "          170,  1472,  2616,  3053,   147,   113,   425,   114,  1107,  1103,\n",
      "         2810,  1115,  1833,  1177,  1209,   102])\n",
      "entity_list: ['output: cost function']\n",
      "entity_token: [tensor([5964,  131, 2616, 3053])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that doing so will improve P. This is in contrast to pure optimization, where minimizing J is a\n",
      "content_token: tensor([  101,  1115,  1833,  1177,  1209,  4607,   153,   119,  1188,  1110,\n",
      "         1107,  5014,  1106,  5805, 25161,   117,  1187,  8715, 25596,   147,\n",
      "         1110,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: minimizing J is a goal in and of itself. Optimization algorithms for training deep models also\n",
      "content_token: tensor([  101,  8715, 25596,   147,  1110,   170,  2273,  1107,  1105,  1104,\n",
      "         2111,   119,  9126,  3121,  3080,  8569, 14975,  1111,  2013,  1996,\n",
      "         3584,  1145,   102])\n",
      "entity_list: ['minimizing J', 'Optimization algorithms', 'training deep models']\n",
      "entity_token: [tensor([ 8715, 25596,   147]), tensor([ 9126,  3121,  3080,  8569, 14975]), tensor([2013, 1996, 3584])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: deep models also typically include some specialization on the specific structure of machine\n",
      "content_token: tensor([ 101, 1996, 3584, 1145, 3417, 1511, 1199, 1957, 2734, 1113, 1103, 2747,\n",
      "        2401, 1104, 3395,  102])\n",
      "entity_list: ['deep models']\n",
      "entity_token: [tensor([1996, 3584])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of machine learning objective functions. Typically, the cost function can be written as an average\n",
      "content_token: tensor([  101,  1104,  3395,  3776,  7649,  4226,   119, 16304,   117,  1103,\n",
      "         2616,  3053,  1169,  1129,  1637,  1112,  1126,  1903,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as an average over the training set, such as J(θ) = E L(f(x;θ),y), (8.1) (x,y) pˆ data ∼ where L is\n",
      "content_token: tensor([ 101, 1112, 1126, 1903, 1166, 1103, 2013, 1383,  117, 1216, 1112,  147,\n",
      "         113,  425,  114,  134,  142,  149,  113,  175,  113,  193,  132,  425,\n",
      "         114,  117,  194,  114,  117,  113,  129,  119,  122,  114,  113,  193,\n",
      "         117,  194,  114,  100, 2233,  100, 1187,  149, 1110,  102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: data ∼ where L is the per-example loss function, f(x;θ) is the predicted output when the input is\n",
      "content_token: tensor([  101,  2233,   100,  1187,   149,  1110,  1103,  1679,   118,  1859,\n",
      "         2445,  3053,   117,   175,   113,   193,   132,   425,   114,  1110,\n",
      "         1103, 10035,  5964,  1165,  1103,  7758,  1110,   102])\n",
      "entity_list: ['per-example loss function']\n",
      "entity_token: [tensor([1679,  118, 1859, 2445, 3053])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: when the input is x, pˆ is the empirical distribution. In the supervised learning case, data y is\n",
      "content_token: tensor([  101,  1165,  1103,  7758,  1110,   193,   117,   100,  1110,  1103,\n",
      "        20607,  3735,   119,  1130,  1103, 14199,  3776,  1692,   117,  2233,\n",
      "          194,  1110,   102])\n",
      "entity_list: ['supervised learning']\n",
      "entity_token: [tensor([14199,  3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: case, data y is the target output. Throughout this chapter, we develop the unregularized supervised\n",
      "content_token: tensor([  101,  1692,   117,  2233,   194,  1110,  1103,  4010,  5964,   119,\n",
      "         7092,  1142,  6073,   117,  1195,  3689,  1103,  8362,  1874, 13830,\n",
      "         5815,  2200, 14199,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: supervised case, where the arguments to L are f(x;θ) and y. However, it is trivial to extend this\n",
      "content_token: tensor([  101, 14199,  1692,   117,  1187,  1103,  9989,  1106,   149,  1132,\n",
      "          175,   113,   193,   132,   425,   114,  1105,   194,   119,  1438,\n",
      "          117,  1122,  1110, 23594,  1106,  7532,  1142,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to extend this development, for example, to include θ or x as arguments, or to exclude y as\n",
      "content_token: tensor([  101,  1106,  7532,  1142,  1718,   117,  1111,  1859,   117,  1106,\n",
      "         1511,   425,  1137,   193,  1112,  9989,   117,  1137,  1106, 26132,\n",
      "          194,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: or to exclude y as arguments, in order to develop various forms of regularization or unsupervised\n",
      "content_token: tensor([  101,  1137,  1106, 26132,   194,  1112,  9989,   117,  1107,  1546,\n",
      "         1106,  3689,  1672,  2769,  1104,  2366,  2734,  1137,  8362,  6385,\n",
      "         3365, 16641,  1181,   102])\n",
      "entity_list: ['regularization', 'unsupervised']\n",
      "entity_token: [tensor([2366, 2734]), tensor([ 8362,  6385,  3365, 16641,  1181])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 1, 1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: or unsupervised learning. Equation 8.1 defines an objective function with respect to the training\n",
      "content_token: tensor([  101,  1137,  8362,  6385,  3365, 16641,  1181,  3776,   119,   142,\n",
      "        13284,  2116,   129,   119,   122, 12028,  1126,  7649,  3053,  1114,\n",
      "         4161,  1106,  1103,  2013,   102])\n",
      "entity_list: ['unsupervised learning']\n",
      "entity_token: [tensor([ 8362,  6385,  3365, 16641,  1181,  3776])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to the training set. We would usually prefer to minimize the corresponding objective function where\n",
      "content_token: tensor([  101,  1106,  1103,  2013,  1383,   119,  1284,  1156,  1932,  9353,\n",
      "         1106, 20220,  1103,  7671,  7649,  3053,  1187,   102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: function where the expectation is taken across the data generating distribution p rather than just\n",
      "content_token: tensor([  101,  3053,  1187,  1103, 19351,  1110,  1678,  1506,  1103,  2233,\n",
      "        12713,  3735,   185,  1897,  1190,  1198,   102])\n",
      "entity_list: ['data generating distribution']\n",
      "entity_token: [tensor([ 2233, 12713,  3735])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: p rather than just data over the finite training set: J (θ) = E L(f(x;θ),y). (8.2) ∗ (x,y) p data ∼\n",
      "content_token: tensor([  101,   185,  1897,  1190,  1198,  2233,  1166,  1103, 10996,  2013,\n",
      "         1383,   131,   147,   113,   425,   114,   134,   142,   149,   113,\n",
      "          175,   113,   193,   132,   425,   114,   117,   194,   114,   119,\n",
      "          113,   129,   119,   123,   114,   852,   113,   193,   117,   194,\n",
      "          114,   185,  2233,   100,   102])\n",
      "entity_list: ['finite training set']\n",
      "entity_token: [tensor([10996,  2013,  1383])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ∗ (x,y) p data ∼ 8.1.1 Empirical Risk Minimization The goal of a machine learning algorithm is to\n",
      "content_token: tensor([  101,   852,   113,   193,   117,   194,   114,   185,  2233,   100,\n",
      "          129,   119,   122,   119,   122, 18653,  8508, 17211, 19547, 14393,\n",
      "         3080,  8569,  1109,  2273,  1104,   170,  3395,  3776,  9932,  1110,\n",
      "         1106,   102])\n",
      "entity_list: ['machine learning algorithm']\n",
      "entity_token: [tensor([3395, 3776, 9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 2, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithm is to reduce the expected generalization error given by equation 8.2. This quantity is\n",
      "content_token: tensor([  101,  9932,  1110,  1106,  4851,  1103,  2637,  1704,  2734,  7353,\n",
      "         1549,  1118,  8381,   129,   119,   123,   119,  1188, 11978,  1110,\n",
      "          102])\n",
      "entity_list: ['expected generalization error']\n",
      "entity_token: [tensor([2637, 1704, 2734, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This quantity is known as the risk. We emphasize here that the expectation is taken over the true\n",
      "content_token: tensor([  101,  1188, 11978,  1110,  1227,  1112,  1103,  3187,   119,  1284,\n",
      "        19291,  1303,  1115,  1103, 19351,  1110,  1678,  1166,  1103,  2276,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: taken over the true underlying distribution p . If we knew data the true distribution p (x,y), risk\n",
      "content_token: tensor([  101,  1678,  1166,  1103,  2276, 10311,  3735,   185,   119,  1409,\n",
      "         1195,  1450,  2233,  1103,  2276,  3735,   185,   113,   193,   117,\n",
      "          194,   114,   117,  3187,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: p (x,y), risk minimization would be an optimization task data 275 CHAPTER 8. OPTIMIZATION FOR\n",
      "content_token: tensor([  101,   185,   113,   193,   117,   194,   114,   117,  3187,  8715,\n",
      "         3080,  8569,  1156,  1129,  1126, 25161,  4579,  2233, 18783,  8203,\n",
      "          129,   119,   152,  2101, 21669, 14038,  5301, 13821, 24805,   143,\n",
      "         9565,   102])\n",
      "entity_list: ['risk minimization']\n",
      "entity_token: [tensor([3187, 8715, 3080, 8569])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 8. OPTIMIZATION FOR TRAINING DEEP MODELS solvable by an optimization algorithm. However, when we do\n",
      "content_token: tensor([  101,   129,   119,   152,  2101, 21669, 14038,  5301, 13821, 24805,\n",
      "          143,  9565,   157,  9664, 11607, 15740, 18581, 16668,   150, 15609,\n",
      "        21678,  1708,  1177,  1233, 12598,  1118,  1126, 25161,  9932,   119,\n",
      "         1438,   117,  1165,  1195,  1202,   102])\n",
      "entity_list: ['optimization algorithm']\n",
      "entity_token: [tensor([25161,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: However, when we do not know p (x,y) data but only have a training set of samples, we have a\n",
      "content_token: tensor([ 101, 1438,  117, 1165, 1195, 1202, 1136, 1221,  185,  113,  193,  117,\n",
      "         194,  114, 2233, 1133, 1178, 1138,  170, 2013, 1383, 1104, 8025,  117,\n",
      "        1195, 1138,  170,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: samples, we have a machine learning problem. The simplest way to convert a machine learning problem\n",
      "content_token: tensor([  101,  8025,   117,  1195,  1138,   170,  3395,  3776,  2463,   119,\n",
      "         1109, 23565,  1236,  1106, 10454,   170,  3395,  3776,  2463,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning problem back into an op- timization problem is to minimize the expected loss on the\n",
      "content_token: tensor([  101,  3776,  2463,  1171,  1154,  1126, 11769,   118,   189,  4060,\n",
      "         2734,  2463,  1110,  1106, 20220,  1103,  2637,  2445,  1113,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: loss on the training set. This means replacing the true distribution p(x,y) with the empirical\n",
      "content_token: tensor([  101,  2445,  1113,  1103,  2013,  1383,   119,  1188,  2086,  5861,\n",
      "         1103,  2276,  3735,   185,   113,   193,   117,   194,   114,  1114,\n",
      "         1103, 20607,   102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with the empirical distribution pˆ(x,y) defined by the training set. We now minimize the empirical\n",
      "content_token: tensor([  101,  1114,  1103, 20607,  3735,   100,   113,   193,   117,   194,\n",
      "          114,  3393,  1118,  1103,  2013,  1383,   119,  1284,  1208, 20220,\n",
      "         1103, 20607,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the empirical risk m 1 E [L(f(x;θ),y)] = L(f(x(i);θ),y(i) ) (8.3) x,y pˆdata(x,y) m ∼ i=1  where m\n",
      "content_token: tensor([  101,  1103, 20607,  3187,   182,   122,   142,   164,   149,   113,\n",
      "          175,   113,   193,   132,   425,   114,   117,   194,   114,   166,\n",
      "          134,   149,   113,   175,   113,   193,   113,   178,   114,   132,\n",
      "          425,   114,   117,   194,   113,   178,   114,   114,   113,   129,\n",
      "          119,   124,   114,   193,   117,   194,   100,   113,   193,   117,\n",
      "          194,   114,   182,   100,   178,   134,   122,  1187,   182,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: m ∼ i=1  where m is the number of training examples. The training process based on minimizing this\n",
      "content_token: tensor([  101,   182,   100,   178,   134,   122,  1187,   182,  1110,  1103,\n",
      "         1295,  1104,  2013,  5136,   119,  1109,  2013,  1965,  1359,  1113,\n",
      "         8715, 25596,  1142,   102])\n",
      "entity_list: ['training examples']\n",
      "entity_token: [tensor([2013, 5136])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on minimizing this average training error is known as empirical risk minimization. In this setting,\n",
      "content_token: tensor([  101,  1113,  8715, 25596,  1142,  1903,  2013,  7353,  1110,  1227,\n",
      "         1112, 20607,  3187,  8715,  3080,  8569,   119,  1130,  1142,  3545,\n",
      "          117,   102])\n",
      "entity_list: ['empirical risk minimization']\n",
      "entity_token: [tensor([20607,  3187,  8715,  3080,  8569])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In this setting, machine learning is still very similar to straightforward optimization. Rather\n",
      "content_token: tensor([  101,  1130,  1142,  3545,   117,  3395,  3776,  1110,  1253,  1304,\n",
      "         1861,  1106, 21546, 25161,   119, 11056,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Rather than optimizing the risk directly, we optimize the empirical risk, and hope that the risk\n",
      "content_token: tensor([  101, 11056,  1190, 11769,  3121, 25596,  1103,  3187,  2626,   117,\n",
      "         1195, 11769,  3121, 19092,  1103, 20607,  3187,   117,  1105,  2810,\n",
      "         1115,  1103,  3187,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: hope that the risk decreases significantly as well. A variety of theoretical results establish\n",
      "content_token: tensor([  101,  2810,  1115,  1103,  3187, 19377,  5409,  1112,  1218,   119,\n",
      "          138,  2783,  1104, 10093,  2686,  4586,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: results establish conditions under which the true risk can be expected to decrease by various\n",
      "content_token: tensor([ 101, 2686, 4586, 2975, 1223, 1134, 1103, 2276, 3187, 1169, 1129, 2637,\n",
      "        1106, 9711, 1118, 1672,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: decrease by various amounts. However, empirical risk minimization is prone to overfitting. Models\n",
      "content_token: tensor([  101,  9711,  1118,  1672,  7919,   119,  1438,   117, 20607,  3187,\n",
      "         8715,  3080,  8569,  1110, 13557,  1106,  1166, 14067,  1916,   119,\n",
      "        24025,   102])\n",
      "entity_list: ['empirical risk minimization']\n",
      "entity_token: [tensor([20607,  3187,  8715,  3080,  8569])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: overfitting. Models with high capacity can simply memorize the training set. In many cases,\n",
      "content_token: tensor([  101,  1166, 14067,  1916,   119, 24025,  1114,  1344,  3211,  1169,\n",
      "         2566,  1143, 26271,  3708,  1103,  2013,  1383,   119,  1130,  1242,\n",
      "         2740,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: set. In many cases, empirical risk minimization is not really feasible. The most effective modern\n",
      "content_token: tensor([  101,  1383,   119,  1130,  1242,  2740,   117, 20607,  3187,  8715,\n",
      "         3080,  8569,  1110,  1136,  1541, 25667,   119,  1109,  1211,  3903,\n",
      "         2030,   102])\n",
      "entity_list: ['empirical risk minimization']\n",
      "entity_token: [tensor([20607,  3187,  8715,  3080,  8569])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: effective modern optimization algorithms are based on gradient descent, but many useful loss\n",
      "content_token: tensor([  101,  3903,  2030, 25161, 14975,  1132,  1359,  1113, 19848,  6585,\n",
      "          117,  1133,  1242,  5616,  2445,   102])\n",
      "entity_list: ['gradient descent', 'optimization algorithms']\n",
      "entity_token: [tensor([19848,  6585]), tensor([25161, 14975])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: many useful loss functions, such as 0-1 loss, have no useful derivatives (the derivative is either\n",
      "content_token: tensor([  101,  1242,  5616,  2445,  4226,   117,  1216,  1112,   121,   118,\n",
      "          122,  2445,   117,  1138,  1185,  5616, 18952,   113,  1103, 14478,\n",
      "         1110,  1719,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is either zero or undefined everywhere). These two problems mean that, in the context of deep\n",
      "content_token: tensor([  101,  1110,  1719,  6756,  1137,  5576, 11470,  9044,  7244,   114,\n",
      "          119,  1636,  1160,  2645,  1928,  1115,   117,  1107,  1103,  5618,\n",
      "         1104,  1996,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the context of deep learning, we rarely use empirical risk minimization. Instead, we must use a\n",
      "content_token: tensor([  101,  1103,  5618,  1104,  1996,  3776,   117,  1195,  6034,  1329,\n",
      "        20607,  3187,  8715,  3080,  8569,   119,  3743,   117,  1195,  1538,\n",
      "         1329,   170,   102])\n",
      "entity_list: ['empirical risk minimization']\n",
      "entity_token: [tensor([20607,  3187,  8715,  3080,  8569])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we must use a slightly different approach, in which the quantity that we actually optimize is even\n",
      "content_token: tensor([  101,  1195,  1538,  1329,   170,  2776,  1472,  3136,   117,  1107,\n",
      "         1134,  1103, 11978,  1115,  1195,  2140, 11769,  3121, 19092,  1110,\n",
      "         1256,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: optimize is even more different from the quantity that we truly want to optimize. 8.1.2 Surrogate\n",
      "content_token: tensor([  101, 11769,  3121, 19092,  1110,  1256,  1167,  1472,  1121,  1103,\n",
      "        11978,  1115,  1195,  5098,  1328,  1106, 11769,  3121, 19092,   119,\n",
      "          129,   119,   122,   119,   123, 17078, 25232,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 8.1.2 Surrogate Loss Functions and Early Stopping Sometimes, the loss function we actually care\n",
      "content_token: tensor([  101,   129,   119,   122,   119,   123, 17078, 25232, 22008, 16068,\n",
      "        13945,  1105,  4503,  6682,  2624,  5875,   117,  1103,  2445,  3053,\n",
      "         1195,  2140,  1920,   102])\n",
      "entity_list: ['Early Stopping']\n",
      "entity_token: [tensor([4503, 6682, 2624])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we actually care about (say classification error) is not one that can be optimized efficiently. For\n",
      "content_token: tensor([  101,  1195,  2140,  1920,  1164,   113,  1474,  5393,  7353,   114,\n",
      "         1110,  1136,  1141,  1115,  1169,  1129, 11769,  3121, 26740, 19723,\n",
      "          119,  1370,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: efficiently. For example, exactly minimizing expected 0-1 loss is typically intractable\n",
      "content_token: tensor([  101, 19723,   119,  1370,  1859,   117,  2839,  8715, 25596,  2637,\n",
      "          121,   118,   122,  2445,  1110,  3417,  1107, 15017,  1895,   102])\n",
      "entity_list: ['expected 0-1 loss']\n",
      "entity_token: [tensor([2637,  121,  118,  122, 2445])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: intractable (exponential in the input dimension), even for a linear classifier (Marcotte and\n",
      "content_token: tensor([  101,  1107, 15017,  1895,   113,  4252,  5674, 21222,  2916,  1107,\n",
      "         1103,  7758, 11025,   114,   117,  1256,  1111,   170,  7378,  1705,\n",
      "        17792,   113,  8571,  3786,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (Marcotte and Savard, 1992). In such situations, one typically optimizes a surrogate loss function\n",
      "content_token: tensor([  101,   113,  8571,  3786,  1105, 17784, 24698,   117,  1924,   114,\n",
      "          119,  1130,  1216,  7832,   117,  1141,  3417, 11769,  3121, 19092,\n",
      "         1116,   170,  8910, 25232,  2445,  3053,   102])\n",
      "entity_list: ['surrogate loss function']\n",
      "entity_token: [tensor([ 8910, 25232,  2445,  3053])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
      "        1, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: loss function instead, which acts as a proxy but has advantages. For example, the negative\n",
      "content_token: tensor([  101,  2445,  3053,  1939,   117,  1134,  4096,  1112,   170,  5250,\n",
      "        16844,  1133,  1144, 13300,   119,  1370,  1859,   117,  1103,  4366,\n",
      "          102])\n",
      "entity_list: ['loss function']\n",
      "entity_token: [tensor([2445, 3053])]\n",
      "label: tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the negative log-likelihood of the correct class is typically used as a surrogate for the 0-1 loss.\n",
      "content_token: tensor([  101,  1103,  4366,  9366,   118, 17843,  1104,  1103,  5663,  1705,\n",
      "         1110,  3417,  1215,  1112,   170,  8910, 25232,  1111,  1103,   121,\n",
      "          118,   122,  2445,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for the 0-1 loss. The negative log-likelihood allows the model to estimate the conditional\n",
      "content_token: tensor([  101,  1111,  1103,   121,   118,   122,  2445,   119,  1109,  4366,\n",
      "         9366,   118, 17843,  3643,  1103,  2235,  1106, 10301,  1103, 21152,\n",
      "          102])\n",
      "entity_list: ['negative log-likelihood']\n",
      "entity_token: [tensor([ 4366,  9366,   118, 17843])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the conditional probability of the classes, given the input, and if the model can do that well,\n",
      "content_token: tensor([  101,  1103, 21152,  9750,  1104,  1103,  3553,   117,  1549,  1103,\n",
      "         7758,   117,  1105,  1191,  1103,  2235,  1169,  1202,  1115,  1218,\n",
      "          117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can do that well, then it can pick the classes that yield the least classification error in\n",
      "content_token: tensor([  101,  1169,  1202,  1115,  1218,   117,  1173,  1122,  1169,  3368,\n",
      "         1103,  3553,  1115, 10972,  1103,  1655,  5393,  7353,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error in expectation. 276 CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS In some cases, a\n",
      "content_token: tensor([  101,  7353,  1107, 19351,   119,  1765,  1545,  8203,   129,   119,\n",
      "          152,  2101, 21669, 14038,  5301, 13821, 24805,   143,  9565,   157,\n",
      "         9664, 11607, 15740, 18581, 16668,   150, 15609, 21678,  1708,  1130,\n",
      "         1199,  2740,   117,   170,   102])\n",
      "entity_list: ['deep models']\n",
      "entity_token: [tensor([1996, 3584])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In some cases, a surrogate loss function actually results in being able to learn more. For example,\n",
      "content_token: tensor([  101,  1130,  1199,  2740,   117,   170,  8910, 25232,  2445,  3053,\n",
      "         2140,  2686,  1107,  1217,  1682,  1106,  3858,  1167,   119,  1370,\n",
      "         1859,   117,   102])\n",
      "entity_list: ['surrogate loss function']\n",
      "entity_token: [tensor([ 8910, 25232,  2445,  3053])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: more. For example, the test set 0-1 loss often continues to decrease for a long time after the\n",
      "content_token: tensor([ 101, 1167,  119, 1370, 1859,  117, 1103, 2774, 1383,  121,  118,  122,\n",
      "        2445, 1510, 3430, 1106, 9711, 1111,  170, 1263, 1159, 1170, 1103,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: long time after the training set 0-1 loss has reached zero, when training using the log-likelihood\n",
      "content_token: tensor([  101,  1263,  1159,  1170,  1103,  2013,  1383,   121,   118,   122,\n",
      "         2445,  1144,  1680,  6756,   117,  1165,  2013,  1606,  1103,  9366,\n",
      "          118, 17843,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the log-likelihood surrogate. This is because even when the expected 0-1 loss is zero, one can\n",
      "content_token: tensor([  101,  1103,  9366,   118, 17843,  8910, 25232,   119,  1188,  1110,\n",
      "         1272,  1256,  1165,  1103,  2637,   121,   118,   122,  2445,  1110,\n",
      "         6756,   117,  1141,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is zero, one can improve the robustness of the classifier by further pushing the classes apart from\n",
      "content_token: tensor([  101,  1110,  6756,   117,  1141,  1169,  4607,  1103, 17351,  1757,\n",
      "         1104,  1103,  1705, 17792,  1118,  1748,  5859,  1103,  3553,  3966,\n",
      "         1121,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: classes apart from each other, obtaining a more confident and reliable classifier, thus extracting\n",
      "content_token: tensor([  101,  3553,  3966,  1121,  1296,  1168,   117, 11621,   170,  1167,\n",
      "         9588,  1105, 10682,  1705, 17792,   117,  2456, 16143,  1158,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: thus extracting more information from the training data than would have been possible by simply\n",
      "content_token: tensor([  101,  2456, 16143,  1158,  1167,  1869,  1121,  1103,  2013,  2233,\n",
      "         1190,  1156,  1138,  1151,  1936,  1118,  2566,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: possible by simply minimizing the average 0-1 loss on the training set. A very important difference\n",
      "content_token: tensor([  101,  1936,  1118,  2566,  8715, 25596,  1103,  1903,   121,   118,\n",
      "          122,  2445,  1113,  1103,  2013,  1383,   119,   138,  1304,  1696,\n",
      "         3719,   102])\n",
      "entity_list: ['training set\\n\\n实体: average 0-1 loss', 'training set属于深度学习领域。']\n",
      "entity_token: [tensor([2013, 1383,  100,  100,  131, 1903,  121,  118,  122, 2445]), tensor([2013, 1383,  100,  100,  100,  100,  100,  100,  100,  100,  886])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: difference between optimization in general and optimization as we use it for training algorithms is\n",
      "content_token: tensor([  101,  3719,  1206, 25161,  1107,  1704,  1105, 25161,  1112,  1195,\n",
      "         1329,  1122,  1111,  2013, 14975,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithms is that training algorithms do not usually halt at a local minimum. Instead, a machine\n",
      "content_token: tensor([  101, 14975,  1110,  1115,  2013, 14975,  1202,  1136,  1932,  9700,\n",
      "         1120,   170,  1469,  5867,   119,  3743,   117,   170,  3395,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Instead, a machine learning algorithm usually minimizes a surrogate loss function but halts when a\n",
      "content_token: tensor([  101,  3743,   117,   170,  3395,  3776,  9932,  1932, 20220,  1116,\n",
      "          170,  8910, 25232,  2445,  3053,  1133,  9700,  1116,  1165,   170,\n",
      "          102])\n",
      "entity_list: ['machine learning algorithm', 'surrogate loss function']\n",
      "entity_token: [tensor([3395, 3776, 9932]), tensor([ 8910, 25232,  2445,  3053])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: but halts when a convergence criterion based on early stopping (section 7.8) is satisfied.\n",
      "content_token: tensor([  101,  1133,  9700,  1116,  1165,   170, 25628, 26440,  1359,  1113,\n",
      "         1346,  7202,   113,  2237,   128,   119,   129,   114,  1110,  8723,\n",
      "          119,   102])\n",
      "entity_list: ['early stopping']\n",
      "entity_token: [tensor([1346, 7202])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 7.8) is satisfied. Typically the early stopping criterion is based on the true underlying loss\n",
      "content_token: tensor([  101,   128,   119,   129,   114,  1110,  8723,   119, 16304,  1103,\n",
      "         1346,  7202, 26440,  1110,  1359,  1113,  1103,  2276, 10311,  2445,\n",
      "          102])\n",
      "entity_list: ['early stopping criterion']\n",
      "entity_token: [tensor([ 1346,  7202, 26440])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: underlying loss function, such as 0-1 loss measured on a validation set, and is designed to cause\n",
      "content_token: tensor([  101, 10311,  2445,  3053,   117,  1216,  1112,   121,   118,   122,\n",
      "         2445,  7140,  1113,   170,  9221,  1891,  1383,   117,  1105,  1110,\n",
      "         2011,  1106,  2612,   102])\n",
      "entity_list: ['underlying loss function']\n",
      "entity_token: [tensor([10311,  2445,  3053])]\n",
      "label: tensor([0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: designed to cause the algorithm to halt whenever overfitting begins to occur. Training often halts\n",
      "content_token: tensor([  101,  2011,  1106,  2612,  1103,  9932,  1106,  9700,  7747,  1166,\n",
      "        14067,  1916,  3471,  1106,  4467,   119,  5513,  1510,  9700,  1116,\n",
      "          102])\n",
      "entity_list: ['overfitting']\n",
      "entity_token: [tensor([ 1166, 14067,  1916])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: often halts while the surrogate loss function still has large derivatives, which is very different\n",
      "content_token: tensor([  101,  1510,  9700,  1116,  1229,  1103,  8910, 25232,  2445,  3053,\n",
      "         1253,  1144,  1415, 18952,   117,  1134,  1110,  1304,  1472,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is very different from the pure optimization setting, where an optimization algorithm is considered\n",
      "content_token: tensor([  101,  1110,  1304,  1472,  1121,  1103,  5805, 25161,  3545,   117,\n",
      "         1187,  1126, 25161,  9932,  1110,  1737,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is considered to have converged when the gradient becomes very small. 8.1.3 Batch and Minibatch\n",
      "content_token: tensor([  101,  1110,  1737,  1106,  1138, 14255,  4121,  3660,  1165,  1103,\n",
      "        19848,  3316,  1304,  1353,   119,   129,   119,   122,   119,   124,\n",
      "        21928,  1732,  1105, 14393, 14602,  1732,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Batch and Minibatch Algorithms One aspect of machine learning algorithms that separates them from\n",
      "content_token: tensor([  101, 21928,  1732,  1105, 14393, 14602,  1732,  2586, 18791,  7088,\n",
      "         4206,  1448,  7631,  1104,  3395,  3776, 14975,  1115, 20229,  1172,\n",
      "         1121,   102])\n",
      "entity_list: ['in deep learning', 'where datasets can be very large', 'this approach can be computationally expensive and time-consuming. \\n\\nTo address this issue', 'batch and minibatch optimization techniques were introduced. In batch optimization', 'the entire dataset is divided into smaller subsets called batches', 'and the model parameters are updated based on the average gradient computed from each batch. This approach reduces the memory requirements and allows for parallelization of computations.\\n\\nMinibatch optimization is similar to batch optimization but involves using even smaller subsets called minibatches. The minibatches are randomly sampled from the dataset', 'and the model parameters are updated based on the average gradient computed from each minibatch. This approach further reduces the memory requirements and allows for faster computations.\\n\\nBatch and minibatch optimization are commonly used in deep learning algorithms', 'such as neural networks', 'to improve training efficiency and scalability. These techniques have become essential in handling large-scale datasets and complex models in the field of deep learning.']\n",
      "entity_token: [tensor([1107, 1996, 3776]), tensor([ 1187,  2233, 27948,  1169,  1129,  1304,  1415]), tensor([ 1142,  3136,  1169,  1129, 19903,  1193,  5865,  1105,  1159,   118,\n",
      "        16114,   119,  1706,  4134,  1142,  2486]), tensor([15817,  1105,  8715, 14602,  1732, 25161,  4884,  1127,  2234,   119,\n",
      "         1130, 15817, 25161]), tensor([ 1103,  2072,  2233,  9388,  1110,  3233,  1154,  2964, 18005,  1116,\n",
      "         1270, 15817,  1279]), tensor([ 1105,  1103,  2235, 11934,  1132,  8054,  1359,  1113,  1103,  1903,\n",
      "        19848,  3254, 18505,  1121,  1296, 15817,   119,  1188,  3136, 13822,\n",
      "         1103,  2962,  5420,  1105,  3643,  1111,  5504,  2734,  1104,  3254,\n",
      "        19675,  1116,   119, 14393, 14602,  1732, 25161,  1110,  1861,  1106,\n",
      "        15817, 25161,  1133,  6808,  1606,  1256,  2964, 18005,  1116,  1270,\n",
      "         8715, 14602,  7486,   119,  1109,  8715, 14602,  7486,  1132, 19729,\n",
      "        20744,  1121,  1103,  2233,  9388]), tensor([ 1105,  1103,  2235, 11934,  1132,  8054,  1359,  1113,  1103,  1903,\n",
      "        19848,  3254, 18505,  1121,  1296,  8715, 14602,  1732,   119,  1188,\n",
      "         3136,  1748, 13822,  1103,  2962,  5420,  1105,  3643,  1111,  4946,\n",
      "         3254, 19675,  1116,   119, 21928,  1732,  1105,  8715, 14602,  1732,\n",
      "        25161,  1132,  3337,  1215,  1107,  1996,  3776, 14975]), tensor([ 1216,  1112, 18250,  6379]), tensor([ 1106,  4607,  2013,  8096,  1105,   188,  7867,  6328,   119,  1636,\n",
      "         4884,  1138,  1561,  6818,  1107,  8130,  1415,   118,  3418,  2233,\n",
      "        27948,  1105,  2703,  3584,  1107,  1103,  1768,  1104,  1996,  3776,\n",
      "          119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: separates them from general optimization algorithms is that the objective function usually\n",
      "content_token: tensor([  101, 20229,  1172,  1121,  1704, 25161, 14975,  1110,  1115,  1103,\n",
      "         7649,  3053,  1932,   102])\n",
      "entity_list: ['in deep learning', 'the objective function is often a non-linear and complex function that depends on the activation function used in the neural network.\\n\\nThe activation function introduces non-linearity into the neural network and allows it to learn complex patterns and representations. Common activation functions used in deep learning include the sigmoid function', 'the rectified linear unit ReLU function', 'and the hyperbolic tangent function. These activation functions transform the input data and introduce non-linearity to capture the complexity of the underlying data distribution.\\n\\nDue to the non-linear nature of the objective function in deep learning', 'traditional optimization algorithms may not be directly applicable. Instead', 'specialized optimization algorithms', 'such as stochastic gradient descent SGD', 'RMSprop', 'and Adam', 'are commonly used in deep learning. These algorithms iteratively update the model parameters based on the gradients computed from a subset of the training data.\\n\\nThe choice of activation function and optimization algorithm can significantly impact the performance and convergence of deep learning models. Researchers and practitioners in the field of deep learning are constantly exploring and developing new activation functions and optimization algorithms to improve the efficiency and effectiveness of deep learning models.']\n",
      "entity_token: [tensor([1107, 1996, 3776]), tensor([ 1103,  7649,  3053,  1110,  1510,   170,  1664,   118,  7378,  1105,\n",
      "         2703,  3053,  1115,  9113,  1113,  1103, 14915,  3053,  1215,  1107,\n",
      "         1103, 18250,  2443,   119,  1109, 14915,  3053, 14681,  1664,   118,\n",
      "         7378,  1785,  1154,  1103, 18250,  2443,  1105,  3643,  1122,  1106,\n",
      "         3858,  2703,  6692,  1105, 16539,   119,  6869, 14915,  4226,  1215,\n",
      "         1107,  1996,  3776,  1511,  1103, 27466,  1403,  3702,  2386,  3053]), tensor([ 1103,  1231,  5822,  6202,  7378,  2587, 11336,  2162,  2591,  3053]), tensor([ 1105,  1103,   177, 24312, 21022, 15925, 11549,  3053,   119,  1636,\n",
      "        14915,  4226, 11303,  1103,  7758,  2233,  1105,  8698,  1664,   118,\n",
      "         7378,  1785,  1106,  4821,  1103, 12133,  1104,  1103, 10311,  2233,\n",
      "         3735,   119,  4187,  1106,  1103,  1664,   118,  7378,  2731,  1104,\n",
      "         1103,  7649,  3053,  1107,  1996,  3776]), tensor([ 2361, 25161, 14975,  1336,  1136,  1129,  2626, 13036,   119,  3743]), tensor([ 7623, 25161, 14975]), tensor([ 1216,  1112,   188,  2430,  7147,  5668, 19848,  6585,   156,  2349,\n",
      "         2137]), tensor([  155,  7182,  1643, 12736]), tensor([1105, 3379]), tensor([ 1132,  3337,  1215,  1107,  1996,  3776,   119,  1636, 14975,  1122,\n",
      "        21126,  1193, 11984,  1103,  2235, 11934,  1359,  1113,  1103, 19848,\n",
      "         1116,  3254, 18505,  1121,   170, 18005,  1104,  1103,  2013,  2233,\n",
      "          119,  1109,  3026,  1104, 14915,  3053,  1105, 25161,  9932,  1169,\n",
      "         5409,  3772,  1103,  2099,  1105, 25628,  1104,  1996,  3776,  3584,\n",
      "          119, 26982,  1105, 16681,  1107,  1103,  1768,  1104,  1996,  3776,\n",
      "         1132,  7480, 12138,  1105,  4297,  1207, 14915,  4226,  1105, 25161,\n",
      "        14975,  1106,  4607,  1103,  8096,  1105, 12949,  1104,  1996,  3776,\n",
      "         3584,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: function usually decomposes as a sum over the training examples. Optimization algorithms for\n",
      "content_token: tensor([  101,  3053,  1932,  1260,  8178, 14811,  1116,  1112,   170,  7584,\n",
      "         1166,  1103,  2013,  5136,   119,  9126,  3121,  3080,  8569, 14975,\n",
      "         1111,   102])\n",
      "entity_list: ['deep learning typically involve the minimization of this sum over the training examples. This is known as empirical risk minimization', 'where the objective function is the average loss function over the training examples.\\n\\nIn deep learning', 'the objective function is commonly defined as the loss function', 'which quantifies the discrepancy between the predicted output of the model and the true output. The loss function measures how well the model is performing and provides feedback for the optimization algorithm to update the model parameters. Common loss functions used in deep learning include mean squared error MSE', 'cross-entropy loss', 'and softmax loss', 'depending on the nature of the prediction task.\\n\\nTo minimize the objective function', 'optimization algorithms for deep learning iteratively update the model parameters based on the gradients of the loss function with respect to the parameters. This process is known as backpropagation', 'and the desired convergence properties.\\n\\nSome commonly used optimization algorithms in deep learning include stochastic gradient descent SGD', 'which updates the parameters based on the gradients computed from a randomly selected subset of the training examples minibatches; Adam', 'which combines the concepts of momentum and adaptive learning rates; and RMSprop', \"which performs adaptive learning rate scaling based on the magnitude of the gradients.\\n\\nThese optimization algorithms are critical in training deep learning models as they ensure that the model converges to a good solution by iteratively minimizing the objective function. They provide the means to efficiently update the model parameters and improve the model's performance over time.\"]\n",
      "entity_token: [tensor([ 1996,  3776,  3417,  8803,  1103,  8715,  3080,  8569,  1104,  1142,\n",
      "         7584,  1166,  1103,  2013,  5136,   119,  1188,  1110,  1227,  1112,\n",
      "        20607,  3187,  8715,  3080,  8569]), tensor([1187, 1103, 7649, 3053, 1110, 1103, 1903, 2445, 3053, 1166, 1103, 2013,\n",
      "        5136,  119, 1130, 1996, 3776]), tensor([1103, 7649, 3053, 1110, 3337, 3393, 1112, 1103, 2445, 3053]), tensor([ 1134,   186, 27280,  9387,  1103,  6187,  1874, 10224,  3457,  1206,\n",
      "         1103, 10035,  5964,  1104,  1103,  2235,  1105,  1103,  2276,  5964,\n",
      "          119,  1109,  2445,  3053,  5252,  1293,  1218,  1103,  2235,  1110,\n",
      "         4072,  1105,  2790, 13032,  1111,  1103, 25161,  9932,  1106, 11984,\n",
      "         1103,  2235, 11934,   119,  6869,  2445,  4226,  1215,  1107,  1996,\n",
      "         3776,  1511,  1928, 23215,  7353, 10978,  2036]), tensor([ 2771,   118,  4035, 25444,  2445]), tensor([ 1105,  2991, 22871,  2445]), tensor([ 5763,  1113,  1103,  2731,  1104,  1103, 20770,  4579,   119,  1706,\n",
      "        20220,  1103,  7649,  3053]), tensor([25161, 14975,  1111,  1996,  3776,  1122, 21126,  1193, 11984,  1103,\n",
      "         2235, 11934,  1359,  1113,  1103, 19848,  1116,  1104,  1103,  2445,\n",
      "         3053,  1114,  4161,  1106,  1103, 11934,   119,  1188,  1965,  1110,\n",
      "         1227,  1112,  1171,  1643, 12736, 15446,  2116]), tensor([ 1105,  1103,  8759, 25628,  4625,   119,  1789,  3337,  1215, 25161,\n",
      "        14975,  1107,  1996,  3776,  1511,   188,  2430,  7147,  5668, 19848,\n",
      "         6585,   156,  2349,  2137]), tensor([ 1134, 15549,  1103, 11934,  1359,  1113,  1103, 19848,  1116,  3254,\n",
      "        18505,  1121,   170, 19729,  2700, 18005,  1104,  1103,  2013,  5136,\n",
      "         8715, 14602,  7486,   132,  3379]), tensor([ 1134, 14215,  1103,  8550,  1104, 11550,  1105, 25031,  3776,  5600,\n",
      "          132,  1105,   155,  7182,  1643, 12736]), tensor([ 1134, 10383, 25031,  3776,  2603,   188,  7867,  1158,  1359,  1113,\n",
      "         1103, 10094,  1104,  1103, 19848,  1116,   119,  1636, 25161, 14975,\n",
      "         1132,  3607,  1107,  2013,  1996,  3776,  3584,  1112,  1152,  4989,\n",
      "         1115,  1103,  2235, 14255,  4121,  7562,  1106,   170,  1363,  5072,\n",
      "         1118,  1122, 21126,  1193,  8715, 25596,  1103,  7649,  3053,   119,\n",
      "         1220,  2194,  1103,  2086,  1106, 19723, 11984,  1103,  2235, 11934,\n",
      "         1105,  4607,  1103,  2235,   112,   188,  2099,  1166,  1159,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithms for machine learning typically compute each update to the parameters based on an\n",
      "content_token: tensor([  101, 14975,  1111,  3395,  3776,  3417,  3254, 22662,  1296, 11984,\n",
      "         1106,  1103, 11934,  1359,  1113,  1126,   102])\n",
      "entity_list: ['objective function', 'the objective function is typically decomposed as a sum over the training examples', 'and optimization algorithms aim to minimize this objective function.\\n\\nThe main idea behind optimization algorithms for machine learning is to iteratively update the parameters of the model in the direction that reduces the objective function. The updates are computed based on the gradients of the objective function with respect to the parameters. The gradient indicates the direction of steepest ascent', 'we can move in the direction of steepest descent', 'which leads to a decrease in the objective function.\\n\\nOne commonly used optimization algorithm is gradient descent', 'which updates the parameters in the opposite direction of the gradient. The size of the update is determined by the learning rate', 'stochastic gradient descent SGD was introduced. SGD computes the gradient and updates the parameters based on a randomly selected subset of the training examples. This allows for faster updates and better scalability. However', 'mini-batch gradient descent combines the advantages of batch and stochastic gradient descent. Instead of using the entire dataset or a single example', 'mini-batch gradient descent computes the gradient and updates the parameters based on a small batch of training examples. This balances the computational efficiency of SGD with the stability of batch gradient descent.\\n\\nOther optimization algorithms for machine learning include adaptive learning rate methods like Adagrad', 'RMSprop', 'and Adam. These algorithms adjust the learning rate based on the historical gradients to improve convergence speed and stability.\\n\\nIn summary', 'optimization algorithms for machine learning compute updates to the model parameters based on the objective function', 'typically decomposed as a sum over the training examples. Gradient descent', 'stochastic gradient descent', 'mini-batch gradient descent', 'and adaptive learning rate methods are commonly used in machine learning to efficiently and effectively minimize the objective function and improve model performance.']\n",
      "entity_token: [tensor([7649, 3053]), tensor([ 1103,  7649,  3053,  1110,  3417,  1260,  8178, 13541,  1112,   170,\n",
      "         7584,  1166,  1103,  2013,  5136]), tensor([ 1105, 25161, 14975,  6457,  1106, 20220,  1142,  7649,  3053,   119,\n",
      "         1109,  1514,  1911,  1481, 25161, 14975,  1111,  3395,  3776,  1110,\n",
      "         1106,  1122, 21126,  1193, 11984,  1103, 11934,  1104,  1103,  2235,\n",
      "         1107,  1103,  2447,  1115, 13822,  1103,  7649,  3053,   119,  1109,\n",
      "        15549,  1132,  3254, 18505,  1359,  1113,  1103, 19848,  1116,  1104,\n",
      "         1103,  7649,  3053,  1114,  4161,  1106,  1103, 11934,   119,  1109,\n",
      "        19848,  6653,  1103,  2447,  1104,  9458,  2556, 19079]), tensor([1195, 1169, 1815, 1107, 1103, 2447, 1104, 9458, 2556, 6585]), tensor([ 1134,  4501,  1106,   170,  9711,  1107,  1103,  7649,  3053,   119,\n",
      "         1448,  3337,  1215, 25161,  9932,  1110, 19848,  6585]), tensor([ 1134, 15549,  1103, 11934,  1107,  1103,  3714,  2447,  1104,  1103,\n",
      "        19848,   119,  1109,  2060,  1104,  1103, 11984,  1110,  3552,  1118,\n",
      "         1103,  3776,  2603]), tensor([  188,  2430,  7147,  5668, 19848,  6585,   156,  2349,  2137,  1108,\n",
      "         2234,   119,   156,  2349,  2137,  3254, 22662,  1116,  1103, 19848,\n",
      "         1105, 15549,  1103, 11934,  1359,  1113,   170, 19729,  2700, 18005,\n",
      "         1104,  1103,  2013,  5136,   119,  1188,  3643,  1111,  4946, 15549,\n",
      "         1105,  1618,   188,  7867,  6328,   119,  1438]), tensor([ 8715,   118, 15817, 19848,  6585, 14215,  1103, 13300,  1104, 15817,\n",
      "         1105,   188,  2430,  7147,  5668, 19848,  6585,   119,  3743,  1104,\n",
      "         1606,  1103,  2072,  2233,  9388,  1137,   170,  1423,  1859]), tensor([ 8715,   118, 15817, 19848,  6585,  3254, 22662,  1116,  1103, 19848,\n",
      "         1105, 15549,  1103, 11934,  1359,  1113,   170,  1353, 15817,  1104,\n",
      "         2013,  5136,   119,  1188,  5233,  1116,  1103, 19903,  8096,  1104,\n",
      "          156,  2349,  2137,  1114,  1103,  9397,  1104, 15817, 19848,  6585,\n",
      "          119,  2189, 25161, 14975,  1111,  3395,  3776,  1511, 25031,  3776,\n",
      "         2603,  4069,  1176, 19222, 20561]), tensor([  155,  7182,  1643, 12736]), tensor([ 1105,  3379,   119,  1636, 14975, 14878,  1103,  3776,  2603,  1359,\n",
      "         1113,  1103,  3009, 19848,  1116,  1106,  4607, 25628,  2420,  1105,\n",
      "         9397,   119,  1130, 14940]), tensor([25161, 14975,  1111,  3395,  3776,  3254, 22662, 15549,  1106,  1103,\n",
      "         2235, 11934,  1359,  1113,  1103,  7649,  3053]), tensor([ 3417,  1260,  8178, 13541,  1112,   170,  7584,  1166,  1103,  2013,\n",
      "         5136,   119,   144,  9871,  9080,  6585]), tensor([  188,  2430,  7147,  5668, 19848,  6585]), tensor([ 8715,   118, 15817, 19848,  6585]), tensor([ 1105, 25031,  3776,  2603,  4069,  1132,  3337,  1215,  1107,  3395,\n",
      "         3776,  1106, 19723,  1105,  5877, 20220,  1103,  7649,  3053,  1105,\n",
      "         4607,  2235,  2099,   119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: based on an expected value of the cost function estimated using only a subset of the terms of the\n",
      "content_token: tensor([  101,  1359,  1113,  1126,  2637,  2860,  1104,  1103,  2616,  3053,\n",
      "         3555,  1606,  1178,   170, 18005,  1104,  1103,  2538,  1104,  1103,\n",
      "          102])\n",
      "entity_list: ['objective function. This idea is commonly used in optimization algorithms for machine learning', 'especially in the context of large-scale datasets and deep learning models.\\n\\nThe expected value of the cost function is estimated by considering only a subset of the terms of the objective function. This subset is randomly sampled from the entire dataset', 'which helps to reduce the computational burden and speed up the optimization process.\\n\\nThere are various optimization algorithms that utilize this concept. One well-known algorithm is stochastic gradient descent SGD', 'which updates the model parameters based on the gradient estimated from a randomly selected training example. The expectation is approximated by taking the average of the gradients computed from multiple iterations of updating the model parameters using different training examples.\\n\\nAnother variant is called mini-batch gradient descent', 'which is a compromise between batch gradient descent and stochastic gradient descent. Instead of considering a single training example', 'mini-batch gradient descent computes the gradient based on a small batch of randomly selected examples. This allows for further reduction in computational cost compared to batch gradient descent', \"while also providing more stable updates compared to stochastic gradient descent.\\n\\nThese algorithms leverage the principle of estimating the expected value of the cost function using a subset of the terms to efficiently optimize the model parameters. They are particularly effective for large-scale problems where computing the full objective function is impractical or too time-consuming.\\n\\nIt's worth noting that the choice of the subset whether it's a single training example\", 'a mini-batch', 'or some other sampling strategy and the size of the subset can vary depending on the specific optimization algorithm and the characteristics of the problem at hand.\\n\\nIn summary', 'optimization algorithms in machine learning often estimate the expected value of the cost function using a subset of the terms. Stochastic gradient descent', 'mini-batch gradient descent', 'and other variants are designed to efficiently update the model parameters based on such estimates', 'enabling effective optimization even with large-scale datasets.']\n",
      "entity_token: [tensor([ 7649,  3053,   119,  1188,  1911,  1110,  3337,  1215,  1107, 25161,\n",
      "        14975,  1111,  3395,  3776]), tensor([ 2108,  1107,  1103,  5618,  1104,  1415,   118,  3418,  2233, 27948,\n",
      "         1105,  1996,  3776,  3584,   119,  1109,  2637,  2860,  1104,  1103,\n",
      "         2616,  3053,  1110,  3555,  1118,  6103,  1178,   170, 18005,  1104,\n",
      "         1103,  2538,  1104,  1103,  7649,  3053,   119,  1188, 18005,  1110,\n",
      "        19729, 20744,  1121,  1103,  2072,  2233,  9388]), tensor([ 1134,  6618,  1106,  4851,  1103, 19903, 11904,  1105,  2420,  1146,\n",
      "         1103, 25161,  1965,   119,  1247,  1132,  1672, 25161, 14975,  1115,\n",
      "        17573,  1142,  3400,   119,  1448,  1218,   118,  1227,  9932,  1110,\n",
      "          188,  2430,  7147,  5668, 19848,  6585,   156,  2349,  2137]), tensor([ 1134, 15549,  1103,  2235, 11934,  1359,  1113,  1103, 19848,  3555,\n",
      "         1121,   170, 19729,  2700,  2013,  1859,   119,  1109, 19351,  1110,\n",
      "        17325,  1181,  1118,  1781,  1103,  1903,  1104,  1103, 19848,  1116,\n",
      "         3254, 18505,  1121,  2967,  1122, 17166,  1116,  1104,  1146, 23562,\n",
      "         1103,  2235, 11934,  1606,  1472,  2013,  5136,   119,  2543,  8120,\n",
      "         1110,  1270,  8715,   118, 15817, 19848,  6585]), tensor([ 1134,  1110,   170, 13018,  1206, 15817, 19848,  6585,  1105,   188,\n",
      "         2430,  7147,  5668, 19848,  6585,   119,  3743,  1104,  6103,   170,\n",
      "         1423,  2013,  1859]), tensor([ 8715,   118, 15817, 19848,  6585,  3254, 22662,  1116,  1103, 19848,\n",
      "         1359,  1113,   170,  1353, 15817,  1104, 19729,  2700,  5136,   119,\n",
      "         1188,  3643,  1111,  1748,  7234,  1107, 19903,  2616,  3402,  1106,\n",
      "        15817, 19848,  6585]), tensor([ 1229,  1145,  3558,  1167,  6111, 15549,  3402,  1106,   188,  2430,\n",
      "         7147,  5668, 19848,  6585,   119,  1636, 14975, 24228,  1103,  6708,\n",
      "         1104, 12890, 27182,  1103,  2637,  2860,  1104,  1103,  2616,  3053,\n",
      "         1606,   170, 18005,  1104,  1103,  2538,  1106, 19723, 11769,  3121,\n",
      "        19092,  1103,  2235, 11934,   119,  1220,  1132,  2521,  3903,  1111,\n",
      "         1415,   118,  3418,  2645,  1187, 12783,  1103,  1554,  7649,  3053,\n",
      "         1110, 24034, 18890,  1137,  1315,  1159,   118, 16114,   119,  1135,\n",
      "          112,   188,  3869,  9095,  1115,  1103,  3026,  1104,  1103, 18005,\n",
      "         2480,  1122,   112,   188,   170,  1423,  2013,  1859]), tensor([  170,  8715,   118, 15817]), tensor([ 1137,  1199,  1168, 18200,  5564,  1105,  1103,  2060,  1104,  1103,\n",
      "        18005,  1169,  7907,  5763,  1113,  1103,  2747, 25161,  9932,  1105,\n",
      "         1103,  5924,  1104,  1103,  2463,  1120,  1289,   119,  1130, 14940]), tensor([25161, 14975,  1107,  3395,  3776,  1510, 10301,  1103,  2637,  2860,\n",
      "         1104,  1103,  2616,  3053,  1606,   170, 18005,  1104,  1103,  2538,\n",
      "          119,  1457,  9962, 25066, 19848,  6585]), tensor([ 8715,   118, 15817, 19848,  6585]), tensor([ 1105,  1168, 10317,  1132,  2011,  1106, 19723, 11984,  1103,  2235,\n",
      "        11934,  1359,  1113,  1216, 10777]), tensor([12619,  3903, 25161,  1256,  1114,  1415,   118,  3418,  2233, 27948,\n",
      "          119])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the terms of the full cost function. For example, maximum likelihood estimation problems, when\n",
      "content_token: tensor([  101,  1104,  1103,  2538,  1104,  1103,  1554,  2616,  3053,   119,\n",
      "         1370,  1859,   117,  4177, 17843, 12890, 21517,  2645,   117,  1165,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: problems, when viewed in log space, decompose into a sum over each example: m θ = argmax logp\n",
      "content_token: tensor([  101,  2645,   117,  1165,  6497,  1107,  9366,  2000,   117,  1260,\n",
      "         8178, 14811,  1154,   170,  7584,  1166,  1296,  1859,   131,   182,\n",
      "          425,   134,   170, 10805, 22871,  9366,  1643,   102])\n",
      "entity_list: ['maximum likelihood estimation']\n",
      "entity_token: [tensor([ 4177, 17843, 12890, 21517])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: m θ = argmax logp (x(i),y(i) ;θ). (8.4) ML model θ i=1  Maximizing this sum is equivalent to\n",
      "content_token: tensor([  101,   182,   425,   134,   170, 10805, 22871,  9366,  1643,   113,\n",
      "          193,   113,   178,   114,   117,   194,   113,   178,   114,   132,\n",
      "          425,   114,   119,   113,   129,   119,   125,   114,   150,  2162,\n",
      "         2235,   425,   178,   134,   122, 25217,  4404,  1142,  7584,  1110,\n",
      "         4976,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is equivalent to maximizing the expectation over the empirical distribution defined by the training\n",
      "content_token: tensor([  101,  1110,  4976,  1106, 12477,  8745, 25596,  1103, 19351,  1166,\n",
      "         1103, 20607,  3735,  3393,  1118,  1103,  2013,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by the training set: J(θ) = E logp (x,y;θ). (8.5) x,y pˆ model data ∼ Most of the properties of the\n",
      "content_token: tensor([ 101, 1118, 1103, 2013, 1383,  131,  147,  113,  425,  114,  134,  142,\n",
      "        9366, 1643,  113,  193,  117,  194,  132,  425,  114,  119,  113,  129,\n",
      "         119,  126,  114,  193,  117,  194,  100, 2235, 2233,  100, 2082, 1104,\n",
      "        1103, 4625, 1104, 1103,  102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: properties of the objective function J used by most of our opti- mization algorithms are also\n",
      "content_token: tensor([  101,  4625,  1104,  1103,  7649,  3053,   147,  1215,  1118,  1211,\n",
      "         1104,  1412, 11769,  3121,   118,  1940,  8569, 14975,  1132,  1145,\n",
      "          102])\n",
      "entity_list: ['objective function', 'optimization algorithms']\n",
      "entity_token: [tensor([7649, 3053]), tensor([25161, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithms are also expectations over the training set. For example, the 277 CHAPTER 8.\n",
      "content_token: tensor([  101, 14975,  1132,  1145, 11471,  1166,  1103,  2013,  1383,   119,\n",
      "         1370,  1859,   117,  1103,  1765,  1559,  8203,   129,   119,   102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the 277 CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS most commonly used property is the\n",
      "content_token: tensor([  101,  1103,  1765,  1559,  8203,   129,   119,   152,  2101, 21669,\n",
      "        14038,  5301, 13821, 24805,   143,  9565,   157,  9664, 11607, 15740,\n",
      "        18581, 16668,   150, 15609, 21678,  1708,  1211,  3337,  1215,  2400,\n",
      "         1110,  1103,   102])\n",
      "entity_list: ['OPTIMIZATION FOR TRAINING DEEP MODELS']\n",
      "entity_token: [tensor([  152,  2101, 21669, 14038,  5301, 13821, 24805,   143,  9565,   157,\n",
      "         9664, 11607, 15740, 18581, 16668,   150, 15609, 21678,  1708])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: property is the gradient: J(θ) = E logp (x,y;θ). (8.6) ∇θ x,y ∼pˆdata∇θ model Computing this\n",
      "content_token: tensor([  101,  2400,  1110,  1103, 19848,   131,   147,   113,   425,   114,\n",
      "          134,   142,  9366,  1643,   113,   193,   117,   194,   132,   425,\n",
      "          114,   119,   113,   129,   119,   127,   114,   100,   193,   117,\n",
      "          194,   100,  2235, 20463,  1142,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Computing this expectation exactly is very expensive because it requires evaluating the model on\n",
      "content_token: tensor([  101, 20463,  1142, 19351,  2839,  1110,  1304,  5865,  1272,  1122,\n",
      "         5315, 27698,  1103,  2235,  1113,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the model on every example in the entire dataset. In practice, we can compute these expectations by\n",
      "content_token: tensor([  101,  1103,  2235,  1113,  1451,  1859,  1107,  1103,  2072,  2233,\n",
      "         9388,   119,  1130,  2415,   117,  1195,  1169,  3254, 22662,  1292,\n",
      "        11471,  1118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: expectations by randomly sampling a small number of examples from the dataset, then taking the\n",
      "content_token: tensor([  101, 11471,  1118, 19729, 18200,   170,  1353,  1295,  1104,  5136,\n",
      "         1121,  1103,  2233,  9388,   117,  1173,  1781,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: then taking the average over only those examples. Recall that the standard error of the mean\n",
      "content_token: tensor([  101,  1173,  1781,  1103,  1903,  1166,  1178,  1343,  5136,   119,\n",
      "        11336,  7867,  1233,  1115,  1103,  2530,  7353,  1104,  1103,  1928,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: error of the mean (equation 5.46) estimated from n samples is given by σ/√n, where σ is the true\n",
      "content_token: tensor([ 101, 7353, 1104, 1103, 1928,  113, 8381,  126,  119, 3993,  114, 3555,\n",
      "        1121,  183, 8025, 1110, 1549, 1118,  436,  120,  854, 1179,  117, 1187,\n",
      "         436, 1110, 1103, 2276,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: where σ is the true standard deviation of the value of the samples. The denominator of √n shows\n",
      "content_token: tensor([  101,  1187,   436,  1110,  1103,  2276,  2530,  1260, 27444,  1104,\n",
      "         1103,  2860,  1104,  1103,  8025,   119,  1109, 10552, 18882, 24226,\n",
      "         1766,  1104,   854,  1179,  2196,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of √n shows that there are less than linear returns to using more examples to estimate the\n",
      "content_token: tensor([  101,  1104,   854,  1179,  2196,  1115,  1175,  1132,  1750,  1190,\n",
      "         7378,  5166,  1106,  1606,  1167,  5136,  1106, 10301,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to estimate the gradient. Compare two hypothetical estimates of the gradient, one based on 100\n",
      "content_token: tensor([  101,  1106, 10301,  1103, 19848,   119,  3291,  8223,  8836,  1160,\n",
      "          177,  1183, 11439, 27861, 10777,  1104,  1103, 19848,   117,  1141,\n",
      "         1359,  1113,  1620,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: one based on 100 examples and another based on 10,000 examples. The latter requires 100 times more\n",
      "content_token: tensor([ 101, 1141, 1359, 1113, 1620, 5136, 1105, 1330, 1359, 1113, 1275,  117,\n",
      "        1288, 5136,  119, 1109, 2985, 5315, 1620, 1551, 1167,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 100 times more computation than the former, but reduces the standard error of the mean only by a\n",
      "content_token: tensor([  101,  1620,  1551,  1167,  3254, 19675,  1190,  1103,  1393,   117,\n",
      "         1133, 13822,  1103,  2530,  7353,  1104,  1103,  1928,  1178,  1118,\n",
      "          170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the mean only by a factor of 10. Most optimization algorithms converge much faster (in terms of\n",
      "content_token: tensor([  101,  1103,  1928,  1178,  1118,   170,  5318,  1104,  1275,   119,\n",
      "         2082, 25161, 14975, 14255,  4121,  2176,  1277,  4946,   113,  1107,\n",
      "         2538,  1104,   102])\n",
      "entity_list: ['optimization algorithms']\n",
      "entity_token: [tensor([25161, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: faster (in terms of total computation, not in terms of number of updates) if they are allowed to\n",
      "content_token: tensor([  101,  4946,   113,  1107,  2538,  1104,  1703,  3254, 19675,   117,\n",
      "         1136,  1107,  2538,  1104,  1295,  1104, 15549,   114,  1191,  1152,\n",
      "         1132,  2148,  1106,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: they are allowed to rapidly compute approximate estimates of the gradient rather than slowly\n",
      "content_token: tensor([  101,  1152,  1132,  2148,  1106,  5223,  3254, 22662, 17325, 10777,\n",
      "         1104,  1103, 19848,  1897,  1190,  2494,   102])\n",
      "entity_list: ['gradient']\n",
      "entity_token: [tensor([19848])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: rather than slowly computing the exact gradient. Another consideration motivating statistical\n",
      "content_token: tensor([  101,  1897,  1190,  2494, 12783,  1103,  6129, 19848,   119,  2543,\n",
      "         9486,   182,  3329, 19012, 11435,   102])\n",
      "entity_list: ['exact gradient']\n",
      "entity_token: [tensor([ 6129, 19848])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: statistical estimation of the gradient from a small number of samples is redundancy in the training\n",
      "content_token: tensor([  101, 11435, 12890, 21517,  1104,  1103, 19848,  1121,   170,  1353,\n",
      "         1295,  1104,  8025,  1110,  1894, 22902,  7232,  1107,  1103,  2013,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in the training set. In the worst case, all m samples in the training set could be identical copies\n",
      "content_token: tensor([ 101, 1107, 1103, 2013, 1383,  119, 1130, 1103, 4997, 1692,  117, 1155,\n",
      "         182, 8025, 1107, 1103, 2013, 1383, 1180, 1129, 6742, 4034,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: be identical copies of each other. A sampling- based estimate of the gradient could compute the\n",
      "content_token: tensor([  101,  1129,  6742,  4034,  1104,  1296,  1168,   119,   138, 18200,\n",
      "          118,  1359, 10301,  1104,  1103, 19848,  1180,  3254, 22662,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: could compute the correct gradient with a single sample, using m times less computation than the\n",
      "content_token: tensor([  101,  1180,  3254, 22662,  1103,  5663, 19848,  1114,   170,  1423,\n",
      "         6876,   117,  1606,   182,  1551,  1750,  3254, 19675,  1190,  1103,\n",
      "          102])\n",
      "entity_list: ['sampling-based estimate']\n",
      "entity_token: [tensor([18200,   118,  1359, 10301])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: than the naive approach. In practice, we are unlikely to truly encounter this worst-case situation,\n",
      "content_token: tensor([  101,  1190,  1103, 22607,  3136,   119,  1130,  2415,   117,  1195,\n",
      "         1132,  9803,  1106,  5098,  8107,  1142,  4997,   118,  1692,  2820,\n",
      "          117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: situation, but we may find large numbers of examples that all make very similar contributions to\n",
      "content_token: tensor([ 101, 2820,  117, 1133, 1195, 1336, 1525, 1415, 2849, 1104, 5136, 1115,\n",
      "        1155, 1294, 1304, 1861, 5353, 1106,  102])\n",
      "entity_list: ['large numbers of examples']\n",
      "entity_token: [tensor([1415, 2849, 1104, 5136])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: contributions to the gradient. Optimization algorithms that use the entire training set are called\n",
      "content_token: tensor([  101,  5353,  1106,  1103, 19848,   119,  9126,  3121,  3080,  8569,\n",
      "        14975,  1115,  1329,  1103,  2072,  2013,  1383,  1132,  1270,   102])\n",
      "entity_list: ['optimization algorithms']\n",
      "entity_token: [tensor([25161, 14975])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: set are called batch or deterministic gradient methods, because they process all of the training\n",
      "content_token: tensor([  101,  1383,  1132,  1270, 15817,  1137,  1260,  2083, 25685,  5668,\n",
      "        19848,  4069,   117,  1272,  1152,  1965,  1155,  1104,  1103,  2013,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: all of the training examples simultaneously in a large batch. This terminology can be somewhat\n",
      "content_token: tensor([  101,  1155,  1104,  1103,  2013,  5136,  7344,  1107,   170,  1415,\n",
      "        15817,   119,  1188, 20925,  1169,  1129,  4742,   102])\n",
      "entity_list: ['deterministic gradient methods']\n",
      "entity_token: [tensor([ 1260,  2083, 25685,  5668, 19848,  4069])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can be somewhat confusing because the word “batch” is also often used to describe the minibatch\n",
      "content_token: tensor([  101,  1169,  1129,  4742, 18110,  1272,  1103,  1937,   789, 15817,\n",
      "          790,  1110,  1145,  1510,  1215,  1106,  5594,  1103,  8715, 14602,\n",
      "         1732,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the minibatch used by minibatch stochastic gradient descent. Typically the term “batch gradient\n",
      "content_token: tensor([  101,  1103,  8715, 14602,  1732,  1215,  1118,  8715, 14602,  1732,\n",
      "          188,  2430,  7147,  5668, 19848,  6585,   119, 16304,  1103,  1858,\n",
      "          789, 15817, 19848,   102])\n",
      "entity_list: ['minibatch stochastic gradient descent']\n",
      "entity_token: [tensor([ 8715, 14602,  1732,   188,  2430,  7147,  5668, 19848,  6585])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: “batch gradient descent” implies the use of the full training set, while the use of the term\n",
      "content_token: tensor([  101,   789, 15817, 19848,  6585,   790, 12942,  1103,  1329,  1104,\n",
      "         1103,  1554,  2013,  1383,   117,  1229,  1103,  1329,  1104,  1103,\n",
      "         1858,   102])\n",
      "entity_list: ['batch gradient descent']\n",
      "entity_token: [tensor([15817, 19848,  6585])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the use of the term “batch” to describe a group of examples does not. For example, it is very\n",
      "content_token: tensor([  101,  1103,  1329,  1104,  1103,  1858,   789, 15817,   790,  1106,\n",
      "         5594,   170,  1372,  1104,  5136,  1674,  1136,   119,  1370,  1859,\n",
      "          117,  1122,  1110,  1304,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: example, it is very common to use the term “batch size” to describe the size of a minibatch.\n",
      "content_token: tensor([  101,  1859,   117,  1122,  1110,  1304,  1887,  1106,  1329,  1103,\n",
      "         1858,   789, 15817,  2060,   790,  1106,  5594,  1103,  2060,  1104,\n",
      "          170,  8715, 14602,  1732,   119,   102])\n",
      "entity_list: ['batch size', 'minibatch']\n",
      "entity_token: [tensor([15817,  2060]), tensor([ 8715, 14602,  1732])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of a minibatch. Optimization algorithms that use only a single example at a time are sometimes\n",
      "content_token: tensor([  101,  1104,   170,  8715, 14602,  1732,   119,  9126,  3121,  3080,\n",
      "         8569, 14975,  1115,  1329,  1178,   170,  1423,  1859,  1120,   170,\n",
      "         1159,  1132,  2121,   102])\n",
      "entity_list: ['minibatch', 'Optimization algorithms']\n",
      "entity_token: [tensor([ 8715, 14602,  1732]), tensor([ 9126,  3121,  3080,  8569, 14975])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: time are sometimes called stochastic or sometimes online methods. The term online is usually\n",
      "content_token: tensor([ 101, 1159, 1132, 2121, 1270,  188, 2430, 7147, 5668, 1137, 2121, 3294,\n",
      "        4069,  119, 1109, 1858, 3294, 1110, 1932,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: online is usually reserved for the case where the examples are drawn from a stream of continually\n",
      "content_token: tensor([  101,  3294,  1110,  1932,  9142,  1111,  1103,  1692,  1187,  1103,\n",
      "         5136,  1132,  3795,  1121,   170,  5118,  1104, 15395,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of continually created examples rather than from a fixed-size training set over which several\n",
      "content_token: tensor([  101,  1104, 15395,  1687,  5136,  1897,  1190,  1121,   170,  4275,\n",
      "          118,  2060,  2013,  1383,  1166,  1134,  1317,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: over which several passes are made. Most algorithms used for deep learning fall somewhere in\n",
      "content_token: tensor([  101,  1166,  1134,  1317,  4488,  1132,  1189,   119,  2082, 14975,\n",
      "         1215,  1111,  1996,  3776,  2303,  4476,  1107,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: fall somewhere in between, using more 278 CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS than one\n",
      "content_token: tensor([  101,  2303,  4476,  1107,  1206,   117,  1606,  1167, 27832,  8203,\n",
      "          129,   119,   152,  2101, 21669, 14038,  5301, 13821, 24805,   143,\n",
      "         9565,   157,  9664, 11607, 15740, 18581, 16668,   150, 15609, 21678,\n",
      "         1708,  1190,  1141,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: MODELS than one but less than all of the training examples. These were traditionally called\n",
      "content_token: tensor([  101,   150, 15609, 21678,  1708,  1190,  1141,  1133,  1750,  1190,\n",
      "         1155,  1104,  1103,  2013,  5136,   119,  1636,  1127,  7440,  1270,\n",
      "          102])\n",
      "entity_list: ['deep models']\n",
      "entity_token: [tensor([1996, 3584])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: called minibatch or minibatch stochastic methods and it is now common to simply call them\n",
      "content_token: tensor([  101,  1270,  8715, 14602,  1732,  1137,  8715, 14602,  1732,   188,\n",
      "         2430,  7147,  5668,  4069,  1105,  1122,  1110,  1208,  1887,  1106,\n",
      "         2566,  1840,  1172,   102])\n",
      "entity_list: ['minibatch']\n",
      "entity_token: [tensor([ 8715, 14602,  1732])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to simply call them stochastic methods. The canonical example of a stochastic method is stochastic\n",
      "content_token: tensor([  101,  1106,  2566,  1840,  1172,   188,  2430,  7147,  5668,  4069,\n",
      "          119,  1109, 21768,  1859,  1104,   170,   188,  2430,  7147,  5668,\n",
      "         3442,  1110,   188,  2430,  7147,  5668,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is stochastic gradient descent, presented in detail in section 8.3.1. Minibatch sizes are generally\n",
      "content_token: tensor([  101,  1110,   188,  2430,  7147,  5668, 19848,  6585,   117,  2756,\n",
      "         1107,  6505,  1107,  2237,   129,   119,   124,   119,   122,   119,\n",
      "        14393, 14602,  1732, 10855,  1132,  2412,   102])\n",
      "entity_list: ['stochastic gradient descent']\n",
      "entity_token: [tensor([  188,  2430,  7147,  5668, 19848,  6585])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sizes are generally driven by the following factors: Larger batches provide a more accurate\n",
      "content_token: tensor([  101, 10855,  1132,  2412,  4940,  1118,  1103,  1378,  5320,   131,\n",
      "        10236,  1197, 15817,  1279,  2194,   170,  1167,  8026,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a more accurate estimate of the gradient, but with • less than linear returns. Multicore\n",
      "content_token: tensor([  101,   170,  1167,  8026, 10301,  1104,  1103, 19848,   117,  1133,\n",
      "         1114,   794,  1750,  1190,  7378,  5166,   119, 18447,  9475,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: returns. Multicore architectures are usually underutilized by extremely small batches. • This\n",
      "content_token: tensor([  101,  5166,   119, 18447,  9475,  4220,  1116,  1132,  1932,  1223,\n",
      "        16065,  2646,  5305,  1118,  4450,  1353, 15817,  1279,   119,   794,\n",
      "         1188,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: batches. • This motivates using some absolute minimum batch size, below which there is no reduction\n",
      "content_token: tensor([  101, 15817,  1279,   119,   794,  1188,   182,  3329, 18851,  1116,\n",
      "         1606,  1199,  7846,  5867, 15817,  2060,   117,  2071,  1134,  1175,\n",
      "         1110,  1185,  7234,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is no reduction in the time to process a minibatch. If all examples in the batch are to be\n",
      "content_token: tensor([  101,  1110,  1185,  7234,  1107,  1103,  1159,  1106,  1965,   170,\n",
      "         8715, 14602,  1732,   119,  1409,  1155,  5136,  1107,  1103, 15817,\n",
      "         1132,  1106,  1129,   102])\n",
      "entity_list: ['minibatch']\n",
      "entity_token: [tensor([ 8715, 14602,  1732])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the batch are to be processed in parallel (as is typically • the case), then the amount of memory\n",
      "content_token: tensor([  101,  1103, 15817,  1132,  1106,  1129, 14659,  1107,  5504,   113,\n",
      "         1112,  1110,  3417,   794,  1103,  1692,   114,   117,  1173,  1103,\n",
      "         2971,  1104,  2962,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: amount of memory scales with the batch size. For many hardware setups this is the limiting factor\n",
      "content_token: tensor([  101,  2971,  1104,  2962,  9777,  1114,  1103, 15817,  2060,   119,\n",
      "         1370,  1242,  8172, 18011,  1116,  1142,  1110,  1103, 15816,  5318,\n",
      "          102])\n",
      "entity_list: ['batch size']\n",
      "entity_token: [tensor([15817,  2060])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the limiting factor in batch size. Some kinds of hardware achieve better runtime with specific\n",
      "content_token: tensor([  101,  1103, 15816,  5318,  1107, 15817,  2060,   119,  1789,  7553,\n",
      "         1104,  8172,  5515,  1618,  1576,  4974,  1114,  2747,   102])\n",
      "entity_list: ['batch size']\n",
      "entity_token: [tensor([15817,  2060])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with specific sizes of arrays. • Especially when using GPUs, it is common for power of 2 batch\n",
      "content_token: tensor([  101,  1114,  2747, 10855,  1104,  9245,  1116,   119,   794, 11008,\n",
      "         1165,  1606, 15175,  2591,  1116,   117,  1122,  1110,  1887,  1111,\n",
      "         1540,  1104,   123, 15817,   102])\n",
      "entity_list: ['GPUs', 'batch']\n",
      "entity_token: [tensor([15175,  2591,  1116]), tensor([15817])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: power of 2 batch sizes to offer better runtime. Typical power of 2 batch sizes range from 32 to\n",
      "content_token: tensor([  101,  1540,  1104,   123, 15817, 10855,  1106,  2906,  1618,  1576,\n",
      "         4974,   119, 23125,  1540,  1104,   123, 15817, 10855,  2079,  1121,\n",
      "         2724,  1106,   102])\n",
      "entity_list: ['batch sizes']\n",
      "entity_token: [tensor([15817, 10855])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: range from 32 to 256, with 16 sometimes being attempted for large models. Small batches can offer a\n",
      "content_token: tensor([  101,  2079,  1121,  2724,  1106, 18440,   117,  1114,  1479,  2121,\n",
      "         1217,  3867,  1111,  1415,  3584,   119,  6844, 15817,  1279,  1169,\n",
      "         2906,   170,   102])\n",
      "entity_list: ['large models']\n",
      "entity_token: [tensor([1415, 3584])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: batches can offer a regularizing effect (Wilson and Martinez, 2003), • perhaps due to the noise\n",
      "content_token: tensor([  101, 15817,  1279,  1169,  2906,   170,  2366,  4404,  2629,   113,\n",
      "         3425,  1105, 16247,   117,  1581,   114,   117,   794,  3229,  1496,\n",
      "         1106,  1103,  4647,   102])\n",
      "entity_list: ['regularizing effect']\n",
      "entity_token: [tensor([2366, 4404, 2629])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: due to the noise they add to the learning process. Generalization error is often best for a batch\n",
      "content_token: tensor([  101,  1496,  1106,  1103,  4647,  1152,  5194,  1106,  1103,  3776,\n",
      "         1965,   119,  1615,  2734,  7353,  1110,  1510,  1436,  1111,   170,\n",
      "        15817,   102])\n",
      "entity_list: ['Generalization error']\n",
      "entity_token: [tensor([1615, 2734, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: best for a batch size of 1. Training with such a small batch size might require a small learning\n",
      "content_token: tensor([  101,  1436,  1111,   170, 15817,  2060,  1104,   122,   119,  5513,\n",
      "         1114,  1216,   170,  1353, 15817,  2060,  1547,  4752,   170,  1353,\n",
      "         3776,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a small learning rate to maintain stability due to the high variance in the estimate of the\n",
      "content_token: tensor([  101,   170,  1353,  3776,  2603,  1106,  4731,  9397,  1496,  1106,\n",
      "         1103,  1344, 26717,  1107,  1103, 10301,  1104,  1103,   102])\n",
      "entity_list: ['learning rate']\n",
      "entity_token: [tensor([3776, 2603])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the estimate of the gradient. The total runtime can be very high due to the need to make more\n",
      "content_token: tensor([  101,  1103, 10301,  1104,  1103, 19848,   119,  1109,  1703,  1576,\n",
      "         4974,  1169,  1129,  1304,  1344,  1496,  1106,  1103,  1444,  1106,\n",
      "         1294,  1167,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: need to make more steps, both because of the reduced learning rate and because it takes more steps\n",
      "content_token: tensor([ 101, 1444, 1106, 1294, 1167, 3343,  117, 1241, 1272, 1104, 1103, 3549,\n",
      "        3776, 2603, 1105, 1272, 1122, 2274, 1167, 3343,  102])\n",
      "entity_list: ['learning rate']\n",
      "entity_token: [tensor([3776, 2603])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it takes more steps to observe the entire training set. Different kinds of algorithms use different\n",
      "content_token: tensor([  101,  1122,  2274,  1167,  3343,  1106, 12326,  1103,  2072,  2013,\n",
      "         1383,   119, 14380,  7553,  1104, 14975,  1329,  1472,   102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: use different kinds of information from the mini- batch in different ways. Some algorithms are more\n",
      "content_token: tensor([  101,  1329,  1472,  7553,  1104,  1869,  1121,  1103,  8715,   118,\n",
      "        15817,  1107,  1472,  3242,   119,  1789, 14975,  1132,  1167,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: algorithms are more sensitive to sampling error than others, either because they use information\n",
      "content_token: tensor([  101, 14975,  1132,  1167,  7246,  1106, 18200,  7353,  1190,  1639,\n",
      "          117,  1719,  1272,  1152,  1329,  1869,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: use information that is difficult to estimate accurately with few samples, or because they use\n",
      "content_token: tensor([  101,  1329,  1869,  1115,  1110,  2846,  1106, 10301, 14702,  1114,\n",
      "         1374,  8025,   117,  1137,  1272,  1152,  1329,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: or because they use information in ways that amplify sampling errors more. Methods that compute\n",
      "content_token: tensor([  101,  1137,  1272,  1152,  1329,  1869,  1107,  3242,  1115,  1821,\n",
      "         1643, 22881, 18200, 11122,  1167,   119, 20569,  1116,  1115,  3254,\n",
      "        22662,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that compute updates based only on the gradient g are usually relatively robust and can handle\n",
      "content_token: tensor([  101,  1115,  3254, 22662, 15549,  1359,  1178,  1113,  1103, 19848,\n",
      "          176,  1132,  1932,  3860, 17351,  1105,  1169,  4282,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: and can handle smaller batch sizes like 100. Second-order methods, which use also the Hessian\n",
      "content_token: tensor([  101,  1105,  1169,  4282,  2964, 15817, 10855,  1176,  1620,   119,\n",
      "         2307,   118,  1546,  4069,   117,  1134,  1329,  1145,  1103, 26349,\n",
      "         1811,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: also the Hessian matrix H and compute updates such as H 1g, typically require much larger batch\n",
      "content_token: tensor([  101,  1145,  1103, 26349,  1811,  8952,   145,  1105,  3254, 22662,\n",
      "        15549,  1216,  1112,   145,   122,  1403,   117,  3417,  4752,  1277,\n",
      "         2610, 15817,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: much larger batch sizes like 10,000. These large batch − sizes are required to minimize\n",
      "content_token: tensor([  101,  1277,  2610, 15817, 10855,  1176,  1275,   117,  1288,   119,\n",
      "         1636,  1415, 15817,   851, 10855,  1132,  2320,  1106, 20220,   102])\n",
      "entity_list: ['large batch sizes']\n",
      "entity_token: [tensor([ 1415, 15817, 10855])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to minimize fluctuations in the estimates of H 1g. Suppose − that H is estimated perfectly but has\n",
      "content_token: tensor([  101,  1106, 20220, 23896,  5822, 24176,  1107,  1103, 10777,  1104,\n",
      "          145,   122,  1403,   119, 15463,  8661,  6787,   851,  1115,   145,\n",
      "         1110,  3555,  6150,  1133,  1144,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: perfectly but has a poor condition number. Multiplication by 279 CHAPTER 8. OPTIMIZATION FOR\n",
      "content_token: tensor([  101,  6150,  1133,  1144,   170,  2869,  3879,  1295,   119, 18447,\n",
      "        15534,  1118,  1765,  1580,  8203,   129,   119,   152,  2101, 21669,\n",
      "        14038,  5301, 13821, 24805,   143,  9565,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 8. OPTIMIZATION FOR TRAINING DEEP MODELS H or its inverse amplifies pre-existing errors, in this\n",
      "content_token: tensor([  101,   129,   119,   152,  2101, 21669, 14038,  5301, 13821, 24805,\n",
      "          143,  9565,   157,  9664, 11607, 15740, 18581, 16668,   150, 15609,\n",
      "        21678,  1708,   145,  1137,  1157, 22127,  1821,  1643,  2646, 16847,\n",
      "         3073,   118,  3685, 11122,   117,  1107,  1142,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: errors, in this case, estimation errors in g. Very small changes in the estimate of g can thus\n",
      "content_token: tensor([  101, 11122,   117,  1107,  1142,  1692,   117, 12890, 21517, 11122,\n",
      "         1107,   176,   119,  6424,  1353,  2607,  1107,  1103, 10301,  1104,\n",
      "          176,  1169,  2456,   102])\n",
      "entity_list: [\"'training'\", \"'deep models'\"]\n",
      "entity_token: [tensor([ 112, 2013,  112]), tensor([ 112, 1996, 3584,  112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of g can thus cause large changes in the update H 1g, even if H were estimated perfectly. Of\n",
      "content_token: tensor([  101,  1104,   176,  1169,  2456,  2612,  1415,  2607,  1107,  1103,\n",
      "        11984,   145,   122,  1403,   117,  1256,  1191,   145,  1127,  3555,\n",
      "         6150,   119,  2096,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: perfectly. Of course, H will be estimated only − approximately, so the update H 1g will contain\n",
      "content_token: tensor([  101,  6150,   119,  2096,  1736,   117,   145,  1209,  1129,  3555,\n",
      "         1178,   851,  2324,   117,  1177,  1103, 11984,   145,   122,  1403,\n",
      "         1209,  4651,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: H 1g will contain even more error than we would − predict from applying a poorly conditioned\n",
      "content_token: tensor([  101,   145,   122,  1403,  1209,  4651,  1256,  1167,  7353,  1190,\n",
      "         1195,  1156,   851, 17163,  1121, 11892,   170,  9874, 25592,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: poorly conditioned operation to the estimate of g. It is also crucial that the minibatches be\n",
      "content_token: tensor([  101,  9874, 25592,  2805,  1106,  1103, 10301,  1104,   176,   119,\n",
      "         1135,  1110,  1145, 10268,  1115,  1103,  8715, 14602,  7486,  1129,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the minibatches be selected randomly. Computing an unbiased estimate of the expected gradient from\n",
      "content_token: tensor([  101,  1103,  8715, 14602,  7486,  1129,  2700, 19729,   119, 20463,\n",
      "         1126,  8362, 10242,  5591, 10301,  1104,  1103,  2637, 19848,  1121,\n",
      "          102])\n",
      "entity_list: ['minibatches']\n",
      "entity_token: [tensor([ 8715, 14602,  7486])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: gradient from a set of samples requires that those samples be independent. We also wish for two\n",
      "content_token: tensor([  101, 19848,  1121,   170,  1383,  1104,  8025,  5315,  1115,  1343,\n",
      "         8025,  1129,  2457,   119,  1284,  1145,  3683,  1111,  1160,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: also wish for two subsequent gradient estimates to be independent from each other, so two\n",
      "content_token: tensor([  101,  1145,  3683,  1111,  1160,  4194, 19848, 10777,  1106,  1129,\n",
      "         2457,  1121,  1296,  1168,   117,  1177,  1160,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: each other, so two subsequent minibatches of examples should also be independent from each other.\n",
      "content_token: tensor([  101,  1296,  1168,   117,  1177,  1160,  4194,  8715, 14602,  7486,\n",
      "         1104,  5136,  1431,  1145,  1129,  2457,  1121,  1296,  1168,   119,\n",
      "          102])\n",
      "entity_list: ['minibatches']\n",
      "entity_token: [tensor([ 8715, 14602,  7486])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from each other. Many datasets are most naturally arranged in a way where successive examples are\n",
      "content_token: tensor([  101,  1121,  1296,  1168,   119,  2408,  2233, 27948,  1132,  1211,\n",
      "         8534,  4768,  1107,   170,  1236,  1187, 11598,  5136,  1132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: examples are highly correlated. For example, we might have a dataset of medical data with a long\n",
      "content_token: tensor([  101,  5136,  1132,  3023, 27053,   119,  1370,  1859,   117,  1195,\n",
      "         1547,  1138,   170,  2233,  9388,  1104,  2657,  2233,  1114,   170,\n",
      "         1263,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: data with a long list of blood sample test results. This list might be arranged so that first we\n",
      "content_token: tensor([ 101, 2233, 1114,  170, 1263, 2190, 1104, 1892, 6876, 2774, 2686,  119,\n",
      "        1188, 2190, 1547, 1129, 4768, 1177, 1115, 1148, 1195,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: so that first we have five blood samples taken at different times from the first patient, then we\n",
      "content_token: tensor([ 101, 1177, 1115, 1148, 1195, 1138, 1421, 1892, 8025, 1678, 1120, 1472,\n",
      "        1551, 1121, 1103, 1148, 5351,  117, 1173, 1195,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: patient, then we have three blood samples taken from the second patient, then the blood samples\n",
      "content_token: tensor([ 101, 5351,  117, 1173, 1195, 1138, 1210, 1892, 8025, 1678, 1121, 1103,\n",
      "        1248, 5351,  117, 1173, 1103, 1892, 8025,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the blood samples from the third patient, and so on. If we were to draw examples in order from this\n",
      "content_token: tensor([ 101, 1103, 1892, 8025, 1121, 1103, 1503, 5351,  117, 1105, 1177, 1113,\n",
      "         119, 1409, 1195, 1127, 1106, 3282, 5136, 1107, 1546, 1121, 1142,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in order from this list, then each of our minibatches would be extremely biased, because it would\n",
      "content_token: tensor([  101,  1107,  1546,  1121,  1142,  2190,   117,  1173,  1296,  1104,\n",
      "         1412,  8715, 14602,  7486,  1156,  1129,  4450, 15069,  1174,   117,\n",
      "         1272,  1122,  1156,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: because it would represent primarily one patient out of the many patients in the dataset. In cases\n",
      "content_token: tensor([ 101, 1272, 1122, 1156, 4248, 3120, 1141, 5351, 1149, 1104, 1103, 1242,\n",
      "        4420, 1107, 1103, 2233, 9388,  119, 1130, 2740,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: dataset. In cases such as these where the order of the dataset holds some significance, it is\n",
      "content_token: tensor([ 101, 2233, 9388,  119, 1130, 2740, 1216, 1112, 1292, 1187, 1103, 1546,\n",
      "        1104, 1103, 2233, 9388, 3486, 1199, 7467,  117, 1122, 1110,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: significance, it is necessary to shuffle the examples before selecting minibatches. For very large\n",
      "content_token: tensor([  101,  7467,   117,  1122,  1110,  3238,  1106,   188,  6583, 13327,\n",
      "         1103,  5136,  1196, 19752,  8715, 14602,  7486,   119,  1370,  1304,\n",
      "         1415,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: For very large datasets, for example datasets containing billions of examples in a data center, it\n",
      "content_token: tensor([  101,  1370,  1304,  1415,  2233, 27948,   117,  1111,  1859,  2233,\n",
      "        27948,  4051,  3775,  1116,  1104,  5136,  1107,   170,  2233,  2057,\n",
      "          117,  1122,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a data center, it can be impractical to sample examples truly uniformly at random every time we\n",
      "content_token: tensor([  101,   170,  2233,  2057,   117,  1122,  1169,  1129, 24034, 18890,\n",
      "         1106,  6876,  5136,  5098,  6029,  1193,  1120,  7091,  1451,  1159,\n",
      "         1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: every time we want to construct a minibatch. Fortunately, in practice it is usually sufficient to\n",
      "content_token: tensor([  101,  1451,  1159,  1195,  1328,  1106,  9417,   170,  8715, 14602,\n",
      "         1732,   119, 18101,   117,  1107,  2415,  1122,  1110,  1932,  6664,\n",
      "         1106,   102])\n",
      "entity_list: ['output: minibatch']\n",
      "entity_token: [tensor([ 5964,   131,  8715, 14602,  1732])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sufficient to shuffle the order of the dataset once and then store it in shuffled fashion. This\n",
      "content_token: tensor([  101,  6664,  1106,   188,  6583, 13327,  1103,  1546,  1104,  1103,\n",
      "         2233,  9388,  1517,  1105,  1173,  2984,  1122,  1107, 20390,  4633,\n",
      "          119,  1188,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: fashion. This will impose a fixed set of possible minibatches of consecutive examples that all\n",
      "content_token: tensor([  101,  4633,   119,  1188,  1209, 19103,   170,  4275,  1383,  1104,\n",
      "         1936,  8715, 14602,  7486,  1104,  4776,  5136,  1115,  1155,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: examples that all models trained thereafter will use, and each individual model will be forced to\n",
      "content_token: tensor([ 101, 5136, 1115, 1155, 3584, 3972, 7321, 1209, 1329,  117, 1105, 1296,\n",
      "        2510, 2235, 1209, 1129, 2257, 1106,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: will be forced to reuse this ordering every time it passes through the training data. However, this\n",
      "content_token: tensor([  101,  1209,  1129,  2257,  1106,  1231,  5613,  1142, 13649,  1451,\n",
      "         1159,  1122,  4488,  1194,  1103,  2013,  2233,   119,  1438,   117,\n",
      "         1142,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: data. However, this deviation from true random selection does not seem to have a significant\n",
      "content_token: tensor([  101,  2233,   119,  1438,   117,  1142,  1260, 27444,  1121,  2276,\n",
      "         7091,  4557,  1674,  1136,  3166,  1106,  1138,   170,  2418,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: have a significant detrimental effect. Failing to ever shuffle the examples in any way can\n",
      "content_token: tensor([  101,  1138,   170,  2418,  1260, 19091, 15595,  2629,   119,   143,\n",
      "        26753,  1106,  1518,   188,  6583, 13327,  1103,  5136,  1107,  1251,\n",
      "         1236,  1169,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in any way can seriously reduce the effectiveness of the algorithm. Many optimization problems in\n",
      "content_token: tensor([  101,  1107,  1251,  1236,  1169,  5536,  4851,  1103, 12949,  1104,\n",
      "         1103,  9932,   119,  2408, 25161,  2645,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: problems in machine learning decompose over examples well enough that we can compute entire\n",
      "content_token: tensor([  101,  2645,  1107,  3395,  3776,  1260,  8178, 14811,  1166,  5136,\n",
      "         1218,  1536,  1115,  1195,  1169,  3254, 22662,  2072,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can compute entire separate updates over different examples in parallel. In other words, we can\n",
      "content_token: tensor([  101,  1169,  3254, 22662,  2072,  2767, 15549,  1166,  1472,  5136,\n",
      "         1107,  5504,   119,  1130,  1168,  1734,   117,  1195,  1169,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: other words, we can compute the update that minimizes J(X) for one minibatch of examples X at the\n",
      "content_token: tensor([  101,  1168,  1734,   117,  1195,  1169,  3254, 22662,  1103, 11984,\n",
      "         1115, 20220,  1116,   147,   113,   161,   114,  1111,  1141,  8715,\n",
      "        14602,  1732,  1104,  5136,   161,  1120,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: examples X at the same time that we compute the update for several other minibatches. Such\n",
      "content_token: tensor([  101,  5136,   161,  1120,  1103,  1269,  1159,  1115,  1195,  3254,\n",
      "        22662,  1103, 11984,  1111,  1317,  1168,  8715, 14602,  7486,   119,\n",
      "         5723,   102])\n",
      "entity_list: ['minibatch']\n",
      "entity_token: [tensor([ 8715, 14602,  1732])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: minibatches. Such asynchronous parallel distributed approaches are discussed further in section\n",
      "content_token: tensor([  101,  8715, 14602,  7486,   119,  5723,  1112, 27250,  8167, 23038,\n",
      "         1361,  5504,  4901,  8015,  1132,  6352,  1748,  1107,  2237,   102])\n",
      "entity_list: ['asynchronous parallel distributed']\n",
      "entity_token: [tensor([ 1112, 27250,  8167, 23038,  1361,  5504,  4901])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: further in section 12.1.3. An interesting motivation for minibatch stochastic gradient descent is\n",
      "content_token: tensor([  101,  1748,  1107,  2237,  1367,   119,   122,   119,   124,   119,\n",
      "         1760,  5426, 15710,  1111,  8715, 14602,  1732,   188,  2430,  7147,\n",
      "         5668, 19848,  6585,  1110,   102])\n",
      "entity_list: ['such as GPUs', 'minibatch stochastic gradient descent also provides a noise injection mechanism that can help escape poor local minima during optimization.\\noutput: minibatch stochastic gradient descent\\n\\ninput: This noise injection is achieved by randomly shuffling the training examples and computing gradients on randomly selected minibatches.\\noutput: minibatch stochastic gradient descent']\n",
      "entity_token: [tensor([ 1216,  1112, 15175,  2591,  1116]), tensor([ 8715, 14602,  1732,   188,  2430,  7147,  5668, 19848,  6585,  1145,\n",
      "         2790,   170,  4647, 14546,  6978,  1115,  1169,  1494,  3359,  2869,\n",
      "         1469,  8715,  1918,  1219, 25161,   119,  5964,   131,  8715, 14602,\n",
      "         1732,   188,  2430,  7147,  5668, 19848,  6585,  7758,   131,  1188,\n",
      "         4647, 14546,  1110,  3890,  1118, 19729,   188,  6583, 20709,  1103,\n",
      "         2013,  5136,  1105, 12783, 19848,  1116,  1113, 19729,  2700,  8715,\n",
      "        14602,  7486,   119,  5964,   131,  8715, 14602,  1732,   188,  2430,\n",
      "         7147,  5668, 19848,  6585])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: gradient descent is that it follows the gradient of the true generalization error (equation 8.2) so\n",
      "content_token: tensor([  101, 19848,  6585,  1110,  1115,  1122,  3226,  1103, 19848,  1104,\n",
      "         1103,  2276,  1704,  2734,  7353,   113,  8381,   129,   119,   123,\n",
      "          114,  1177,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: (equation 8.2) so long as no examples are repeated. Most implementations of minibatch stochastic\n",
      "content_token: tensor([  101,   113,  8381,   129,   119,   123,   114,  1177,  1263,  1112,\n",
      "         1185,  5136,  1132,  4892,   119,  2082,  7249,  1116,  1104,  8715,\n",
      "        14602,  1732,   188,  2430,  7147,  5668,   102])\n",
      "entity_list: ['\\n\\ninput: gradient descent randomly shuffle the training examples to ensure that no examples are repeated within a minibatch.\\noutput: minibatch stochastic gradient descent']\n",
      "entity_token: [tensor([ 7758,   131, 19848,  6585, 19729,   188,  6583, 13327,  1103,  2013,\n",
      "         5136,  1106,  4989,  1115,  1185,  5136,  1132,  4892,  1439,   170,\n",
      "         8715, 14602,  1732,   119,  5964,   131,  8715, 14602,  1732,   188,\n",
      "         2430,  7147,  5668, 19848,  6585])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: stochastic gradient 280 CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS descent shuffle the\n",
      "content_token: tensor([  101,   188,  2430,  7147,  5668, 19848, 13760,  8203,   129,   119,\n",
      "          152,  2101, 21669, 14038,  5301, 13821, 24805,   143,  9565,   157,\n",
      "         9664, 11607, 15740, 18581, 16668,   150, 15609, 21678,  1708,  6585,\n",
      "          188,  6583, 13327,  1103,   102])\n",
      "entity_list: ['meaning that the gradients are computed and updated for each individual example in the training set.\\noutput: stochastic gradient descent']\n",
      "entity_token: [tensor([ 2764,  1115,  1103, 19848,  1116,  1132,  3254, 18505,  1105,  8054,\n",
      "         1111,  1296,  2510,  1859,  1107,  1103,  2013,  1383,   119,  5964,\n",
      "          131,   188,  2430,  7147,  5668, 19848,  6585])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: descent shuffle the dataset once and then pass through it multiple times. On the first pass, each\n",
      "content_token: tensor([  101,  6585,   188,  6583, 13327,  1103,  2233,  9388,  1517,  1105,\n",
      "         1173,  2789,  1194,  1122,  2967,  1551,   119,  1212,  1103,  1148,\n",
      "         2789,   117,  1296,   102])\n",
      "entity_list: ['the order remains fixed.\\noutput: stochastic gradient descent']\n",
      "entity_token: [tensor([ 1103,  1546,  2606,  4275,   119,  5964,   131,   188,  2430,  7147,\n",
      "         5668, 19848,  6585])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: first pass, each minibatch is used to compute an unbiased estimate of the true generalization\n",
      "content_token: tensor([  101,  1148,  2789,   117,  1296,  8715, 14602,  1732,  1110,  1215,\n",
      "         1106,  3254, 22662,  1126,  8362, 10242,  5591, 10301,  1104,  1103,\n",
      "         2276,  1704,  2734,   102])\n",
      "entity_list: ['minibatch', 'generalization']\n",
      "entity_token: [tensor([ 8715, 14602,  1732]), tensor([1704, 2734])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: true generalization error. On the second pass, the estimate becomes biased because it is formed by\n",
      "content_token: tensor([  101,  2276,  1704,  2734,  7353,   119,  1212,  1103,  1248,  2789,\n",
      "          117,  1103, 10301,  3316, 15069,  1174,  1272,  1122,  1110,  1824,\n",
      "         1118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: it is formed by re-sampling values that have already been used, rather than obtaining new fair\n",
      "content_token: tensor([  101,  1122,  1110,  1824,  1118,  1231,   118, 18200,  4718,  1115,\n",
      "         1138,  1640,  1151,  1215,   117,  1897,  1190, 11621,  1207,  4652,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: obtaining new fair samples from the data generating distribution. The fact that stochastic gradient\n",
      "content_token: tensor([  101, 11621,  1207,  4652,  8025,  1121,  1103,  2233, 12713,  3735,\n",
      "          119,  1109,  1864,  1115,   188,  2430,  7147,  5668, 19848,   102])\n",
      "entity_list: ['data generating distribution']\n",
      "entity_token: [tensor([ 2233, 12713,  3735])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: stochastic gradient descent minimizes generalization error is easiest to see in the online learning\n",
      "content_token: tensor([  101,   188,  2430,  7147,  5668, 19848,  6585, 20220,  1116,  1704,\n",
      "         2734,  7353,  1110,   174, 17506,  2556,  1106,  1267,  1107,  1103,\n",
      "         3294,  3776,   102])\n",
      "entity_list: ['stochastic gradient descent', 'generalization error', 'online learning']\n",
      "entity_token: [tensor([  188,  2430,  7147,  5668, 19848,  6585]), tensor([1704, 2734, 7353]), tensor([3294, 3776])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the online learning case, where examples or minibatches are drawn from a stream of data. In other\n",
      "content_token: tensor([  101,  1103,  3294,  3776,  1692,   117,  1187,  5136,  1137,  8715,\n",
      "        14602,  7486,  1132,  3795,  1121,   170,  5118,  1104,  2233,   119,\n",
      "         1130,  1168,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of data. In other words, instead of receiving a fixed-size training set, the learner is similar to\n",
      "content_token: tensor([ 101, 1104, 2233,  119, 1130, 1168, 1734,  117, 1939, 1104, 4172,  170,\n",
      "        4275,  118, 2060, 2013, 1383,  117, 1103, 3858, 1200, 1110, 1861, 1106,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is similar to a living being who sees a new example at each instant, with every example (x,y)\n",
      "content_token: tensor([ 101, 1110, 1861, 1106,  170, 1690, 1217, 1150, 5302,  170, 1207, 1859,\n",
      "        1120, 1296, 6879,  117, 1114, 1451, 1859,  113,  193,  117,  194,  114,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: every example (x,y) coming from the data generating distribution p (x,y). data In this scenario,\n",
      "content_token: tensor([  101,  1451,  1859,   113,   193,   117,   194,   114,  1909,  1121,\n",
      "         1103,  2233, 12713,  3735,   185,   113,   193,   117,   194,   114,\n",
      "          119,  2233,  1130,  1142, 12671,   117,   102])\n",
      "entity_list: ['output: data generating distribution']\n",
      "entity_token: [tensor([ 5964,   131,  2233, 12713,  3735])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In this scenario, examples are never repeated; every experience is a fair sample from p . data The\n",
      "content_token: tensor([  101,  1130,  1142, 12671,   117,  5136,  1132,  1309,  4892,   132,\n",
      "         1451,  2541,  1110,   170,  4652,  6876,  1121,   185,   119,  2233,\n",
      "         1109,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from p . data The equivalence is easiest to derive when both x and y are discrete. In this case,\n",
      "content_token: tensor([  101,  1121,   185,   119,  2233,  1109,   174, 18276, 25388,  1110,\n",
      "          174, 17506,  2556,  1106, 20292,  1165,  1241,   193,  1105,   194,\n",
      "         1132, 18535,   119,  1130,  1142,  1692,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: In this case, the generalization error (equation 8.2) can be written as a sum J (θ) = p\n",
      "content_token: tensor([ 101, 1130, 1142, 1692,  117, 1103, 1704, 2734, 7353,  113, 8381,  129,\n",
      "         119,  123,  114, 1169, 1129, 1637, 1112,  170, 7584,  147,  113,  425,\n",
      "         114,  134,  185,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: as a sum J (θ) = p (x,y)L(f(x;θ),y), (8.7) ∗ data x y  with the exact gradient g = J (θ) = p\n",
      "content_token: tensor([  101,  1112,   170,  7584,   147,   113,   425,   114,   134,   185,\n",
      "          113,   193,   117,   194,   114,   149,   113,   175,   113,   193,\n",
      "          132,   425,   114,   117,   194,   114,   117,   113,   129,   119,\n",
      "          128,   114,   852,  2233,   193,   194,  1114,  1103,  6129, 19848,\n",
      "          176,   134,   147,   113,   425,   114,   134,   185,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: g = J (θ) = p (x,y) L(f(x;θ),y). (8.8) θ ∗ data θ ∇ ∇ x y  We have already seen the same fact\n",
      "content_token: tensor([ 101,  176,  134,  147,  113,  425,  114,  134,  185,  113,  193,  117,\n",
      "         194,  114,  149,  113,  175,  113,  193,  132,  425,  114,  117,  194,\n",
      "         114,  119,  113,  129,  119,  129,  114,  425,  852, 2233,  425,  100,\n",
      "         100,  193,  194, 1284, 1138, 1640, 1562, 1103, 1269, 1864,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: seen the same fact demonstrated for the log-likelihood in equa- tion 8.5 and equation 8.6; we\n",
      "content_token: tensor([  101,  1562,  1103,  1269,  1864,  7160,  1111,  1103,  9366,   118,\n",
      "        17843,  1107,   174, 13284,   118,   189,  1988,   129,   119,   126,\n",
      "         1105,  8381,   129,   119,   127,   132,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: equation 8.6; we observe now that this holds for other functions L besides the likelihood. A\n",
      "content_token: tensor([  101,  8381,   129,   119,   127,   132,  1195, 12326,  1208,  1115,\n",
      "         1142,  3486,  1111,  1168,  4226,   149,  8655,  1103, 17843,   119,\n",
      "          138,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the likelihood. A similar result can be derived when x and y are continuous, under mild assumptions\n",
      "content_token: tensor([  101,  1103, 17843,   119,   138,  1861,  1871,  1169,  1129,  4408,\n",
      "         1165,   193,  1105,   194,  1132,  6803,   117,  1223, 10496, 19129,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: mild assumptions regarding p and L. data Hence, we can obtain an unbiased estimator of the exact\n",
      "content_token: tensor([  101, 10496, 19129,  4423,   185,  1105,   149,   119,  2233, 13615,\n",
      "          117,  1195,  1169,  6268,  1126,  8362, 10242,  5591, 12890, 23021,\n",
      "         1104,  1103,  6129,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the exact gradient of the generalization error by sampling a minibatch of examples x(1),...x(m)\n",
      "content_token: tensor([  101,  1104,  1103,  6129, 19848,  1104,  1103,  1704,  2734,  7353,\n",
      "         1118, 18200,   170,  8715, 14602,  1732,  1104,  5136,   193,   113,\n",
      "          122,   114,   117,   119,   119,   119,   193,   113,   182,   114,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: x(1),...x(m) with cor- { } responding targets y(i) from the data generating distribution p , and\n",
      "content_token: tensor([  101,   193,   113,   122,   114,   117,   119,   119,   119,   193,\n",
      "          113,   182,   114,  1114,  1884,  1197,   118,   196,   198, 16322,\n",
      "         7539,   194,   113,   178,   114,  1121,  1103,  2233, 12713,  3735,\n",
      "          185,   117,  1105,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: p , and computing data the gradient of the loss with respect to the parameters for that minibatch:\n",
      "content_token: tensor([  101,   185,   117,  1105, 12783,  2233,  1103, 19848,  1104,  1103,\n",
      "         2445,  1114,  4161,  1106,  1103, 11934,  1111,  1115,  8715, 14602,\n",
      "         1732,   131,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: for that minibatch: 1 ˆg = L(f(x(i);θ),y(i)). (8.9) θ m∇ i  Updating θ in the direction of gˆ\n",
      "content_token: tensor([  101,  1111,  1115,  8715, 14602,  1732,   131,   122,   100,   134,\n",
      "          149,   113,   175,   113,   193,   113,   178,   114,   132,   425,\n",
      "          114,   117,   194,   113,   178,   114,   114,   119,   113,   129,\n",
      "          119,   130,   114,   425,   100,   178,  3725, 23562,   425,  1107,\n",
      "         1103,  2447,  1104,   100,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the direction of gˆ performs SGD on the generalization error. Of course, this interpretation only\n",
      "content_token: tensor([  101,  1103,  2447,  1104,   100, 10383,   156,  2349,  2137,  1113,\n",
      "         1103,  1704,  2734,  7353,   119,  2096,  1736,   117,  1142,  7628,\n",
      "         1178,   102])\n",
      "entity_list: ['SGD']\n",
      "entity_token: [tensor([ 156, 2349, 2137])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: interpretation only applies when examples are not reused. Nonetheless, it is usually best to make\n",
      "content_token: tensor([  101,  7628,  1178, 12175,  1165,  5136,  1132,  1136,  1231, 11031,\n",
      "          119, 16097,   117,  1122,  1110,  1932,  1436,  1106,  1294,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: best to make several passes through the training set, unless the training set is extremely large.\n",
      "content_token: tensor([ 101, 1436, 1106, 1294, 1317, 4488, 1194, 1103, 2013, 1383,  117, 4895,\n",
      "        1103, 2013, 1383, 1110, 4450, 1415,  119,  102])\n",
      "entity_list: ['training set']\n",
      "entity_token: [tensor([2013, 1383])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is extremely large. When multiple such epochs are used, only the first epoch follows the unbiased\n",
      "content_token: tensor([  101,  1110,  4450,  1415,   119,  1332,  2967,  1216,   174,  5674,\n",
      "        17704,  1132,  1215,   117,  1178,  1103,  1148,   174,  5674,  1732,\n",
      "         3226,  1103,  8362, 10242,  5591,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the unbiased gradient of the generalization error, but 281\n",
      "content_token: tensor([  101,  1103,  8362, 10242,  5591, 19848,  1104,  1103,  1704,  2734,\n",
      "         7353,   117,  1133, 25567,   102])\n",
      "entity_list: ['generalization error']\n",
      "entity_token: [tensor([1704, 2734, 7353])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 9. CONVOLUTIONAL NETWORKS describes general guidelines for choosing which tools to use in\n",
      "content_token: tensor([  101,  8203,   130,   119, 18732,  2249, 21049,  2162, 16830, 24805,\n",
      "        12507, 26546,  1942,  2924,  9565, 25370,  4856,  1704, 13112,  1111,\n",
      "        11027,  1134,  5537,  1106,  1329,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: tools to use in which circumstances. Research into convolutional network architectures proceeds so\n",
      "content_token: tensor([  101,  5537,  1106,  1329,  1107,  1134,  5607,   119,  2713,  1154,\n",
      "        14255,  6005, 18404,  1348,  2443,  4220,  1116, 11283,  1177,   102])\n",
      "entity_list: ['convolutional network architectures']\n",
      "entity_token: [tensor([14255,  6005, 18404,  1348,  2443,  4220,  1116])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: proceeds so rapidly that a new best architecture for a given benchmark is announced every few weeks\n",
      "content_token: tensor([  101, 11283,  1177,  5223,  1115,   170,  1207,  1436,  4220,  1111,\n",
      "          170,  1549,  6757,  8519,  1110,  1717,  1451,  1374,  2277,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: every few weeks to months, rendering it impractical to describe the best architecture in print.\n",
      "content_token: tensor([  101,  1451,  1374,  2277,  1106,  1808,   117, 15171,  1122, 24034,\n",
      "        18890,  1106,  5594,  1103,  1436,  4220,  1107,  5911,   119,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in print. However, the best architectures have consistently been composed of the building blocks\n",
      "content_token: tensor([  101,  1107,  5911,   119,  1438,   117,  1103,  1436,  4220,  1116,\n",
      "         1138, 10887,  1151,  2766,  1104,  1103,  1459,  5511,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the building blocks described here. 9.1 The Convolution Operation In its most general form,\n",
      "content_token: tensor([  101,  1103,  1459,  5511,  1758,  1303,   119,   130,   119,   122,\n",
      "         1109, 16752,  6005, 18404,  5158,  1130,  1157,  1211,  1704,  1532,\n",
      "          117,   102])\n",
      "entity_list: ['Convolution Operation']\n",
      "entity_token: [tensor([16752,  6005, 18404,  5158])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: most general form, convolution is an operation on two functions of a real- valued argument. To\n",
      "content_token: tensor([  101,  1211,  1704,  1532,   117, 14255,  6005, 18404,  1110,  1126,\n",
      "         2805,  1113,  1160,  4226,  1104,   170,  1842,   118, 11165,  6171,\n",
      "          119,  1706,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: valued argument. To motivate the definition of convolution, we start with examples of two functions\n",
      "content_token: tensor([  101, 11165,  6171,   119,  1706,   182,  3329, 18851,  1103,  5754,\n",
      "         1104, 14255,  6005, 18404,   117,  1195,  1838,  1114,  5136,  1104,\n",
      "         1160,  4226,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of two functions we might use. Suppose we are tracking the location of a spaceship with a laser\n",
      "content_token: tensor([  101,  1104,  1160,  4226,  1195,  1547,  1329,   119, 15463,  8661,\n",
      "         6787,  1195,  1132, 10066,  1103,  2450,  1104,   170,  6966,  3157,\n",
      "         1114,   170, 10221,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with a laser sensor. Our laser sensor provides a single output x(t), the position of the spaceship\n",
      "content_token: tensor([  101,  1114,   170, 10221, 15228,   119,  3458, 10221, 15228,  2790,\n",
      "          170,  1423,  5964,   193,   113,   189,   114,   117,  1103,  1700,\n",
      "         1104,  1103,  6966,  3157,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the spaceship at time t. Both x and t are real-valued, i.e., we can get a different reading from\n",
      "content_token: tensor([  101,  1104,  1103,  6966,  3157,  1120,  1159,   189,   119,  2695,\n",
      "          193,  1105,   189,  1132,  1842,   118, 11165,   117,   178,   119,\n",
      "          174,   119,   117,  1195,  1169,  1243,   170,  1472,  3455,  1121,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: reading from the laser sensor at any instant in time. Now suppose that our laser sensor is somewhat\n",
      "content_token: tensor([  101,  3455,  1121,  1103, 10221, 15228,  1120,  1251,  6879,  1107,\n",
      "         1159,   119,  1986,  6699,  1115,  1412, 10221, 15228,  1110,  4742,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sensor is somewhat noisy. To obtain a less noisy estimate of the spaceship’s position, we would\n",
      "content_token: tensor([  101, 15228,  1110,  4742, 24678,   119,  1706,  6268,   170,  1750,\n",
      "        24678, 10301,  1104,  1103,  6966,  3157,   787,   188,  1700,   117,\n",
      "         1195,  1156,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: position, we would like to average together several measurements. Of course, more recent\n",
      "content_token: tensor([  101,  1700,   117,  1195,  1156,  1176,  1106,  1903,  1487,  1317,\n",
      "        12307,   119,  2096,  1736,   117,  1167,  2793,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: course, more recent measurements are more relevant, so we will want this to be a weighted average\n",
      "content_token: tensor([  101,  1736,   117,  1167,  2793, 12307,  1132,  1167,  7503,   117,\n",
      "         1177,  1195,  1209,  1328,  1142,  1106,  1129,   170, 20167,  1903,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a weighted average that gives more weight to recent measurements. We can do this with a weighting\n",
      "content_token: tensor([  101,   170, 20167,  1903,  1115,  3114,  1167,  2841,  1106,  2793,\n",
      "        12307,   119,  1284,  1169,  1202,  1142,  1114,   170,  2841,  1158,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with a weighting function w(a), where a is the age of a measurement. If we apply such a weighted\n",
      "content_token: tensor([  101,  1114,   170,  2841,  1158,  3053,   192,   113,   170,   114,\n",
      "          117,  1187,   170,  1110,  1103,  1425,  1104,   170, 11842,   119,\n",
      "         1409,  1195,  6058,  1216,   170, 20167,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: such a weighted average operation at every moment, we obtain a new function s providing a smoothed\n",
      "content_token: tensor([  101,  1216,   170, 20167,  1903,  2805,  1120,  1451,  1721,   117,\n",
      "         1195,  6268,   170,  1207,  3053,   188,  3558,   170, 19472,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a smoothed estimate of the position of the spaceship: s(t) = x(a)w(t a)da (9.1) −  This operation\n",
      "content_token: tensor([  101,   170, 19472, 10301,  1104,  1103,  1700,  1104,  1103,  6966,\n",
      "         3157,   131,   188,   113,   189,   114,   134,   193,   113,   170,\n",
      "          114,   192,   113,   189,   170,   114,  5358,   113,   130,   119,\n",
      "          122,   114,   851,  1188,  2805,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: −  This operation is called convolution. The convolution operation is typically denoted with an\n",
      "content_token: tensor([  101,   851,  1188,  2805,  1110,  1270, 14255,  6005, 18404,   119,\n",
      "         1109, 14255,  6005, 18404,  2805,  1110,  3417, 21307,  1114,  1126,\n",
      "          102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: denoted with an asterisk: s(t) = (x w)(t) (9.2) ∗ In our example, w needs to be a valid probability\n",
      "content_token: tensor([  101, 21307,  1114,  1126,  1112,  2083, 13189,   131,   188,   113,\n",
      "          189,   114,   134,   113,   193,   192,   114,   113,   189,   114,\n",
      "          113,   130,   119,   123,   114,   852,  1130,  1412,  1859,   117,\n",
      "          192,  2993,  1106,  1129,   170,  9221,  9750,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a valid probability density function, or the output is not a weighted average. Also, w needs to be\n",
      "content_token: tensor([  101,   170,  9221,  9750,  3476,  3053,   117,  1137,  1103,  5964,\n",
      "         1110,  1136,   170, 20167,  1903,   119,  2907,   117,   192,  2993,\n",
      "         1106,  1129,   102])\n",
      "entity_list: ['valid probability density function']\n",
      "entity_token: [tensor([9221, 9750, 3476, 3053])]\n",
      "label: tensor([0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Also, w needs to be 0 for all negative arguments, or it will look into the future, which is\n",
      "content_token: tensor([ 101, 2907,  117,  192, 2993, 1106, 1129,  121, 1111, 1155, 4366, 9989,\n",
      "         117, 1137, 1122, 1209, 1440, 1154, 1103, 2174,  117, 1134, 1110,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: future, which is presumably beyond our capabilities. These limitations are particular to our\n",
      "content_token: tensor([  101,  2174,   117,  1134,  1110, 11280,  2894,  1412,  9816,   119,\n",
      "         1636, 13004,  1132,  2440,  1106,  1412,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: particular to our example though. In general, convolution is defined for any functions for which\n",
      "content_token: tensor([  101,  2440,  1106,  1412,  1859,  1463,   119,  1130,  1704,   117,\n",
      "        14255,  6005, 18404,  1110,  3393,  1111,  1251,  4226,  1111,  1134,\n",
      "          102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: functions for which the above integral is defined, and may be used for other purposes besides\n",
      "content_token: tensor([  101,  4226,  1111,  1134,  1103,  1807, 10226,  1110,  3393,   117,\n",
      "         1105,  1336,  1129,  1215,  1111,  1168,  4998,  8655,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: purposes besides taking weighted averages. In convolutional network terminology, the first argument\n",
      "content_token: tensor([  101,  4998,  8655,  1781, 20167, 22284,   119,  1130, 14255,  6005,\n",
      "        18404,  1348,  2443, 20925,   117,  1103,  1148,  6171,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the first argument (in this example, the function x) to the convolution is often referred to as the\n",
      "content_token: tensor([  101,  1103,  1148,  6171,   113,  1107,  1142,  1859,   117,  1103,\n",
      "         3053,   193,   114,  1106,  1103, 14255,  6005, 18404,  1110,  1510,\n",
      "         2752,  1106,  1112,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: referred to as the input and the second 331 CHAPTER 9. CONVOLUTIONAL NETWORKS argument (in this\n",
      "content_token: tensor([  101,  2752,  1106,  1112,  1103,  7758,  1105,  1103,  1248,  3081,\n",
      "         1475,  8203,   130,   119, 18732,  2249, 21049,  2162, 16830, 24805,\n",
      "        12507, 26546,  1942,  2924,  9565, 25370,  6171,   113,  1107,  1142,\n",
      "          102])\n",
      "entity_list: [\"'convolutional networks'\"]\n",
      "entity_token: [tensor([  112, 14255,  6005, 18404,  1348,  6379,   112])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: argument (in this example, the function w) as the kernel. The output is sometimes referred to as\n",
      "content_token: tensor([  101,  6171,   113,  1107,  1142,  1859,   117,  1103,  3053,   192,\n",
      "          114,  1112,  1103, 18670,   119,  1109,  5964,  1110,  2121,  2752,\n",
      "         1106,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: referred to as the feature map. In our example, the idea of a laser sensor that can provide\n",
      "content_token: tensor([  101,  2752,  1106,  1112,  1103,  2672,  4520,   119,  1130,  1412,\n",
      "         1859,   117,  1103,  1911,  1104,   170, 10221, 15228,  1115,  1169,\n",
      "         2194,   102])\n",
      "entity_list: ['feature map']\n",
      "entity_token: [tensor([2672, 4520])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that can provide measurements at every instant in time is not realistic. Usually, when we work with\n",
      "content_token: tensor([  101,  1115,  1169,  2194, 12307,  1120,  1451,  6879,  1107,  1159,\n",
      "         1110,  1136, 13142,   119, 12378,   117,  1165,  1195,  1250,  1114,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: when we work with data on a computer, time will be discretized, and our sensor will provide data at\n",
      "content_token: tensor([  101,  1165,  1195,  1250,  1114,  2233,  1113,   170,  2775,   117,\n",
      "         1159,  1209,  1129,  6187,  8127,  2200,   117,  1105,  1412, 15228,\n",
      "         1209,  2194,  2233,  1120,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: provide data at regular intervals. In our example, it might be more realistic to assume that our\n",
      "content_token: tensor([  101,  2194,  2233,  1120,  2366, 14662,   119,  1130,  1412,  1859,\n",
      "          117,  1122,  1547,  1129,  1167, 13142,  1106,  7568,  1115,  1412,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to assume that our laser provides a measurement once per second. The time index t can then take on\n",
      "content_token: tensor([  101,  1106,  7568,  1115,  1412, 10221,  2790,   170, 11842,  1517,\n",
      "         1679,  1248,   119,  1109,  1159,  7448,   189,  1169,  1173,  1321,\n",
      "         1113,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: t can then take on only integer values. If we now assume that x and w are defined only on integer\n",
      "content_token: tensor([  101,   189,  1169,  1173,  1321,  1113,  1178, 18157,  4718,   119,\n",
      "         1409,  1195,  1208,  7568,  1115,   193,  1105,   192,  1132,  3393,\n",
      "         1178,  1113, 18157,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: only on integer t, we can define the discrete convolution: ∞ s(t) = (x w)(t) = x(a)w(t a) (9.3) ∗ −\n",
      "content_token: tensor([  101,  1178,  1113, 18157,   189,   117,  1195,  1169,  9410,  1103,\n",
      "        18535, 14255,  6005, 18404,   131,   855,   188,   113,   189,   114,\n",
      "          134,   113,   193,   192,   114,   113,   189,   114,   134,   193,\n",
      "          113,   170,   114,   192,   113,   189,   170,   114,   113,   130,\n",
      "          119,   124,   114,   852,   851,   102])\n",
      "entity_list: ['discrete convolution']\n",
      "entity_token: [tensor([18535, 14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a) (9.3) ∗ − a= −∞ In machine learning applications, the input is usually a multidimensional array\n",
      "content_token: tensor([  101,   170,   114,   113,   130,   119,   124,   114,   852,   851,\n",
      "          170,   134,   851, 28749,  1130,  3395,  3776,  4683,   117,  1103,\n",
      "         7758,  1110,  1932,   170,  4321,  3309,  2354, 24533,  9245,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: array of data and the kernel is usually a multidimensional array of parameters that are adapted by\n",
      "content_token: tensor([  101,  9245,  1104,  2233,  1105,  1103, 18670,  1110,  1932,   170,\n",
      "         4321,  3309,  2354, 24533,  9245,  1104, 11934,  1115,  1132,  5546,\n",
      "         1118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that are adapted by the learning algorithm. We will refer to these multidimensional arrays as\n",
      "content_token: tensor([  101,  1115,  1132,  5546,  1118,  1103,  3776,  9932,   119,  1284,\n",
      "         1209,  5991,  1106,  1292,  4321,  3309,  2354, 24533,  9245,  1116,\n",
      "         1112,   102])\n",
      "entity_list: ['learning algorithm', 'multidimensional arrays']\n",
      "entity_token: [tensor([3776, 9932]), tensor([ 4321,  3309,  2354, 24533,  9245,  1116])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: arrays as tensors. Because each element of the input and kernel must be explicitly stored\n",
      "content_token: tensor([  101,  9245,  1116,  1112, 27023,  1116,   119,  2279,  1296,  5290,\n",
      "         1104,  1103,  7758,  1105, 18670,  1538,  1129, 12252,  7905,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: explicitly stored separately, we usually assume that these functions are zero everywhere but the\n",
      "content_token: tensor([  101, 12252,  7905, 10380,   117,  1195,  1932,  7568,  1115,  1292,\n",
      "         4226,  1132,  6756,  7244,  1133,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: everywhere but the finite set of points for which we store the values. This means that in practice\n",
      "content_token: tensor([  101,  7244,  1133,  1103, 10996,  1383,  1104,  1827,  1111,  1134,\n",
      "         1195,  2984,  1103,  4718,   119,  1188,  2086,  1115,  1107,  2415,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that in practice we can implement the infinite summation as a summation over a finite number of\n",
      "content_token: tensor([  101,  1115,  1107,  2415,  1195,  1169, 10407,  1103, 13157,  7584,\n",
      "        16059,  1112,   170,  7584, 16059,  1166,   170, 10996,  1295,  1104,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a finite number of array elements. Finally, we often use convolutions over more than one axis at a\n",
      "content_token: tensor([  101,   170, 10996,  1295,  1104,  9245,  3050,   119,  4428,   117,\n",
      "         1195,  1510,  1329, 14255,  6005, 18404,  1116,  1166,  1167,  1190,\n",
      "         1141,  9840,  1120,   170,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: than one axis at a time. For example, if we use a two-dimensional image I as our input, we probably\n",
      "content_token: tensor([ 101, 1190, 1141, 9840, 1120,  170, 1159,  119, 1370, 1859,  117, 1191,\n",
      "        1195, 1329,  170, 1160,  118, 8611, 3077,  146, 1112, 1412, 7758,  117,\n",
      "        1195, 1930,  102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: input, we probably also want to use a two-dimensional kernel K: S(i,j) = (I K)(i,j) = I(m,n)K(i m,j\n",
      "content_token: tensor([  101,  7758,   117,  1195,  1930,  1145,  1328,  1106,  1329,   170,\n",
      "         1160,   118,  8611, 18670,   148,   131,   156,   113,   178,   117,\n",
      "          179,   114,   134,   113,   146,   148,   114,   113,   178,   117,\n",
      "          179,   114,   134,   146,   113,   182,   117,   183,   114,   148,\n",
      "          113,   178,   182,   117,   179,   102])\n",
      "entity_list: ['deep learning']\n",
      "entity_token: [tensor([1996, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: = I(m,n)K(i m,j n). (9.4) ∗ − − m n  Convolution is commutative, meaning we can equivalently\n",
      "content_token: tensor([  101,   134,   146,   113,   182,   117,   183,   114,   148,   113,\n",
      "          178,   182,   117,   179,   183,   114,   119,   113,   130,   119,\n",
      "          125,   114,   852,   851,   851,   182,   183, 16752,  6005, 18404,\n",
      "         1110,  3254, 13601, 18216,   117,  2764,  1195,  1169,  4976,  1193,\n",
      "          102])\n",
      "entity_list: ['deep learning', 'convolution']\n",
      "entity_token: [tensor([1996, 3776]), tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we can equivalently write: S(i,j) = (K I)(i,j) = I(i m,j n)K(m,n). (9.5) ∗ − − m n  Usually the\n",
      "content_token: tensor([  101,  1195,  1169,  4976,  1193,  3593,   131,   156,   113,   178,\n",
      "          117,   179,   114,   134,   113,   148,   146,   114,   113,   178,\n",
      "          117,   179,   114,   134,   146,   113,   178,   182,   117,   179,\n",
      "          183,   114,   148,   113,   182,   117,   183,   114,   119,   113,\n",
      "          130,   119,   126,   114,   852,   851,   851,   182,   183, 12378,\n",
      "         1103,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: m n  Usually the latter formula is more straightforward to implement in a machine learning\n",
      "content_token: tensor([  101,   182,   183, 12378,  1103,  2985,  7893,  1110,  1167, 21546,\n",
      "         1106, 10407,  1107,   170,  3395,  3776,   102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a machine learning library, because there is less variation in the range of valid values of m and\n",
      "content_token: tensor([ 101,  170, 3395, 3776, 3340,  117, 1272, 1175, 1110, 1750, 8516, 1107,\n",
      "        1103, 2079, 1104, 9221, 4718, 1104,  182, 1105,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: values of m and n. The commutative property of convolution arises because we have flipped the\n",
      "content_token: tensor([  101,  4718,  1104,   182,  1105,   183,   119,  1109,  3254, 13601,\n",
      "        18216,  2400,  1104, 14255,  6005, 18404, 20251,  1272,  1195,  1138,\n",
      "         9082,  1103,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: we have flipped the kernel relative to the input, in the sense that as m increases, the index into\n",
      "content_token: tensor([  101,  1195,  1138,  9082,  1103, 18670,  5236,  1106,  1103,  7758,\n",
      "          117,  1107,  1103,  2305,  1115,  1112,   182,  6986,   117,  1103,\n",
      "         7448,  1154,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the index into the input increases, but the index into the kernel decreases. The only reason to\n",
      "content_token: tensor([  101,  1103,  7448,  1154,  1103,  7758,  6986,   117,  1133,  1103,\n",
      "         7448,  1154,  1103, 18670, 19377,   119,  1109,  1178,  2255,  1106,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: The only reason to flip the kernel is to obtain the commutative property. While the commutative\n",
      "content_token: tensor([  101,  1109,  1178,  2255,  1106, 12785,  1103, 18670,  1110,  1106,\n",
      "         6268,  1103,  3254, 13601, 18216,  2400,   119,  1799,  1103,  3254,\n",
      "        13601, 18216,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the commutative property 332 CHAPTER 9. CONVOLUTIONAL NETWORKS is useful for writing proofs, it is\n",
      "content_token: tensor([  101,  1103,  3254, 13601, 18216,  2400,  3081,  1477,  8203,   130,\n",
      "          119, 18732,  2249, 21049,  2162, 16830, 24805, 12507, 26546,  1942,\n",
      "         2924,  9565, 25370,  1110,  5616,  1111,  2269,  6777,  1116,   117,\n",
      "         1122,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: proofs, it is not usually an important property of a neural network implementation. Instead, many\n",
      "content_token: tensor([  101,  6777,  1116,   117,  1122,  1110,  1136,  1932,  1126,  1696,\n",
      "         2400,  1104,   170, 18250,  2443,  7249,   119,  3743,   117,  1242,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Instead, many neural network libraries implement a related function called the cross-correlation,\n",
      "content_token: tensor([  101,  3743,   117,  1242, 18250,  2443,  9818, 10407,   170,  2272,\n",
      "         3053,  1270,  1103,  2771,   118, 18741,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: cross-correlation, which is the same as convolution but without flipping the kernel: S(i,j) = (I\n",
      "content_token: tensor([  101,  2771,   118, 18741,   117,  1134,  1110,  1103,  1269,  1112,\n",
      "        14255,  6005, 18404,  1133,  1443, 21547,  1103, 18670,   131,   156,\n",
      "          113,   178,   117,   179,   114,   134,   113,   146,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: kernel: S(i,j) = (I K)(i,j) = I(i +m,j +n)K(m,n). (9.6) ∗ m n  Many machine learning libraries\n",
      "content_token: tensor([  101, 18670,   131,   156,   113,   178,   117,   179,   114,   134,\n",
      "          113,   146,   148,   114,   113,   178,   117,   179,   114,   134,\n",
      "          146,   113,   178,   116,   182,   117,   179,   116,   183,   114,\n",
      "          148,   113,   182,   117,   183,   114,   119,   113,   130,   119,\n",
      "          127,   114,   852,   182,   183,  2408,  3395,  3776,  9818,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: learning libraries implement cross-correlation but call it convolution. In this text we will follow\n",
      "content_token: tensor([  101,  3776,  9818, 10407,  2771,   118, 18741,  1133,  1840,  1122,\n",
      "        14255,  6005, 18404,   119,  1130,  1142,  3087,  1195,  1209,  2812,\n",
      "          102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: text we will follow this convention of calling both operations convolution, and specify whether we\n",
      "content_token: tensor([  101,  3087,  1195,  1209,  2812,  1142,  6765,  1104,  3516,  1241,\n",
      "         2500, 14255,  6005, 18404,   117,  1105, 22829,  2480,  1195,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: specify whether we mean to flip the kernel or not in contexts where kernel flipping is relevant. In\n",
      "content_token: tensor([  101, 22829,  2480,  1195,  1928,  1106, 12785,  1103, 18670,  1137,\n",
      "         1136,  1107, 20011,  1187, 18670, 21547,  1110,  7503,   119,  1130,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: is relevant. In the context of machine learning, the learning algorithm will learn the appropriate\n",
      "content_token: tensor([ 101, 1110, 7503,  119, 1130, 1103, 5618, 1104, 3395, 3776,  117, 1103,\n",
      "        3776, 9932, 1209, 3858, 1103, 5806,  102])\n",
      "entity_list: ['machine learning']\n",
      "entity_token: [tensor([3395, 3776])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the appropriate values of the kernel in the appropriate place, so an algorithm based on convolution\n",
      "content_token: tensor([  101,  1103,  5806,  4718,  1104,  1103, 18670,  1107,  1103,  5806,\n",
      "         1282,   117,  1177,  1126,  9932,  1359,  1113, 14255,  6005, 18404,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on convolution with kernel flipping will learn a kernel that is flipped relative to the kernel\n",
      "content_token: tensor([  101,  1113, 14255,  6005, 18404,  1114, 18670, 21547,  1209,  3858,\n",
      "          170, 18670,  1115,  1110,  9082,  5236,  1106,  1103, 18670,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to the kernel learned by an algorithm without the flipping. It is also rare for convolution to be\n",
      "content_token: tensor([  101,  1106,  1103, 18670,  3560,  1118,  1126,  9932,  1443,  1103,\n",
      "        21547,   119,  1135,  1110,  1145,  4054,  1111, 14255,  6005, 18404,\n",
      "         1106,  1129,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: convolution to be used alone in machine learning; instead convolution is used simultaneously with\n",
      "content_token: tensor([  101, 14255,  6005, 18404,  1106,  1129,  1215,  2041,  1107,  3395,\n",
      "         3776,   132,  1939, 14255,  6005, 18404,  1110,  1215,  7344,  1114,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: simultaneously with other functions, and the combination of these functions does not commute\n",
      "content_token: tensor([  101,  7344,  1114,  1168,  4226,   117,  1105,  1103,  4612,  1104,\n",
      "         1292,  4226,  1674,  1136,  3254, 13601,  1566,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: does not commute regardless of whether the convolution operation flips its kernel or not. See\n",
      "content_token: tensor([  101,  1674,  1136,  3254, 13601,  1566,  8334,  1104,  2480,  1103,\n",
      "        14255,  6005, 18404,  2805, 12785,  1116,  1157, 18670,  1137,  1136,\n",
      "          119,  3969,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: kernel or not. See figure 9.1 for an example of convolution (without kernel flipping) applied to a\n",
      "content_token: tensor([  101, 18670,  1137,  1136,   119,  3969,  2482,   130,   119,   122,\n",
      "         1111,  1126,  1859,  1104, 14255,  6005, 18404,   113,  1443, 18670,\n",
      "        21547,   114,  3666,  1106,   170,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: applied to a 2-D tensor. Discrete convolution can be viewed as multiplication by a matrix. However,\n",
      "content_token: tensor([  101,  3666,  1106,   170,   123,   118,   141, 27023,   119, 14856,\n",
      "         8127,  1162, 14255,  6005, 18404,  1169,  1129,  6497,  1112,  4321,\n",
      "        15534,  1118,   170,  8952,   119,  1438,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: a matrix. However, the matrix has several entries constrained to be equal to other entries. For\n",
      "content_token: tensor([  101,   170,  8952,   119,  1438,   117,  1103,  8952,  1144,  1317,\n",
      "        10813, 14255, 16468,  9044,  1106,  1129,  4463,  1106,  1168, 10813,\n",
      "          119,  1370,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: other entries. For example, for univariate discrete convolution, each row of the matrix is\n",
      "content_token: tensor([  101,  1168, 10813,   119,  1370,  1859,   117,  1111,  8362, 12416,\n",
      "         3464,  1566, 18535, 14255,  6005, 18404,   117,  1296,  5105,  1104,\n",
      "         1103,  8952,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the matrix is constrained to be equal to the row above shifted by one element. This is known as\n",
      "content_token: tensor([  101,  1104,  1103,  8952,  1110, 14255, 16468,  9044,  1106,  1129,\n",
      "         4463,  1106,  1103,  5105,  1807,  4707,  1118,  1141,  5290,   119,\n",
      "         1188,  1110,  1227,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This is known as a Toeplitz matrix. In two dimensions, a doubly block circulant matrix corresponds\n",
      "content_token: tensor([  101,  1188,  1110,  1227,  1112,   170,  1706,  8043, 15516,  8952,\n",
      "          119,  1130,  1160, 10082,   117,   170,  1202, 10354,  1193,  3510,\n",
      "          172,  3161, 21608,  2227,  8952, 15497,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: matrix corresponds to convolution. In addition to these constraints that several elements be equal\n",
      "content_token: tensor([  101,  8952, 15497,  1106, 14255,  6005, 18404,   119,  1130,  1901,\n",
      "         1106,  1292, 15651,  1115,  1317,  3050,  1129,  4463,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: elements be equal to each other, convolution usually corresponds to a very sparse matrix (a matrix\n",
      "content_token: tensor([  101,  3050,  1129,  4463,  1106,  1296,  1168,   117, 14255,  6005,\n",
      "        18404,  1932, 15497,  1106,   170,  1304, 22726,  8952,   113,   170,\n",
      "         8952,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: matrix (a matrix whose entries are mostly equal to zero). This is because the kernel is usually\n",
      "content_token: tensor([  101,  8952,   113,   170,  8952,  2133, 10813,  1132,  2426,  4463,\n",
      "         1106,  6756,   114,   119,  1188,  1110,  1272,  1103, 18670,  1110,\n",
      "         1932,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: kernel is usually much smaller than the input image. Any neural network algorithm that works with\n",
      "content_token: tensor([  101, 18670,  1110,  1932,  1277,  2964,  1190,  1103,  7758,  3077,\n",
      "          119,  6291, 18250,  2443,  9932,  1115,  1759,  1114,   102])\n",
      "entity_list: ['neural network algorithm']\n",
      "entity_token: [tensor([18250,  2443,  9932])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: that works with matrix multiplication and does not depend on specific properties of the matrix\n",
      "content_token: tensor([  101,  1115,  1759,  1114,  8952,  4321, 15534,  1105,  1674,  1136,\n",
      "        12864,  1113,  2747,  4625,  1104,  1103,  8952,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: of the matrix structure should work with convolution, without requiring any further changes to the\n",
      "content_token: tensor([  101,  1104,  1103,  8952,  2401,  1431,  1250,  1114, 14255,  6005,\n",
      "        18404,   117,  1443,  8753,  1251,  1748,  2607,  1106,  1103,   102])\n",
      "entity_list: ['convolution']\n",
      "entity_token: [tensor([14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: changes to the neural network. Typical convolutional neural networks do make use of further\n",
      "content_token: tensor([  101,  2607,  1106,  1103, 18250,  2443,   119, 23125, 14255,  6005,\n",
      "        18404,  1348, 18250,  6379,  1202,  1294,  1329,  1104,  1748,   102])\n",
      "entity_list: ['neural network', 'convolutional neural networks']\n",
      "entity_token: [tensor([18250,  2443]), tensor([14255,  6005, 18404,  1348, 18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: make use of further specializations in order to deal with large inputs efficiently, but these are\n",
      "content_token: tensor([  101,  1294,  1329,  1104,  1748,  1957, 20412,  1107,  1546,  1106,\n",
      "         2239,  1114,  1415, 22743, 19723,   117,  1133,  1292,  1132,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: but these are not strictly necessary from a theoretical perspective. 333 CHAPTER 9. CONVOLUTIONAL\n",
      "content_token: tensor([  101,  1133,  1292,  1132,  1136, 10802,  3238,  1121,   170, 10093,\n",
      "         7281,   119, 23335,  8203,   130,   119, 18732,  2249, 21049,  2162,\n",
      "        16830, 24805, 12507,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 9. CONVOLUTIONAL NETWORKS Input Kernel a b c d w x e f g h y z i j k l Output aaww ++ bbxx ++ bbww\n",
      "content_token: tensor([  101,   130,   119, 18732,  2249, 21049,  2162, 16830, 24805, 12507,\n",
      "        26546,  1942,  2924,  9565, 25370,  1130, 16156, 25682,  1883,   170,\n",
      "          171,   172,   173,   192,   193,   174,   175,   176,   177,   194,\n",
      "          195,   178,   179,   180,   181,  3929, 16156,   170,  7220,  2246,\n",
      "          116,   116,   171,  1830,  1775,  1775,   116,   116,   171,  1830,\n",
      "         2246,  2246,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ++ bbxx ++ bbww ++ ccxx ++ ccww ++ ddxx ++ eeyy ++ ffzz ffyy ++ ggzz ggyy ++ hhzz eeww ++ ffxx ++\n",
      "content_token: tensor([  101,   116,   116,   171,  1830,  1775,  1775,   116,   116,   171,\n",
      "         1830,  2246,  2246,   116,   116, 14402,  1775,  1775,   116,   116,\n",
      "        14402,  2246,  2246,   116,   116,   173,  1181,  1775,  1775,   116,\n",
      "          116,   174,  2254,  1183,   116,   116,   175,  2087, 16771,   175,\n",
      "        13268,  1183,   116,   116,   176,  1403, 16771,   176,  4873,  1183,\n",
      "          116,   116,   177,  1324, 16771,   174,  5773,  2246,   116,   116,\n",
      "          175,  2087,  1775,  1775,   116,   116,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: eeww ++ ffxx ++ ffww ++ ggxx ++ ggww ++ hhxx ++ iiyy ++ jjzz jjyy ++ kkzz kkyy ++ llzz Figure 9.1:\n",
      "content_token: tensor([  101,   174,  5773,  2246,   116,   116,   175,  2087,  1775,  1775,\n",
      "          116,   116,   175,  2087,  2246,  2246,   116,   116,   176,  1403,\n",
      "         1775,  1775,   116,   116,   176,  1403,  2246,  2246,   116,   116,\n",
      "          177,  1324,  1775,  1775,   116,   116, 25550,  1183,  1183,   116,\n",
      "          116,   179,  3361, 16771,   179,  3361,  1183,  1183,   116,   116,\n",
      "          180,  1377, 16771,   180,  3781,  1183,   116,   116,  1325, 16771,\n",
      "        15982,   130,   119,   122,   131,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ++ llzz Figure 9.1: An example of 2-D convolution without kernel-flipping. In this case we restrict\n",
      "content_token: tensor([  101,   116,   116,  1325, 16771, 15982,   130,   119,   122,   131,\n",
      "         1760,  1859,  1104,   123,   118,   141, 14255,  6005, 18404,  1443,\n",
      "        18670,   118, 21547,   119,  1130,  1142,  1692,  1195, 23951,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: case we restrict the output to only positions where the kernel lies entirely within the image,\n",
      "content_token: tensor([  101,  1692,  1195, 23951,  1103,  5964,  1106,  1178,  3638,  1187,\n",
      "         1103, 18670,  2887,  3665,  1439,  1103,  3077,   117,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: within the image, called “valid” convolution in some contexts. We draw boxes with arrows to\n",
      "content_token: tensor([  101,  1439,  1103,  3077,   117,  1270,   789,  9221,   790, 14255,\n",
      "         6005, 18404,  1107,  1199, 20011,   119,  1284,  3282,  8171,  1114,\n",
      "        15130,  1106,   102])\n",
      "entity_list: ['valid convolution']\n",
      "entity_token: [tensor([ 9221, 14255,  6005, 18404])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: with arrows to indicate how the upper-left element of the output tensor is formed by applying the\n",
      "content_token: tensor([  101,  1114, 15130,  1106,  5057,  1293,  1103,  3105,   118,  1286,\n",
      "         5290,  1104,  1103,  5964, 27023,  1110,  1824,  1118, 11892,  1103,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: by applying the kernel to the corresponding upper-left region of the input tensor. 334\n",
      "content_token: tensor([  101,  1118, 11892,  1103, 18670,  1106,  1103,  7671,  3105,   118,\n",
      "         1286,  1805,  1104,  1103,  7758, 27023,   119,  3081,  1527,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS 10.1 Unfolding Computational Graphs A\n",
      "content_token: tensor([  101,  8203,  1275,   119, 12342,  4880, 24846, 15517,  2036,   150,\n",
      "        15609, 21678, 15740,   131,   155,  8231, 19556, 16941, 15681, 16716,\n",
      "          155,  8231, 19556, 13882, 17145, 26546, 11365,  1275,   119,   122,\n",
      "        12118, 10787,  1158,  3291,  8223, 15012, 15937,   144, 14543,  9524,\n",
      "          138,   102])\n",
      "entity_list: ['SEQUENCE MODELING', 'RECURRENT AND RECURSIVE NETS']\n",
      "entity_token: [tensor([12342,  4880, 24846, 15517,  2036,   150, 15609, 21678, 15740]), tensor([  155,  8231, 19556, 16941, 15681, 16716,   155,  8231, 19556, 13882,\n",
      "        17145, 26546, 11365])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Graphs A computational graph is a way to formalize the structure of a set of computations, such as\n",
      "content_token: tensor([  101,   144, 14543,  9524,   138, 19903, 10873,  1110,   170,  1236,\n",
      "         1106,  4698,  3708,  1103,  2401,  1104,   170,  1383,  1104,  3254,\n",
      "        19675,  1116,   117,  1216,  1112,   102])\n",
      "entity_list: ['computational graph']\n",
      "entity_token: [tensor([19903, 10873])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: such as those involved in mapping inputs and parameters to outputs and loss. Please refer to\n",
      "content_token: tensor([  101,  1216,  1112,  1343,  2017,  1107, 13970, 22743,  1105, 11934,\n",
      "         1106,  5964,  1116,  1105,  2445,   119,  4203,  5991,  1106,   102])\n",
      "entity_list: ['parameters', 'loss']\n",
      "entity_token: [tensor([11934]), tensor([2445])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Please refer to section 6.5.1 for a general introduction. In this section we explain the idea of\n",
      "content_token: tensor([ 101, 4203, 5991, 1106, 2237,  127,  119,  126,  119,  122, 1111,  170,\n",
      "        1704, 4784,  119, 1130, 1142, 2237, 1195, 4137, 1103, 1911, 1104,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: explain the idea of unfolding a recursive or recurrent computation into a computational graph that\n",
      "content_token: tensor([  101,  4137,  1103,  1911,  1104,  8362, 10787,  1158,   170,  1231,\n",
      "        10182,  1733,  2109,  1137,  1231, 21754,  3254, 19675,  1154,   170,\n",
      "        19903, 10873,  1115,   102])\n",
      "entity_list: ['unfolding a recursive or recurrent computation']\n",
      "entity_token: [tensor([ 8362, 10787,  1158,   170,  1231, 10182,  1733,  2109,  1137,  1231,\n",
      "        21754,  3254, 19675])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: graph that has a repetitive structure, typically corresponding to a chain of events. Unfolding this\n",
      "content_token: tensor([  101, 10873,  1115,  1144,   170, 26976,  2401,   117,  3417,  7671,\n",
      "         1106,   170,  4129,  1104,  1958,   119, 12118, 10787,  1158,  1142,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Unfolding this graph results in the sharing of parameters across a deep network structure. For\n",
      "content_token: tensor([  101, 12118, 10787,  1158,  1142, 10873,  2686,  1107,  1103,  6303,\n",
      "         1104, 11934,  1506,   170,  1996,  2443,  2401,   119,  1370,   102])\n",
      "entity_list: ['deep network structure']\n",
      "entity_token: [tensor([1996, 2443, 2401])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: structure. For example, consider the classical form of a dynamical system: s(t) = f(s(t 1);θ),\n",
      "content_token: tensor([ 101, 2401,  119, 1370, 1859,  117, 4615, 1103, 4521, 1532, 1104,  170,\n",
      "        9652, 1348, 1449,  131,  188,  113,  189,  114,  134,  175,  113,  188,\n",
      "         113,  189,  122,  114,  132,  425,  114,  117,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: s(t) = f(s(t 1);θ), (10.1) − where s(t) is called the state of the system. Equation 10.1 is\n",
      "content_token: tensor([  101,   188,   113,   189,   114,   134,   175,   113,   188,   113,\n",
      "          189,   122,   114,   132,   425,   114,   117,   113,  1275,   119,\n",
      "          122,   114,   851,  1187,   188,   113,   189,   114,  1110,  1270,\n",
      "         1103,  1352,  1104,  1103,  1449,   119,   142, 13284,  2116,  1275,\n",
      "          119,   122,  1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: Equation 10.1 is recurrent because the definition of s at time t refers back to the same definition\n",
      "content_token: tensor([  101,   142, 13284,  2116,  1275,   119,   122,  1110,  1231, 21754,\n",
      "         1272,  1103,  5754,  1104,   188,  1120,  1159,   189,  4431,  1171,\n",
      "         1106,  1103,  1269,  5754,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the same definition at time t 1. − For a finite number of time steps τ, the graph can be unfolded\n",
      "content_token: tensor([  101,  1103,  1269,  5754,  1120,  1159,   189,   122,   119,   851,\n",
      "         1370,   170, 10996,  1295,  1104,  1159,  3343,   437,   117,  1103,\n",
      "        10873,  1169,  1129, 27118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can be unfolded by applying the definition τ 1 times. For example, if we unfold equation 10.1 for τ\n",
      "content_token: tensor([  101,  1169,  1129, 27118,  1118, 11892,  1103,  5754,   437,   122,\n",
      "         1551,   119,  1370,  1859,   117,  1191,  1195,  8362, 10787,  8381,\n",
      "         1275,   119,   122,  1111,   437,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: equation 10.1 for τ = 3 time − steps, we obtain s(3) =f(s(2) ;θ) (10.2) =f(f(s(1) ;θ);θ) (10.3)\n",
      "content_token: tensor([ 101, 8381, 1275,  119,  122, 1111,  437,  134,  124, 1159,  851, 3343,\n",
      "         117, 1195, 6268,  188,  113,  124,  114,  134,  175,  113,  188,  113,\n",
      "         123,  114,  132,  425,  114,  113, 1275,  119,  123,  114,  134,  175,\n",
      "         113,  175,  113,  188,  113,  122,  114,  132,  425,  114,  132,  425,\n",
      "         114,  113, 1275,  119,  124,  114,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ;θ);θ) (10.3) Unfolding the equation by repeatedly applying the definition in this way has yielded\n",
      "content_token: tensor([  101,   132,   425,   114,   132,   425,   114,   113,  1275,   119,\n",
      "          124,   114, 12118, 10787,  1158,  1103,  8381,  1118,  8038, 11892,\n",
      "         1103,  5754,  1107,  1142,  1236,  1144, 18826,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: way has yielded an expression that does not involve recurrence. Such an expression can now be\n",
      "content_token: tensor([  101,  1236,  1144, 18826,  1126,  2838,  1115,  1674,  1136,  8803,\n",
      "         1231, 10182, 21629,   119,  5723,  1126,  2838,  1169,  1208,  1129,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: can now be represented by a traditional directed acyclic computational graph. The unfolded\n",
      "content_token: tensor([  101,  1169,  1208,  1129,  2533,  1118,   170,  2361,  2002,   170,\n",
      "         3457,  1665,  8031, 19903, 10873,   119,  1109, 27118,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: graph. The unfolded computational graph of equation 10.1 and equation 10.3 is illustrated in figure\n",
      "content_token: tensor([  101, 10873,   119,  1109, 27118, 19903, 10873,  1104,  8381,  1275,\n",
      "          119,   122,  1105,  8381,  1275,   119,   124,  1110,  8292,  1107,\n",
      "         2482,   102])\n",
      "entity_list: ['computational graph']\n",
      "entity_token: [tensor([19903, 10873])]\n",
      "label: tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: in figure 10.1. ss((......)) ss((tt 11)) ss((tt)) ss((tt++11)) ss((......)) −− ff ff ff ff Figure\n",
      "content_token: tensor([  101,  1107,  2482,  1275,   119,   122,   119,   188,  1116,   113,\n",
      "          113,   119,   119,   119,   119,   119,   119,   114,   114,   188,\n",
      "         1116,   113,   113,   189,  1204,  1429,   114,   114,   188,  1116,\n",
      "          113,   113,   189,  1204,   114,   114,   188,  1116,   113,   113,\n",
      "          189,  1204,   116,   116,  1429,   114,   114,   188,  1116,   113,\n",
      "          113,   119,   119,   119,   119,   119,   119,   114,   114,   851,\n",
      "        25532,   175,  2087,   175,  2087,   175,  2087,   175,  2087, 15982,\n",
      "          102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ff ff ff ff Figure 10.1: The classical dynamical system described by equation 10.1, illustrated as\n",
      "content_token: tensor([  101,   175,  2087,   175,  2087,   175,  2087,   175,  2087, 15982,\n",
      "         1275,   119,   122,   131,  1109,  4521,  9652,  1348,  1449,  1758,\n",
      "         1118,  8381,  1275,   119,   122,   117,  8292,  1112,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: illustrated as an unfolded computational graph. Each node represents the state at some time t and\n",
      "content_token: tensor([  101,  8292,  1112,  1126, 27118, 19903, 10873,   119,  2994, 14372,\n",
      "         5149,  1103,  1352,  1120,  1199,  1159,   189,  1105,   102])\n",
      "entity_list: ['unfolded computational graph']\n",
      "entity_token: [tensor([27118, 19903, 10873])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: at some time t and the function f maps the state at t to the state at t+1. The same parameters (the\n",
      "content_token: tensor([  101,  1120,  1199,  1159,   189,  1105,  1103,  3053,   175,  7415,\n",
      "         1103,  1352,  1120,   189,  1106,  1103,  1352,  1120,   189,   116,\n",
      "          122,   119,  1109,  1269, 11934,   113,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: parameters (the same value of θ used to parametrize f) are used for all time steps. As another\n",
      "content_token: tensor([  101, 11934,   113,  1103,  1269,  2860,  1104,   425,  1215,  1106,\n",
      "        18311, 11006, 28021,  1162,   175,   114,  1132,  1215,  1111,  1155,\n",
      "         1159,  3343,   119,  1249,  1330,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: steps. As another example, let us consider a dynamical system driven by an external signal x(t),\n",
      "content_token: tensor([ 101, 3343,  119, 1249, 1330, 1859,  117, 1519, 1366, 4615,  170, 9652,\n",
      "        1348, 1449, 4940, 1118, 1126, 6298, 4344,  193,  113,  189,  114,  117,\n",
      "         102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: signal x(t), s(t) = f(s(t 1) ,x(t);θ), (10.4) − 375 CHAPTER 10. SEQUENCE MODELING: RECURRENT AND\n",
      "content_token: tensor([  101,  4344,   193,   113,   189,   114,   117,   188,   113,   189,\n",
      "          114,   134,   175,   113,   188,   113,   189,   122,   114,   117,\n",
      "          193,   113,   189,   114,   132,   425,   114,   117,   113,  1275,\n",
      "          119,   125,   114,   851, 19397,  8203,  1275,   119, 12342,  4880,\n",
      "        24846, 15517,  2036,   150, 15609, 21678, 15740,   131,   155,  8231,\n",
      "        19556, 16941, 15681, 16716,   102])\n",
      "entity_list: ['sequence modeling', 'recurrent']\n",
      "entity_token: [tensor([ 4954, 13117]), tensor([ 1231, 21754])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: RECURRENT AND RECURSIVE NETS where we seethat the state now containsinformation about the whole\n",
      "content_token: tensor([  101,   155,  8231, 19556, 16941, 15681, 16716,   155,  8231, 19556,\n",
      "        13882, 17145, 26546, 11365,  1187,  1195,  1267,  7702,  1204,  1103,\n",
      "         1352,  1208,  2515,  1394, 24152,  1164,  1103,  2006,   102])\n",
      "entity_list: ['RECURRENT AND RECURSIVE NETS']\n",
      "entity_token: [tensor([  155,  8231, 19556, 16941, 15681, 16716,   155,  8231, 19556, 13882,\n",
      "        17145, 26546, 11365])]\n",
      "label: tensor([0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: about the whole pastsequence. Recurrent neural networks can be built in many different ways. Much\n",
      "content_token: tensor([  101,  1164,  1103,  2006,  1763,  2217, 25113,   119, 11336, 21754,\n",
      "        18250,  6379,  1169,  1129,  1434,  1107,  1242,  1472,  3242,   119,\n",
      "         6335,   102])\n",
      "entity_list: ['Recurrent neural networks']\n",
      "entity_token: [tensor([11336, 21754, 18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: ways. Much as almost any function can be considered a feedforward neural network, essentially any\n",
      "content_token: tensor([  101,  3242,   119,  6335,  1112,  1593,  1251,  3053,  1169,  1129,\n",
      "         1737,   170,  4877, 14467,  1197,  5984, 18250,  2443,   117,  7588,\n",
      "         1251,   102])\n",
      "entity_list: ['feedforward neural network']\n",
      "entity_token: [tensor([ 4877, 14467,  1197,  5984, 18250,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: essentially any function involving recurrence can be considered a recurrent neural network. Many\n",
      "content_token: tensor([  101,  7588,  1251,  3053,  5336,  1231, 10182, 21629,  1169,  1129,\n",
      "         1737,   170,  1231, 21754, 18250,  2443,   119,  2408,   102])\n",
      "entity_list: ['recurrent neural network']\n",
      "entity_token: [tensor([ 1231, 21754, 18250,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: network. Many recurrent neural networks use equation 10.5 or a similar equation to define the\n",
      "content_token: tensor([  101,  2443,   119,  2408,  1231, 21754, 18250,  6379,  1329,  8381,\n",
      "         1275,   119,   126,  1137,   170,  1861,  8381,  1106,  9410,  1103,\n",
      "          102])\n",
      "entity_list: ['recurrent neural networks']\n",
      "entity_token: [tensor([ 1231, 21754, 18250,  6379])]\n",
      "label: tensor([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: to define the values of their hidden units. To indicate that the state is the hidden units of the\n",
      "content_token: tensor([ 101, 1106, 9410, 1103, 4718, 1104, 1147, 4610, 2338,  119, 1706, 5057,\n",
      "        1115, 1103, 1352, 1110, 1103, 4610, 2338, 1104, 1103,  102])\n",
      "entity_list: ['hidden units']\n",
      "entity_token: [tensor([4610, 2338])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: hidden units of the network, we now rewrite equation 10.4 using the variable h to represent the\n",
      "content_token: tensor([  101,  4610,  2338,  1104,  1103,  2443,   117,  1195,  1208,  1231,\n",
      "         2246, 10587,  8381,  1275,   119,   125,  1606,  1103,  7898,   177,\n",
      "         1106,  4248,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: h to represent the state: h(t) = f(h(t 1) ,x(t) ;θ), (10.5) − illustrated in figure 10.2, typical\n",
      "content_token: tensor([ 101,  177, 1106, 4248, 1103, 1352,  131,  177,  113,  189,  114,  134,\n",
      "         175,  113,  177,  113,  189,  122,  114,  117,  193,  113,  189,  114,\n",
      "         132,  425,  114,  117,  113, 1275,  119,  126,  114,  851, 8292, 1107,\n",
      "        2482, 1275,  119,  123,  117, 4701,  102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: 10.2, typical RNNs will add extra architectural features such as output layers that read\n",
      "content_token: tensor([ 101, 1275,  119,  123,  117, 4701,  155, 2249, 2249, 1116, 1209, 5194,\n",
      "        3908, 6645, 1956, 1216, 1112, 5964, 8798, 1115, 2373,  102])\n",
      "entity_list: ['RNNs']\n",
      "entity_token: [tensor([ 155, 2249, 2249, 1116])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: layers that read information out of the state h to make predictions. When the recurrentnetwork is\n",
      "content_token: tensor([  101,  8798,  1115,  2373,  1869,  1149,  1104,  1103,  1352,   177,\n",
      "         1106,  1294, 23770,   119,  1332,  1103,  1231, 21754,  6097,  5361,\n",
      "         1110,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: recurrentnetwork is trainedto perform a task that requirespredicting the future from the past, the\n",
      "content_token: tensor([  101,  1231, 21754,  6097,  5361,  1110,  3972,  2430,  3870,   170,\n",
      "         4579,  1115,  5315,  1643,  4359, 17882,  1158,  1103,  2174,  1121,\n",
      "         1103,  1763,   117,  1103,   102])\n",
      "entity_list: ['recurrent network']\n",
      "entity_token: [tensor([ 1231, 21754,  2443])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: from the past, the network typically learns to use h(t) as a kind of lossy summary of the\n",
      "content_token: tensor([  101,  1121,  1103,  1763,   117,  1103,  2443,  3417, 10123,  1106,\n",
      "         1329,   177,   113,   189,   114,  1112,   170,  1912,  1104,  2445,\n",
      "         1183, 14940,  1104,  1103,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: summary of the task-relevant aspects of the past sequence of inputs up to t. This summary is in\n",
      "content_token: tensor([  101, 14940,  1104,  1103,  4579,   118,  7503,  5402,  1104,  1103,\n",
      "         1763,  4954,  1104, 22743,  1146,  1106,   189,   119,  1188, 14940,\n",
      "         1110,  1107,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: This summary is in general necessarily lossy, since it maps an arbitrary length sequence (x(t),x(t\n",
      "content_token: tensor([  101,  1188, 14940,  1110,  1107,  1704,  9073,  2445,  1183,   117,\n",
      "         1290,  1122,  7415,  1126, 16439,  2251,  4954,   113,   193,   113,\n",
      "          189,   114,   117,   193,   113,   189,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: sequence (x(t),x(t 1),x(t 2),...,x(2),x(1)) to a fixed length vector h(t). Depending on the − −\n",
      "content_token: tensor([  101,  4954,   113,   193,   113,   189,   114,   117,   193,   113,\n",
      "          189,   122,   114,   117,   193,   113,   189,   123,   114,   117,\n",
      "          119,   119,   119,   117,   193,   113,   123,   114,   117,   193,\n",
      "          113,   122,   114,   114,  1106,   170,  4275,  2251,  9479,   177,\n",
      "          113,   189,   114,   119, 19285,  1113,  1103,   851,   851,   102])\n",
      "entity_list: ['fixed length vector']\n",
      "entity_token: [tensor([4275, 2251, 9479])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: on the − − training criterion, this summary might selectively keep some aspects of the past\n",
      "content_token: tensor([  101,  1113,  1103,   851,   851,  2013, 26440,   117,  1142, 14940,\n",
      "         1547, 14930,  1193,  1712,  1199,  5402,  1104,  1103,  1763,   102])\n",
      "entity_list: []\n",
      "entity_token: []\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: aspects of the past sequence with more precision than other aspects. For example, if the RNN is\n",
      "content_token: tensor([  101,  5402,  1104,  1103,  1763,  4954,  1114,  1167, 13218,  1190,\n",
      "         1168,  5402,   119,  1370,  1859,   117,  1191,  1103,   155,  2249,\n",
      "         2249,  1110,   102])\n",
      "entity_list: ['RNN']\n",
      "entity_token: [tensor([ 155, 2249, 2249])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: if the RNN is used in statistical language modeling, typically to predict the next word given\n",
      "content_token: tensor([  101,  1191,  1103,   155,  2249,  2249,  1110,  1215,  1107, 11435,\n",
      "         1846, 13117,   117,  3417,  1106, 17163,  1103,  1397,  1937,  1549,\n",
      "          102])\n",
      "entity_list: ['RNN', 'statistical language modeling']\n",
      "entity_token: [tensor([ 155, 2249, 2249]), tensor([11435,  1846, 13117])]\n",
      "label: tensor([0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "content: the next word given previous words, it may not be necessary to store all of the information in the\n",
      "content_token: tensor([ 101, 1103, 1397, 1937, 1549, 2166, 1734,  117, 1122, 1336, 1136, 1129,\n",
      "        3238, 1106, 2984, 1155, 1104, 1103, 1869, 1107, 1103,  102])\n",
      "entity_list: ['RNN']\n",
      "entity_token: [tensor([ 155, 2249, 2249])]\n",
      "label: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "----------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for dimension 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m content_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10.1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprocess_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 68\u001b[0m, in \u001b[0;36mprocess_content\u001b[1;34m(content_dict, content_list)\u001b[0m\n\u001b[0;32m     63\u001b[0m     entity_token\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     64\u001b[0m         tokenizer(e, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     65\u001b[0m     )\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Find position of entities in content\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mfind_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Save the results to a CSV file\u001b[39;00m\n\u001b[0;32m     71\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     72\u001b[0m     [\n\u001b[0;32m     73\u001b[0m         [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     80\u001b[0m )\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mfind_position\u001b[1;34m(content_token, entity_token)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(content_token) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(entity) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(content_token[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(entity)] \u001b[38;5;241m==\u001b[39m entity):\n\u001b[1;32m---> 20\u001b[0m             \u001b[43mposition\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     21\u001b[0m             position[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(entity)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m position\n",
      "\u001b[1;31mIndexError\u001b[0m: index 20 is out of bounds for dimension 0 with size 20"
     ]
    }
   ],
   "source": [
    "content_list = [\"20.1\", \"5.2\", \"7.1\", \"9.1\", \"5.1\", \"7.2\", \"8.1\", \"10.1\"]\n",
    "process_content(content_dict, content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "35ee6242-c675-42e5-b346-241bbbc10db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:30:24.565791Z",
     "iopub.status.busy": "2024-02-21T10:30:24.565791Z",
     "iopub.status.idle": "2024-02-21T10:30:24.570139Z",
     "shell.execute_reply": "2024-02-21T10:30:24.570139Z",
     "shell.execute_reply.started": "2024-02-21T10:30:24.565791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataDir + \"relations/\" + \"sample.csv\", index_col=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

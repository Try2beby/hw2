{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased', cache_dir=\"../../../BERT/large\")\n",
    "model = BertModel.from_pretrained(\"bert-large-cased\", cache_dir=\"../../../BERT/large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./relations/sample.csv')\n",
    "df['label'] = df['label'].apply(lambda x: x[8:-2].replace('\\n','').replace(' ','').split(\",\"))\n",
    "df['label'] = df['label'].apply(lambda x: [int(i) for i in x])\n",
    "df['text'] = df['text'].apply(lambda x: x[2: -2].split(\"', '\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data['text'][index]\n",
    "        labels = self.data['label'][index]\n",
    "\n",
    "        # Convert tokens to token IDs\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(i.replace(' ','') for i in text)\n",
    "\n",
    "        # Create input tensors\n",
    "        input_ids = torch.tensor(token_ids)\n",
    "        label_ids = torch.tensor(labels)\n",
    "\n",
    "        return input_ids, label_ids\n",
    "\n",
    "# Create the NER dataset\n",
    "ner_dataset = NERDataset(df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the training dataset and dataloader\n",
    "train_dataset = NERDataset(train_df, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the validation dataset and dataloader\n",
    "val_dataset = NERDataset(val_df, tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3084, 2393, 119, 117, 1368, 114, 1110, 1126, 1859, 1104, 170, 3594, 7305, 1115, 7743, 11552, 1116, 1105, 19428, 21225, 1116, 1242, 102]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'et',\n",
       " 'al',\n",
       " '.',\n",
       " ',',\n",
       " '2012',\n",
       " ')',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'a',\n",
       " 'software',\n",
       " 'package',\n",
       " 'that',\n",
       " 'automatically',\n",
       " 'detect',\n",
       " '##s',\n",
       " 'and',\n",
       " 'stab',\n",
       " '##ilize',\n",
       " '##s',\n",
       " 'many',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "Deep learning": {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "summary": "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog."
    },
    "Machine learning": {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods."
    },
    "Statistics": {
        "url": "https://en.wikipedia.org/wiki/Statistics",
        "summary": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\n"
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Learning": {
        "url": "https://en.wikipedia.org/wiki/Learning",
        "summary": "Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, animals, and some machines; there is also evidence for some kind of learning in certain plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulate from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be \"lost\" from that which cannot be retrieved.Human learning starts at birth (it might even start before in terms of an embryo's need for both interaction with, and freedom within its environment within the womb.) and continues until death as a consequence of ongoing interactions between people and their environment. The nature and processes involved in learning are studied in many established fields (including educational psychology, neuropsychology, experimental psychology, cognitive sciences, and pedagogy), as well as emerging fields of knowledge (e.g. with a shared interest in the topic of learning from safety events such as incidents/accidents, or in collaborative learning health systems). Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event cannot be avoided or escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development.Play has been approached by several theorists as a form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games. For Vygotsky, however, play is the first form of learning language and communication, and the stage where a child begins to understand rules and symbols. This has led to a view that learning in organisms is always related to semiosis, and is often associated with representational systems/activity."
    },
    "Google Street View": {
        "url": "https://en.wikipedia.org/wiki/Google_Street_View",
        "summary": "Google Street View is a technology featured in Google Maps and Google Earth that provides interactive panoramas from positions along many streets in the world. It was launched in 2007 in several cities in the United States, and has since expanded to include all of the country's major and minor cities, as well as cities and rural areas of many other countries worldwide. Streets with Street View imagery available are shown as blue lines on Google Maps.\nGoogle Street View displays interactively panoramas of stitched VR photographs. Most photography is done by car, but some is done by tricycle, camel, boat, snowmobile, underwater apparatus, and on foot."
    },
    "Speech recognition": {
        "url": "https://en.wikipedia.org/wiki/Speech_recognition",
        "summary": "Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\nSome speech recognition systems require \"solly\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker-independent\" systems. Systems that use training are called \"speaker dependent\".\nSpeech recognition applications include voice user interfaces such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).\nThe term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\nFrom the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems."
    },
    "Metadata": {
        "url": "https://en.wikipedia.org/wiki/Metadata",
        "summary": "Metadata (or metainformation) is \"data that provides information about other data\", but not the content of the data itself, such as the text of a message or the image itself. There are many distinct types of metadata, including:\n\nDescriptive metadata \u2013 the descriptive information about a resource. It is used for discovery and identification. It includes elements such as title, abstract, author, and keywords.\nStructural metadata \u2013 metadata about containers of data and indicates how compound objects are put together, for example, how pages are ordered to form chapters. It describes the types, versions, relationships, and other characteristics of digital materials.\nAdministrative metadata \u2013 the information to help manage a resource, like resource type, permissions, and when and how it was created.\nReference metadata \u2013 the information about the contents and quality of statistical data.\nStatistical metadata \u2013 also called process data, may describe processes that collect, process, or produce statistical data.\nLegal metadata \u2013 provides information about the creator, copyright holder, and public licensing, if provided.Metadata is not strictly bound to one of these categories, as it can describe a piece of data in many other ways.\n\n"
    },
    "Information": {
        "url": "https://en.wikipedia.org/wiki/Information",
        "summary": "Information is an abstract concept that refers to that which has the power to inform.  At the most fundamental level, information pertains to the interpretation (perhaps formally) of that which may be sensed, or their abstractions.  Any natural process that is not completely random and any observable pattern in any medium can be said to convey some amount of information.  Whereas digital signals and other data use discrete signs to convey information, other phenomena and artifacts such as analogue signals, poems, pictures, music or other sounds, and currents convey information in a more continuous form.  Information is not knowledge itself, but the meaning that may be derived from a representation through interpretation.The concept of information is relevant or connected to various concepts, including constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, proposition, representation, and entropy.\nInformation is often processed iteratively: Data available at one step are processed into information to be interpreted and processed at the next step.  For example, in written text each symbol or letter conveys information relevant to the word it is part of, each word conveys information relevant to the phrase it is part of, each phrase conveys information relevant to the sentence it is part of, and so on until at the final step information is interpreted and becomes knowledge in a given domain.  In a digital signal, bits may be interpreted into the symbols, letters, numbers, or structures that convey the information available at the next level up.  The key characteristic of information is that it is subject to interpretation and processing.\nThe derivation of information from a signal or message may be thought of as the resolution of ambiguity or uncertainty that arises during the interpretation of patterns within the signal or message.Information may be structured as data. Redundant data can be compressed up to an optimal size, which is the theoretical limit of compression.\nThe information available through a collection of data may be derived by analysis.  For example, a restaurant collects data from every customer order.  That information may be analyzed to produce knowledge that is put to use when the business subsequently wants to identify the most popular or least popular dish.Information can be transmitted in time, via data storage, and space, via communication and telecommunication. Information is expressed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, all information is always conveyed as the content of a message.\nInformation can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication.\nThe uncertainty of an event is measured by its probability of occurrence. Uncertainty is inversely proportional to the probability of occurrence. Information theory takes advantage of this by concluding that more uncertain events require more information to resolve their uncertainty. The bit is a typical unit of information. It is 'that which reduces uncertainty by half'. Other units such as the nat may be used. For example, the information encoded in one \"fair\" coin flip is log2(2/1) = 1 bit, and in two fair coin flips is log2(4/1) = 2 bits. A 2011 Science article estimates that 97% of technologically stored information was already in digital bits in 2007 and that the year 2002 was the beginning of the digital age for information storage (with digital storage capacity bypassing analogue for the first time)."
    },
    "Prediction": {
        "url": "https://en.wikipedia.org/wiki/Prediction",
        "summary": "A prediction (Latin pr\u00e6-, \"before,\" and dicere, \"to say\"), or forecast, is a statement about a future event or data. They are often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference from \"estimation\"; different authors and disciplines ascribe different connotations.\nFuture events are necessarily uncertain, so guaranteed accurate information about the future is impossible. Prediction can be useful to assist in making plans about possible developments."
    },
    "Unsupervised": {
        "url": "https://en.wikipedia.org/wiki/Unsupervised",
        "summary": "Unsupervised is an American adult animated sitcom created by David Hornsby, Rob Rosell, and Scott Marder which ran on FX from January 19 to December 20, 2012. The show was created, and for the most part, written by David Hornsby, Scott Marder, and Rob Rosell.On November 17, 2012, the series was canceled after one season."
    },
    "Data set": {
        "url": "https://en.wikipedia.org/wiki/Data_set",
        "summary": "A data set (or dataset) is a collection of data. In the case of tabular data, a data set corresponds to one or more database tables, where every column of a table represents a particular variable, and each row corresponds to a given record of the data set in question. The data set lists values for each of the variables, such as for example height and weight of an object, for each member of the data set.  Data sets can also consist of a collection of documents or files.In the open data discipline, data set is the unit to measure the information released in a public open data repository.  The European data.europa.eu portal aggregates more than a million data sets."
    },
    "Supervised learning": {
        "url": "https://en.wikipedia.org/wiki/Supervised_learning",
        "summary": "Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n\n"
    },
    "Unsupervised learning": {
        "url": "https://en.wikipedia.org/wiki/Unsupervised_learning",
        "summary": "Unsupervised learning is a paradigm in machine learning where, in contrast to supervised learning and semi-supervised learning, algorithms learn patterns exclusively from unlabeled data.\n\n"
    },
    "Linear regression": {
        "url": "https://en.wikipedia.org/wiki/Linear_regression",
        "summary": "In statistics, linear regression is a linear approach for modelling a predictive relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables), which are measured without error. The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.If the explanatory variables are measured with error then errors-in-variables models are required, also known as measurement error models.\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n\nIf the goal is error reduction in prediction or forecasting, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\nIf the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Use of the Mean Squared Error(MSE) as the cost on a dataset that has many large outliers, can result in a model that fits the outliers more than the true data due to the higher importance assigned by MSE to large errors. So a cost functions that are robust to outliers should be used if the dataset has many large outliers.  Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous."
    },
    "Training": {
        "url": "https://en.wikipedia.org/wiki/Training",
        "summary": "Training is teaching, or developing in oneself or others, any skills and knowledge  or fitness that relate to specific useful competencies. Training has specific goals of improving one's capability, capacity, productivity and performance. It forms the core of apprenticeships and provides the backbone of content at institutes of technology (also known as technical colleges or polytechnics). In addition to the basic training required for a trade, occupation or profession, training may continue beyond initial competence to maintain, upgrade and update skills throughout working life. People within some professions and occupations may refer to this sort of training as professional development. Training also refers to the development of physical fitness related to a specific competence, such as sport, martial arts, military applications and some other occupations."
    },
    "Robustness": {
        "url": "https://en.wikipedia.org/wiki/Robustness",
        "summary": "Robustness is the property of being strong and healthy in constitution. When it is transposed into a system, it refers to the ability of tolerating perturbations that might affect the system's functional body. In the same line robustness can be defined as \"the ability of a system to resist change without adapting its initial stable configuration\". \n\"Robustness in the small\" refers to situations wherein perturbations are small in magnitude, which considers that the \"small\" magnitude hypothesis can be difficult to verify because \"small\" or \"large\" depends on the specific problem. Conversely, \"Robustness in the large problem\" refers to situations wherein no assumptions can be made about the magnitude of perturbations, which can either be small or large.\nIt has been discussed that robustness has two dimensions: resistance and avoidance.\n\n"
    },
    "Minimally invasive procedure": {
        "url": "https://en.wikipedia.org/wiki/Minimally_invasive_procedure",
        "summary": "Minimally invasive procedures (also known as minimally invasive surgeries) encompass surgical techniques that limit the size of incisions needed, thereby reducing wound healing time, associated pain, and risk of infection. Surgery by definition is invasive and many operations requiring incisions of some size are referred to as open surgery. Incisions made during open surgery can sometimes leave large wounds that may be painful and take a long time to heal. Advancements in medical technologies have enabled the development and regular use of minimally invasive procedures. For example, endovascular aneurysm repair, a minimally invasive surgery, has become the most common method of repairing abdominal aortic aneurysms in the US as of 2003. The procedure involves much smaller incisions than the corresponding open surgery procedure of open aortic surgery.Interventional radiologists were the forerunners of minimally invasive procedures. Using imaging techniques, radiologists were able to direct interventional instruments through the body by way of catheters instead of the large incisions needed in traditional surgery. As a result, many conditions once requiring surgery can now be treated non-surgically.Diagnostic techniques that do not involve incisions, puncturing the skin, or the introduction of foreign objects or materials into the body are known as non-invasive procedures. Several treatment procedures are classified as non-invasive. A major example of a non-invasive alternative treatment to surgery is radiation therapy, also called radiotherapy."
    },
    "Overfitting": {
        "url": "https://en.wikipedia.org/wiki/Overfitting",
        "summary": "In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.:\u200a45\u200aUnderfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.\nThe possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend. \nAs an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. \nThe potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.\nTo lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter."
    },
    "Misfit stream": {
        "url": "https://en.wikipedia.org/wiki/Misfit_stream",
        "summary": "A misfit stream is  a river that is either too large or too small to have eroded the valley or cave passage in which it flows. This term is also used for a stream or river with meanders that obviously are not proportional in size to the meanders of the valley or meander scars cut into its valley walls. If the misfit stream is too large for either its valley or meanders, it is known as an overfit stream. If the misfit stream is too small for either its valley or meanders, it is known as an underfit stream.The term misfit stream is often incorrectly used as a synonym for an underfit stream. An underfit stream is a type of misfit stream whose discharge is too small to be correlated with either existing channel characteristics, i.e. meander radius, wavelength and channel width, or valley size"
    },
    "Model": {
        "url": "https://en.wikipedia.org/wiki/Model",
        "summary": "A model is an informative representation of an object, person or system. The term originally denoted the plans of a building in late 16th-century English, and derived via French and Italian ultimately from Latin modulus, a measure.Models can be divided into physical models (e.g. a ship model or a fashion model) and abstract models (e.g. a set of mathematical equations describing the workings of the atmosphere for the purpose of weather forecasting). Abstract or conceptual models are central to philosophy of science.In scholarly research and applied science, a model should not be confused with a theory: while a model seeks only to represent reality with the purpose of better understanding or predicting the world, a theory is more ambitious in that it claims to be an explanation of reality."
    },
    "Optimization problem": {
        "url": "https://en.wikipedia.org/wiki/Optimization_problem",
        "summary": "In mathematics, engineering, computer science and economics, an optimization problem is the problem of finding the best solution from all feasible solutions.\nOptimization problems can be divided into two categories, depending on whether the variables are continuous or discrete: \n\nAn optimization problem with discrete variables is known as a discrete optimization, in which an object such as an integer, permutation or graph must be found from a countable set.\nA problem with continuous variables is known as a continuous optimization, in which an optimal value from a continuous function must be found. They can include constrained problems and multimodal problems.\n\n"
    },
    "Vapnik\u2013Chervonenkis dimension": {
        "url": "https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension",
        "summary": "In Vapnik\u2013Chervonenkis theory, the Vapnik\u2013Chervonenkis (VC) dimension is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a set of functions that can be learned by a statistical binary classification algorithm. It is defined as the cardinality of the largest set of points that the algorithm can shatter, which means the algorithm can always learn a perfect classifier for any labeling of at least one configuration of those data points. It was originally defined by Vladimir Vapnik and Alexey Chervonenkis.Informally, the capacity of a classification model is related to how complicated it can be. For example, consider the thresholding of a high-degree polynomial: if the polynomial evaluates above zero, that point is classified as positive, otherwise as negative. A high-degree polynomial can be wiggly, so it can fit a given set of training points well. But one can expect that the classifier will make errors on other points, because it is too wiggly. Such a polynomial has a high capacity. A much simpler alternative is to threshold a linear function. This function may not fit the training set well, because it has a low capacity. This notion of capacity is made rigorous below."
    },
    "Statistical learning theory": {
        "url": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
        "summary": "Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics."
    },
    "Parametric model": {
        "url": "https://en.wikipedia.org/wiki/Parametric_model",
        "summary": "In statistics, a parametric model or parametric family or finite-dimensional model is a particular class of statistical models. Specifically, a parametric model is a family of probability distributions that has a finite number of parameters."
    },
    "Nonparametric statistics": {
        "url": "https://en.wikipedia.org/wiki/Nonparametric_statistics",
        "summary": "Nonparametric statistics is a type of statistical analysis that does not rely on the assumption of a specific underlying distribution (such as the normal distribution), or any other specific assumptions about the population parameters (such as mean and variance). This is in contrast to parametric statistics, which make such assumptions about the population. In nonparametric statistics, a distribution may not be specified at all, or a distribution may be specified but its parameters, such as the mean and variance, are not assumed to have a known value or distribution in advance. In some cases, parameters may be generated from the data, such as the median. Nonparametric statistics can be used for descriptive statistics or statistical inference. Nonparametric tests are often used when the assumptions of parametric tests are evidently violated."
    },
    "Mathematical optimization": {
        "url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
        "summary": "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.\n\n"
    },
    "Hyperparameter": {
        "url": "https://en.wikipedia.org/wiki/Hyperparameter",
        "summary": "In Bayesian statistics, a hyperparameter is a parameter of a prior distribution; the term is used to distinguish them from parameters of the model for the underlying system under analysis.\nFor example, if one is using a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then:\n\np is a parameter of the underlying system (Bernoulli distribution), and\n\u03b1 and \u03b2 are parameters of the prior distribution (beta distribution), hence hyperparameters.One may take a single value for a given hyperparameter, or one can iterate and take a probability distribution on the hyperparameter itself, called a hyperprior."
    },
    "Training, validation, and test data sets": {
        "url": "https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets",
        "summary": "In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided into multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation, and test sets.\nThe model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.\nSuccessively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units\u2014layers and layer widths\u2014in a neural network). Validation datasets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set).\nThis simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term \"validation set\" is sometimes used instead of \"test set\" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available."
    },
    "Data": {
        "url": "https://en.wikipedia.org/wiki/Data",
        "summary": "In common usage and statistics, data (US: ; UK: ) is a collection of discrete or continuous values that convey information, describing the quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted formally.  A datum is an individual value in a collection of data. Data is usually organized into structures such as tables that provide additional context and meaning, and which may themselves be used as data in larger structures. Data may be used as variables in a computational process. Data may represent abstract ideas or concrete measurements.\nData is commonly used in scientific research, economics, and in virtually every other form of human organizational activity.  Examples of data sets include price indices (such as consumer price index), unemployment rates, literacy rates, and census data. In this context, data represents the raw facts and figures from which useful information can be extracted. \nData is collected using techniques such as measurement, observation, query, or analysis, and is typically represented as numbers or characters which may be further processed. Field data is data that is collected in an uncontrolled in-situ environment. Experimental data is data that is generated in the course of a controlled scientific experiment.  Data is analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis.  Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed and obvious instrument or data entry errors are corrected.\nData can be seen as the smallest units of factual information that can be used as a basis for calculation, reasoning, or discussion. Data can range from abstract ideas to concrete measurements, including, but not limited to, statistics. Thematically connected data presented in some relevant context can be viewed as information. Contextually connected pieces of information can then be described as data insights or intelligence. The stock of insights and intelligence that accumulates over time resulting from the synthesis of data into information, can then be described as knowledge. Data has been described as \"the new oil of the digital economy\". Data, as a general concept, refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.\nAdvances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible. (Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.) In response, the relatively new field of data science uses machine learning (and other artificial intelligence (AI)) methods that allow for efficient applications of analytic methods to big data."
    },
    "Unit of observation": {
        "url": "https://en.wikipedia.org/wiki/Unit_of_observation",
        "summary": "In statistics, a unit of observation is the unit described by the data that one analyzes. A study may treat groups as a unit of observation with a country as the unit of analysis, drawing conclusions on group characteristics from data collected at the national level. For example, in a study of the demand for money, the unit of observation might be chosen as the individual, with different observations (data points) for a given point in time differing as to which individual they refer to; or the unit of observation might be the country, with different observations differing only in regard to the country they refer to.\n\n"
    },
    "Bias": {
        "url": "https://en.wikipedia.org/wiki/Bias",
        "summary": "Bias is a disproportionate weight in favor of or against an idea or thing, usually in a way that is closed-minded, prejudicial, or unfair. Biases can be innate or learned. People may develop biases for or against an individual, a group, or a belief. In science and engineering, a bias is a systematic error. Statistical bias results from an unfair sampling of a population, or from an estimation process that does not give accurate results on average."
    },
    "Estimator": {
        "url": "https://en.wikipedia.org/wiki/Estimator",
        "summary": "In statistics, an estimator is a rule for calculating an estimate of a given quantity based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished. For example, the sample mean is a commonly used estimator of the population mean.\nThere are point and interval estimators. The point estimators yield single-valued results. This is in contrast to an interval estimator, where the result would be a range of plausible values. \"Single value\" does not necessarily mean \"single number\", but includes vector valued or function valued estimators.\nEstimation theory is concerned with the properties of estimators; that is, with defining properties that can be used to compare different estimators (different rules for creating estimates) for the same quantity, based on the same data. Such properties can be used to determine the best rules to use under given circumstances. However, in robust statistics, statistical theory goes on to consider the balance between having good properties, if tightly defined assumptions hold, and having less good properties that hold under wider conditions."
    },
    "Sigma": {
        "url": "https://en.wikipedia.org/wiki/Sigma",
        "summary": "Sigma  ( SIG-m\u0259; uppercase \u03a3, lowercase \u03c3, lowercase in word-final position \u03c2; Greek: \u03c3\u03af\u03b3\u03bc\u03b1) is the eighteenth letter of the Greek alphabet. In the system of Greek numerals, it has a value of 200. In general mathematics, uppercase \u03a3 is used as an operator for summation. When used at the end of a letter-case word (one that does not use all caps), the final form (\u03c2) is used. In \u1f48\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2 (Odysseus), for example, the two lowercase sigmas (\u03c3) in the center of the name are distinct from the word-final sigma (\u03c2) at the end. The Latin letter S derives from sigma while the Cyrillic letter Es derives from a lunate form of this letter."
    },
    "Normal distribution": {
        "url": "https://en.wikipedia.org/wiki/Normal_distribution",
        "summary": "In statistics, a normal distribution or Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              \u03c3\n              \n                \n                  2\n                  \u03c0\n                \n              \n            \n          \n        \n        \n          e\n          \n            \u2212\n            \n              \n                1\n                2\n              \n            \n            \n              \n                (\n                \n                  \n                    \n                      x\n                      \u2212\n                      \u03bc\n                    \n                    \u03c3\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\n  The parameter \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   is the mean or expectation of the distribution (and also its median and mode), while the parameter \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   is its standard deviation. The variance of the distribution is \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  . A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate.\nNormal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable\u2014whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal.Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies. For instance, any linear combination of a fixed collection of independent normal deviates is a normal deviate. Many results and methods, such as propagation of uncertainty and least squares parameter fitting, can be derived analytically in explicit form when the relevant variables are normally distributed.\nA normal distribution is sometimes informally called a bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions). For other names, see Naming.\nThe univariate probability distribution is generalized for vectors in the multivariate normal distribution and for matrices in the matrix normal distribution."
    },
    "Variance": {
        "url": "https://en.wikipedia.org/wiki/Variance",
        "summary": "In probability theory and statistics, variance is the squared deviation from the mean of a random variable. The standard deviation is obtained as the square root of the variance. Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. It is the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  , \n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle s^{2}}\n  , \n  \n    \n      \n        Var\n        \u2061\n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\operatorname {Var} (X)}\n  , \n  \n    \n      \n        V\n        (\n        X\n        )\n      \n    \n    {\\displaystyle V(X)}\n  , or \n  \n    \n      \n        \n          V\n        \n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\mathbb {V} (X)}\n  .An advantage of variance as a measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation; for example, the variance of a sum of uncorrelated random variables is equal to the sum of their variances. A disadvantage of the variance for practical applications is that, unlike the standard deviation, its units differ from the random variable, which is why the standard deviation is more commonly reported as a measure of dispersion once the calculation is finished.\nThere are two distinct concepts that are both called \"variance\". One, as discussed above, is part of a theoretical probability distribution and is defined by an equation. The other variance is a characteristic of a set of observations. When variance is calculated from observations, those observations are typically measured from a real world system. If all possible observations of the system are present then the calculated variance is called the population variance. Normally, however, only a subset is available, and the variance calculated from this is called the sample variance. The variance calculated from a sample is considered an estimate of the full population variance. There are multiple ways to calculate an estimate of the population variance, as discussed in the section below.\nThe two kinds of variance are closely related. To see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations. If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance. Variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling."
    },
    "Probability distribution": {
        "url": "https://en.wikipedia.org/wiki/Probability_distribution",
        "summary": "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).For instance, if X is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). More commonly, probability distributions are used to compare the relative occurrence of many different random values.\nProbability distributions can be defined in different ways and for discrete or for continuous variables. Distributions with special properties or for especially important applications are given specific names."
    },
    "Maximum likelihood estimation": {
        "url": "https://en.wikipedia.org/wiki/Maximum_likelihood_estimation",
        "summary": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.If the likelihood function is differentiable, the derivative test for finding maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved analytically; for instance, the ordinary least squares estimator for a linear regression model maximizes the likelihood when the random errors are assumed to have normal distributions with the same variance.From the perspective of Bayesian inference, MLE is generally equivalent to maximum a posteriori (MAP) estimation with uniform prior distributions (or a normal prior distribution with a standard deviation of infinity). In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood.\n\n"
    },
    "A priori and a posteriori": {
        "url": "https://en.wikipedia.org/wiki/A_priori_and_a_posteriori",
        "summary": "A priori ('from the earlier') and a posteriori ('from the later') are Latin phrases used in philosophy to distinguish types of knowledge, justification, or argument by their reliance on experience. A priori knowledge is independent from any experience. Examples include mathematics, tautologies and deduction from pure reason. A posteriori knowledge depends on empirical evidence. Examples include most fields of science and aspects of personal knowledge.\nThe terms originate from the analytic methods found in Organon, a collection of works by Aristotle. Prior analytics (a priori) is about deductive logic, which comes from definitions and first principles. Posterior analytics (a posteriori) is about inductive logic, which comes from observational evidence.\nBoth terms appear in Euclid's Elements and were popularized by Immanuel Kant's Critique of Pure Reason, an influential work in the history of philosophy. Both terms are primarily used as modifiers to the noun knowledge (i.e., a priori knowledge). A priori can be used to modify other nouns such as truth. Philosophers may use apriority, apriorist and aprioricity as nouns referring to the quality of being a priori."
    },
    "Probability": {
        "url": "https://en.wikipedia.org/wiki/Probability",
        "summary": "Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true.  The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes ('heads' and 'tails') are both equally probable; the probability of 'heads' equals the probability of 'tails'; and since no other outcomes are possible, the probability of either 'heads' or 'tails' is 1/2 (which could also be written as 0.5 or 50%).\nThese concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.\n\n"
    },
    "Bayesian linear regression": {
        "url": "https://en.wikipedia.org/wiki/Bayesian_linear_regression",
        "summary": "Bayesian linear regression is a type of conditional modeling in which the mean of one variable is described by a linear combination of other variables, with the goal of obtaining the posterior probability of the regression coefficients (as well as other parameters describing the distribution of the regressand) and ultimately allowing the out-of-sample prediction of the regressand (often labelled \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  ) conditional on observed values of the regressors (usually \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  ). The simplest and most widely used version of this model is the normal linear model, in which \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is distributed Gaussian. In this model, and under a particular choice of prior probabilities for the parameters\u2014so-called conjugate priors\u2014the posterior can be found analytically. With more arbitrarily chosen priors, the posteriors generally have to be approximated."
    },
    "Alpha": {
        "url": "https://en.wikipedia.org/wiki/Alpha",
        "summary": "Alpha  (uppercase \u0391, lowercase \u03b1; Ancient Greek: \u1f04\u03bb\u03c6\u03b1, \u00e1lpha, or Greek: \u03ac\u03bb\u03c6\u03b1, romanized: \u00e1lfa) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for \"ox\". Letters that arose from alpha include the Latin letter A and the Cyrillic letter \u0410."
    },
    "Lambda": {
        "url": "https://en.wikipedia.org/wiki/Lambda",
        "summary": "Lambda (; uppercase \u039b, lowercase \u03bb; Greek: \u03bb\u03ac\u03bc(\u03b2)\u03b4\u03b1, l\u00e1m(b)da) is the eleventh letter of the Greek alphabet, representing the voiced alveolar lateral approximant IPA: [l]. In the system of Greek numerals, lambda has a value of 30. Lambda is derived from the Phoenician Lamed . Lambda gave rise to the Latin L and the Cyrillic El (\u041b). The ancient grammarians and dramatists give evidence to the pronunciation as [la\u02d0bda\u02d0] (\u03bb\u03ac\u03b2\u03b4\u03b1) in Classical Greek times. In Modern Greek, the name of the letter, \u039b\u03ac\u03bc\u03b4\u03b1, is pronounced [\u02c8lam.\u00f0a].\nIn early Greek alphabets, the shape and orientation of lambda varied. Most variants consisted of two straight strokes, one longer than the other, connected at their ends. The angle might be in the upper-left, lower-left (\"Western\" alphabets) or top (\"Eastern\" alphabets). Other variants had a vertical line with a horizontal or sloped stroke running to the right. With the general adoption of the Ionic alphabet, Greek settled on an angle at the top; the Romans put the angle at the lower-left.\nThe HTML 4 character entity references for the Greek capital and small letter lambda are &#923; and &#955; respectively. The Unicode code points for lambda are U+039B and U+03BB."
    },
    "Support vector machine": {
        "url": "https://en.wikipedia.org/wiki/Support_vector_machine",
        "summary": "In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\nThe support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters."
    },
    "Logistic regression": {
        "url": "https://en.wikipedia.org/wiki/Logistic_regression",
        "summary": "In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See \u00a7 Background and \u00a7 Definition for formal mathematics, and \u00a7 Example for a worked example.\nBinary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see \u00a7 Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See \u00a7 Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.\nAnalogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see \u00a7 Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the \"simplest\" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see \u00a7 Maximum entropy.\nThe parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see \u00a7 Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see \u00a7 Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined \"logit\"; see \u00a7 History."
    },
    "K-nearest neighbors algorithm": {
        "url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
        "summary": "In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\nA peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data."
    },
    "Decision tree": {
        "url": "https://en.wikipedia.org/wiki/Decision_tree",
        "summary": "A decision tree  is a decision support hierarchical model that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\nDecision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning."
    },
    "Sparse approximation": {
        "url": "https://en.wikipedia.org/wiki/Sparse_approximation",
        "summary": "Sparse approximation (also known as sparse representation) theory deals with sparse solutions for systems of linear equations. Techniques for finding these solutions and exploiting them in applications have found wide use in image processing, signal processing, machine learning, medical imaging, and more."
    },
    "One-dimensional space": {
        "url": "https://en.wikipedia.org/wiki/One-dimensional_space",
        "summary": "A one-dimensional space (1D space) is a mathematical space in which location can be specified with a single coordinate. An example is the number line, each point of which is described by a single real number.Any straight line or smooth curve is a one-dimensional space, regardless of the dimension of the ambient space in which the line or curve is embedded. Examples include the circle on a plane, or a parametric space curve.\nIn algebraic geometry there are several structures that are one-dimensional spaces but are usually referred to by more specific terms. Any field \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n   is a one-dimensional vector space over itself. The projective line over \n  \n    \n      \n        K\n        ,\n      \n    \n    {\\displaystyle K,}\n   denoted \n  \n    \n      \n        \n          \n            P\n          \n          \n            1\n          \n        \n        (\n        K\n        )\n        ,\n      \n    \n    {\\displaystyle \\mathbf {P} ^{1}(K),}\n   is a one-dimensional space. In particular, if the field is the complex numbers \n  \n    \n      \n        \n          C\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbb {C} ,}\n   then the complex projective line \n  \n    \n      \n        \n          \n            P\n          \n          \n            1\n          \n        \n        (\n        \n          C\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {P} ^{1}(\\mathbb {C} )}\n   is one-dimensional with respect to \n  \n    \n      \n        \n          C\n        \n      \n    \n    {\\displaystyle \\mathbb {C} }\n   (but is sometimes called the Riemann sphere, as it is a model of the sphere, two-dimensional with respect to real-number coordinates).\nFor every eigenvector of a linear transformation T on a vector space V, there is a one-dimensional space A \u2282 V generated by the eigenvector such that T(A) = A, that is, A is an invariant set under the action of T.In Lie theory, a one-dimensional subspace of a Lie algebra is mapped to a one-parameter group under the Lie group\u2013Lie algebra correspondence.More generally, a ring is a length-one module over itself. Similarly, the projective line over a ring is a one-dimensional space over the ring. In case the ring is an algebra over a field, these spaces are one-dimensional with respect to the algebra, even if the algebra is of higher dimensionality."
    },
    "Plane (mathematics)": {
        "url": "https://en.wikipedia.org/wiki/Plane_(mathematics)",
        "summary": "In mathematics, a plane is a two-dimensional space or flat surface that extends indefinitely. \nA plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. \nWhen working exclusively in two-dimensional Euclidean space, the definite article is used, so the Euclidean plane refers to the whole space. \nMany fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional or planar space."
    },
    "Mean squared error": {
        "url": "https://en.wikipedia.org/wiki/Mean_squared_error",
        "summary": "In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution).\nThe MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero.\nThe MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."
    },
    "Least squares": {
        "url": "https://en.wikipedia.org/wiki/Least_squares",
        "summary": "The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value and the fitted value provided by a model) made in the results of each individual equation.\nThe most important application is in data fitting. When the problem has substantial uncertainties in the independent variable (the x variable), then simple regression and least-squares methods have problems; in such cases, the methodology required for fitting errors-in-variables models may be considered instead of that for least squares.\nLeast squares problems fall into two categories: linear or ordinary least squares and nonlinear least squares, depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\nPolynomial least squares describes the variance in a prediction of the dependent variable as a function of the independent variable and the deviations from the fitted curve.\nWhen the observations come from an exponential family with identity as its natural sufficient statistics and mild-conditions are satisfied (e.g. for normal, exponential, Poisson and binomial distributions), standardized least-squares estimates and maximum-likelihood estimates are identical. The method of least squares can also be derived as a method of moments estimator.\nThe following discussion is mostly presented in terms of linear functions but the use of least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the least-squares method may be used to fit a generalized linear model.\nThe least-squares method was officially discovered and published by Adrien-Marie Legendre (1805), though it is usually also co-credited to Carl Friedrich Gauss (1809), who contributed significant theoretical advances to the method, and may have also used it in his earlier work in 1794 and 1795."
    },
    "Singular value decomposition": {
        "url": "https://en.wikipedia.org/wiki/Singular_value_decomposition",
        "summary": "In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any \n  \n    \n      \n         \n        m\n        \u00d7\n        n\n         \n      \n    \n    {\\displaystyle \\ m\\times n\\ }\n   matrix. It is related to the polar decomposition.\nSpecifically, the singular value decomposition of an \n  \n    \n      \n         \n        m\n        \u00d7\n        n\n         \n      \n    \n    {\\displaystyle \\ m\\times n\\ }\n   complex matrix M is a factorization of the form \n  \n    \n      \n         \n        \n          M\n        \n        =\n        \n          U\n          \u03a3\n          \n            V\n            \n              \u2217\n            \n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle \\ \\mathbf {M} =\\mathbf {U\\Sigma V^{*}} \\ ,}\n   where U is an \n  \n    \n      \n         \n        m\n        \u00d7\n        m\n         \n      \n    \n    {\\displaystyle \\ m\\times m\\ }\n   complex unitary matrix, \n  \n    \n      \n         \n        \n          \u03a3\n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {\\Sigma } \\ }\n   is an \n  \n    \n      \n         \n        m\n        \u00d7\n        n\n         \n      \n    \n    {\\displaystyle \\ m\\times n\\ }\n   rectangular diagonal matrix with non-negative real numbers on the diagonal, V is an \n  \n    \n      \n        n\n        \u00d7\n        n\n      \n    \n    {\\displaystyle n\\times n}\n   complex unitary matrix, and \n  \n    \n      \n         \n        \n          \n            V\n            \n              \u2217\n            \n          \n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {V^{*}} \\ }\n   is the conjugate transpose of V. Such decomposition always exists for any complex matrix.  If M is real, then U and V can be guaranteed to be real orthogonal matrices; in such contexts, the SVD is often denoted \n  \n    \n      \n         \n        \n          \n            U\n            \u03a3\n            V\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\ \\mathbf {U\\Sigma V} ^{\\operatorname {T} }}\n  .\nThe diagonal entries \n  \n    \n      \n         \n        \n          \u03c3\n          \n            i\n          \n        \n        =\n        \n          \u03a3\n          \n            i\n            i\n          \n        \n         \n      \n    \n    {\\displaystyle \\ \\sigma _{i}=\\Sigma _{ii}\\ }\n   of \n  \n    \n      \n         \n        \n          \u03a3\n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {\\Sigma } \\ }\n   are uniquely determined by M and are known as the singular values of M. The number of non-zero singular values is equal to the rank of M. The columns of U and the columns of V are called left-singular vectors and right-singular vectors of M, respectively. They form two sets of orthonormal bases u1, ..., um  and v1, ..., vn , and if they are sorted so that the singular values \n  \n    \n      \n         \n        \n          \u03c3\n          \n            i\n          \n        \n         \n      \n    \n    {\\displaystyle \\ \\sigma _{i}\\ }\n   with value zero are all in the highest-numbered columns (or rows), the singular value decomposition can be written as \n  \n    \n      \n         \n        \n          M\n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            r\n          \n        \n        \n          \u03c3\n          \n            i\n          \n        \n        \n          \n            u\n          \n          \n            i\n          \n        \n        \n          \n            v\n          \n          \n            i\n          \n          \n            \u2217\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle \\ \\mathbf {M} =\\sum _{i=1}^{r}\\sigma _{i}\\mathbf {u} _{i}\\mathbf {v} _{i}^{*}\\ ,}\n   where \n  \n    \n      \n         \n        r\n        \u2264\n        min\n        {\n        m\n        ,\n        n\n        }\n         \n      \n    \n    {\\displaystyle \\ r\\leq \\min\\{m,n\\}\\ }\n   is the rank of M.\nThe SVD is not unique. It is always possible to choose the decomposition so that the singular values \n  \n    \n      \n        \n          \u03a3\n          \n            i\n            i\n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{ii}}\n   are in descending order. In this case, \n  \n    \n      \n        \n          \u03a3\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma } }\n   (but not U and V) is uniquely determined by M.\nThe term sometimes refers to the compact SVD, a similar decomposition \n  \n    \n      \n         \n        \n          M\n        \n        =\n        \n          U\n          \u03a3\n          \n            V\n            \n              \u2217\n            \n          \n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {M} =\\mathbf {U\\Sigma V^{*}} \\ }\n   in which \n  \n    \n      \n         \n        \n          \u03a3\n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {\\Sigma } \\ }\n   is square diagonal of size \n  \n    \n      \n        r\n        \u00d7\n        r\n      \n    \n    {\\displaystyle r\\times r}\n  , where \n  \n    \n      \n         \n        r\n        \u2264\n        min\n        {\n        m\n        ,\n        n\n        }\n         \n      \n    \n    {\\displaystyle \\ r\\leq \\min\\{m,n\\}\\ }\n   is the rank of M, and has only the non-zero singular values. In this variant, U is an \n  \n    \n      \n        m\n        \u00d7\n        r\n      \n    \n    {\\displaystyle m\\times r}\n   semi-unitary matrix and \n  \n    \n      \n         \n        \n          V\n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {V} \\ }\n   is an \n  \n    \n      \n        n\n        \u00d7\n        r\n      \n    \n    {\\displaystyle n\\times r}\n    semi-unitary matrix, such that \n  \n    \n      \n         \n        \n          \n            U\n            \n              \u2217\n            \n          \n          U\n        \n        =\n        \n          \n            V\n            \n              \u2217\n            \n          \n          V\n        \n        =\n        \n          \n            I\n          \n          \n            r\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\ \\mathbf {U^{*}U} =\\mathbf {V^{*}V} =\\mathbf {I} _{r}\\ .}\n  \nMathematical applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix.  The SVD is also extremely useful in all areas of science, engineering, and statistics, such as signal processing, least squares fitting of data, and process control."
    },
    "Eigenvalues and eigenvectors": {
        "url": "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors",
        "summary": "In linear algebra, it is often important to know which vectors have their directions unchanged by a linear transformation.  An eigenvector () or characteristic vector is such a vector.  Thus an eigenvector \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   of a linear transformation \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is scaled by a constant factor \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   when the linear transformation is applied to it: \n  \n    \n      \n        T\n        \n          v\n        \n        =\n        \u03bb\n        \n          v\n        \n      \n    \n    {\\displaystyle T\\mathbf {v} =\\lambda \\mathbf {v} }\n  .  The corresponding eigenvalue, characteristic value, or characteristic root is the multiplying factor \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  .\nGeometrically, vectors are multi-dimensional quantities with magnitude and direction, often pictured as arrows. A linear transformation rotates, stretches, or shears the vectors it acts upon. Its eigenvectors are those vectors that are only stretched, with no rotation or shear. The corresponding eigenvalue is the factor by which an eigenvector is stretched or squished. If the eigenvalue is negative, the eigenvector's direction is reversed.The eigenvectors and eigenvalues of a transformation serve to characterize it, and so they play important roles in all the areas where linear algebra is applied, from geology to quantum mechanics. In particular, it is often the case that a system is represented by a linear transformation whose outputs are fed as inputs to the same inputs (feedback).  In such an application, the largest eigenvalue is of particular importance, because it governs the long-term behavior of the system, after many applications of the linear transformation, and the associated eigenvector is the steady state of the system.\n\n"
    },
    "One-hot": {
        "url": "https://en.wikipedia.org/wiki/One-hot",
        "summary": "In digital circuits and machine learning, a one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0). A similar implementation in which all bits are '1' except one '0' is sometimes called one-cold. In statistics, dummy variables represent a similar technique for representing categorical data."
    },
    "Feature learning": {
        "url": "https://en.wikipedia.org/wiki/Feature_learning",
        "summary": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform  a specific task.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\nFeature learning can be either supervised, unsupervised or self-supervised.\n\nIn supervised feature learning, features are learned using labeled input data. Labeled data includes input-label pairs where the input is given to the model and it must produce the ground truth label as the correct answer. This can be leveraged to generate feature representations with the model which result in high label prediction accuracy. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.\nIn unsupervised feature learning, features are learned with unlabeled input data by analyzing the relationship between points in the dataset.  Examples include dictionary learning, independent component analysis, matrix factorization and various forms of clustering.\nIn self-supervised feature learning, features are learned using unlabeled data like unsupervised learning, however input-label pairs are constructed from each data point, which enables learning the structure of the data through supervised methods such as gradient descent. Classical examples include word embeddings and autoencoders. SSL has since been applied to many modalities through the use of deep neural network architectures such as CNNs and transformers.\n\n"
    },
    "K-means clustering": {
        "url": "https://en.wikipedia.org/wiki/K-means_clustering",
        "summary": "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\nThe unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm."
    },
    "Stochastic gradient descent": {
        "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent",
        "summary": "Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.While the basic idea behind stochastic approximation can be traced back to the Robbins\u2013Monro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning."
    },
    "Cluster analysis": {
        "url": "https://en.wikipedia.org/wiki/Cluster_analysis",
        "summary": "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\nCluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.\nBesides the term clustering, there is a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek \u03b2\u03cc\u03c4\u03c1\u03c5\u03c2 \"grape\"), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.\nCluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology."
    },
    "Stochastic": {
        "url": "https://en.wikipedia.org/wiki/Stochastic",
        "summary": "Stochastic (; from Ancient Greek  \u03c3\u03c4\u03cc\u03c7\u03bf\u03c2 (st\u00f3khos) 'aim, guess') refers to the property of being well-described by a random probability distribution. Although stochasticity and randomness are distinct in that the former refers to a modeling approach and the latter refers to phenomena themselves, these two terms are often used synonymously. Furthermore, in probability theory, the formal concept of a stochastic process is also referred to as a random process.Stochasticity is used in many different fields, including the natural sciences such as biology, chemistry, ecology,  neuroscience, and physics, as well as technology and engineering fields such as image processing, signal processing, information theory, computer science, cryptography, and telecommunications. It is also used in finance, due to seemingly random changes in financial markets as well as in medicine, linguistics,  music, media, colour theory,  botany, manufacturing, and geomorphology.\n\n"
    },
    "Loss function": {
        "url": "https://en.wikipedia.org/wiki/Loss_function",
        "summary": "In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function)  is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.\nIn statistics, typically a  loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century.  In the context of economics, for example, this is usually economic cost or regret.  In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cram\u00e9r in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss."
    },
    "Computational resource": {
        "url": "https://en.wikipedia.org/wiki/Computational_resource",
        "summary": "In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems.\nThe simplest computational resources are computation time, the number of steps necessary to solve a problem, and memory space, the amount of storage needed while solving the problem, but many more complicated resources have been defined.A computational problem is generally defined in terms of its action on any valid input. Examples of problems might be \"given an integer n, determine whether n is prime\", or \"given two numbers x and y, calculate the product x*y\".  As the inputs get bigger, the amount of computational resources needed to solve a problem will increase.  Thus, the resources needed to solve a problem are described in terms of asymptotic analysis, by identifying the resources as a function of the length or size of the input.  Resource usage is often partially quantified using Big O notation.\nComputational resources are useful because we can study which problems can be computed in a certain amount of each computational resource.  In this way, we can determine whether algorithms for solving the problem are optimal and we can make statements about an algorithm's efficiency.  The set of all of the computational problems that can be solved using a certain amount of a certain computational resource is a complexity class, and relationships between different complexity classes are one of the most important topics in complexity theory."
    },
    "Simulation": {
        "url": "https://en.wikipedia.org/wiki/Simulation",
        "summary": "A simulation is an imitative representation of a process or system that could exist in the real world. In this broad sense, simulation can often be used interchangeably with model. Sometimes a clear distinction between the two terms is made, in which simulations require the use of models; the model represents the key characteristics or behaviors of the selected system or process, whereas the simulation represents the evolution of the model over time. Another way to distinguish between the terms is to define simulation as experimentation with the help of a model. This definition includes time-independent simulations. Often, computers are used to execute the simulation.\nSimulation is used in many contexts, such as simulation of technology for performance tuning or optimizing, safety engineering, testing, training, education, and video games. Simulation is also used with scientific modelling of natural systems or human systems to gain insight into their functioning, as in economics. Simulation can be used to show the eventual real effects of alternative conditions and courses of action. Simulation is also used when the real system cannot be engaged, because it may not be accessible, or it may be dangerous or unacceptable to engage, or it is being designed but not yet built, or it may simply not exist.Key issues in modeling and simulation include the acquisition of valid sources of information about the relevant selection of key characteristics and behaviors used to build the model, the use of simplifying approximations and assumptions within the model, and fidelity and validity of the simulation outcomes. Procedures and protocols for model verification and validation are an ongoing field of academic study, refinement, research and development in simulations technology or practice, particularly in the work of computer simulation.\n\n"
    },
    "Recall (memory)": {
        "url": "https://en.wikipedia.org/wiki/Recall_(memory)",
        "summary": "Recall in memory refers to the mental process of retrieval of information from the past. Along with encoding and storage, it is one of the three core processes of memory. There are three main types of recall: free recall, cued recall and serial recall. Psychologists test these forms of recall as a way to study the memory processes of humans and animals.\nTwo main theories of the process of recall are the two-stage theory and the theory of encoding specificity."
    },
    "Artificial intelligence": {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "summary": "Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. It is a field of study in computer science which develops and studies intelligent machines. It may also refer to the intelligent machines themselves.\nAI technology is widely used throughout industry, government and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding, but after 2012, when deep learning surpassed all previous AI techniques, there was a vast increase in funding and interest.\nThe various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.\nTo solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and many other fields."
    },
    "Computational biology": {
        "url": "https://en.wikipedia.org/wiki/Computational_biology",
        "summary": "Computational biology refers to the use of data analysis, mathematical modeling and computational simulations to understand biological systems and relationships. An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics. It differs from biological computing, a subfield of computer science and engineering which uses bioengineering to build computers.\n\n"
    },
    "Computer science": {
        "url": "https://en.wikipedia.org/wiki/Computer_science",
        "summary": "Computer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). Though more often considered an academic discipline, computer science is closely related to computer programming.Algorithms and data structures are central to computer science.\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human\u2013computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\nThe fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\n\n"
    },
    "Nearest neighbour algorithm": {
        "url": "https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm",
        "summary": "The nearest neighbor algorithm was one of the first algorithms used to solve the travelling salesman problem approximately. In that problem, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. The algorithm quickly yields a short tour, but usually not the optimal one."
    },
    "Graphics processing unit": {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\n"
    },
    "Neuron": {
        "url": "https://en.wikipedia.org/wiki/Neuron",
        "summary": "Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \nThe neuron is the main component of nervous tissue in all animals except sponges and placozoa. Non-animals like plants and fungi do not have nerve cells. The ability to generate electric signals first appeared in evolution 700 million years ago. 800 million years ago, predecessors of neurons were the peptidergic secretory cells. They eventually gained new gene modules which enabled cells to create post-synaptic scaffolds and ion channels that generate fast electrical signals. The ability to generate electric signals was a key innovation in the evolution of the nervous system.Neurons are typically classified into three types based on their function. Sensory neurons respond to stimuli such as touch, sound, or light that affect the cells of the sensory organs, and they send signals to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to control everything from muscle contractions to glandular output. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord. When multiple neurons are functionally connected together, they form what is called a neural circuit.\nNeurons are special cells which are made up of some structures that are common to all other eukaryotic cells such as the cell body (soma), a nucleus, smooth and rough endoplasmic reticulum, Golgi apparatus, mitochondria, and other cellular components. Additionally, neurons have other unique structures  such as dendrites, and a single axon. The soma is a compact structure, and the axon and dendrites are filaments extruding from the soma. Dendrites typically branch profusely and extend a few hundred micrometers from the soma. The axon leaves the soma at a swelling called the axon hillock and travels for as far as 1 meter in humans or more in other species. It branches but usually maintains a constant diameter. At the farthest tip of the axon's branches are axon terminals, where the neuron can transmit a signal across the synapse to another cell. Neurons may lack dendrites or have no axon. The term neurite is used to describe either a dendrite or an axon, particularly when the cell is undifferentiated.\nMost neurons receive signals via the dendrites and soma and send out signals down the axon. At the majority of synapses, signals cross from the axon of one neuron to a dendrite of another. However, synapses can connect an axon to another axon or a dendrite to another dendrite.\nThe signaling process is partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage gradients across their membranes. If the voltage changes by a large enough amount over a short interval, the neuron generates an all-or-nothing electrochemical pulse called an action potential. This potential travels rapidly along the axon and activates synaptic connections as it reaches them. Synaptic signals may be excitatory or inhibitory, increasing or reducing the net voltage that reaches the soma.\nIn most cases, neurons are generated by neural stem cells during brain development and childhood. Neurogenesis largely ceases during adulthood in most areas of the brain."
    },
    "Nonlinear dimensionality reduction": {
        "url": "https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction",
        "summary": "Nonlinear dimensionality reduction, also known as manifold learning, refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself. The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis."
    }
}
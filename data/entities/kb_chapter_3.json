{
    "Information theory": {
        "url": "https://en.wikipedia.org/wiki/Information_theory",
        "summary": "Information theory is the mathematical study of the quantification, storage, and communication of information. The field was originally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.:\u200avii\u200a The field, in  applied mathematics, is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.\nA key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy, less uncertainty) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security.\nApplications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference, cryptography, neurobiology, perception, linguistics, the evolution and function of molecular codes (bioinformatics), thermal physics, molecular dynamics, quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection, pattern recognition, anomaly detection and even art creation."
    },
    "Probability theory": {
        "url": "https://en.wikipedia.org/wiki/Probability_theory",
        "summary": "Probability theory or probability calculus is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event.\nCentral subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes (which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion).\nAlthough it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem.\nAs a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics or sequential estimation. A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics."
    },
    "Central processing unit": {
        "url": "https://en.wikipedia.org/wiki/Central_processing_unit",
        "summary": "A central processing unit (CPU)\u2014also called a central processor or main processor\u2014is the most important processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry, and specialized coprocessors such as graphics processing units (GPUs).\nThe form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic\u2013logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components.\nMost modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to support CPU-level multithreading. Most modern CPUs have privileged mode to support operating systems and hypervisor mode to support virtualization.\nAn IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC)."
    },
    "Software": {
        "url": "https://en.wikipedia.org/wiki/Software",
        "summary": "Software is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.\nAt the lowest programming level, executable code consists of machine language instructions supported by an individual processor\u2014typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer\u2014an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to \"jump\" to a different instruction or is interrupted by the operating system. As of 2023, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.\nThe majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler, an interpreter, or a combination of the two. Software may also be written in a low-level assembly language that has a strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler."
    },
    "Computer scientist": {
        "url": "https://en.wikipedia.org/wiki/Computer_scientist",
        "summary": "A computer scientist is a scholar who specializes in the academic study of computer science.Computer scientists typically work on the theoretical side of computation. Although computer scientists can also focus their work and research on specific areas (such as algorithm and data structure development and design, software engineering, information theory, database theory, theoretical computer science, numerical analysis, programming language theory, compiler, computer graphics, computer vision, robotics, computer architecture, operating system), their foundation is the theoretical study of computing from which these other fields derive.A primary goal of computer scientists is to develop or validate models, often mathematical, to describe the properties of computational systems (processors, programs, computers interacting with people, computers interacting with other computers, etc.) with an overall objective of discovering designs that yield useful benefits (faster, smaller, cheaper, more precise, etc.)."
    },
    "Software engineering": {
        "url": "https://en.wikipedia.org/wiki/Software_engineering",
        "summary": "Software engineering is an engineering-based approach to software development.\nA software engineer is a person who applies the engineering design process to design, develop, test, maintain, and evaluate computer software. The term programmer is sometimes used as a synonym, but may emphasize software implementation over design and can also lack connotations of engineering education or skills.Engineering techniques are used to inform the software development process, which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management, which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning."
    },
    "Bayesian probability": {
        "url": "https://en.wikipedia.org/wiki/Bayesian_probability",
        "summary": "Bayesian probability ( BAY-zee-\u0259n or  BAY-zh\u0259n) is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledge or as quantification of a personal belief.The Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses; that is, with propositions whose truth or falsity is unknown. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability.\nBayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies a prior probability. This, in turn, is then updated to a posterior probability in the light of new, relevant data (evidence). The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.\nThe term Bayesian derives from the 18th-century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference.:\u200a131\u200a Mathematician Pierre-Simon Laplace pioneered and popularized what is now called Bayesian probability.:\u200a97\u201398\u200a"
    },
    "Uncertainty": {
        "url": "https://en.wikipedia.org/wiki/Uncertainty",
        "summary": "Uncertainty or Incertitude refers to epistemic situations involving imperfect or unknown information. It applies to predictions of future events, to physical measurements that are already made, or to the unknown. Uncertainty arises in partially observable or stochastic environments, as well as due to ignorance, indolence, or both. It arises in any number of fields, including insurance, philosophy, physics, statistics, economics, finance, medicine, psychology, sociology, engineering, metrology, meteorology, ecology and information science.\n\n"
    },
    "Probability distribution": {
        "url": "https://en.wikipedia.org/wiki/Probability_distribution",
        "summary": "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).For instance, if X is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). More commonly, probability distributions are used to compare the relative occurrence of many different random values.\nProbability distributions can be defined in different ways and for discrete or for continuous variables. Distributions with special properties or for especially important applications are given specific names.\n\n"
    },
    "Expected value": {
        "url": "https://en.wikipedia.org/wiki/Expected_value",
        "summary": "In probability theory, the expected value (also called expectation, expectancy, expectation operator, mathematical expectation, mean, average, or first moment) is a generalization of the weighted average. Informally, the expected value is the arithmetic mean of a large number of independently selected outcomes of a random variable. Since it is obtained through arithmetic, the expected value sometimes may not even be included in the  sample data set; it is not the value you would \"expect\" to get in reality.\nThe expected value of a random variable with a finite number of outcomes is a weighted average of all possible outcomes.  In the case of a continuum of possible outcomes, the expectation is defined by integration. In the axiomatic foundation for probability provided by measure theory, the expectation is given by Lebesgue integration.\nThe expected value of a random variable X is often denoted by E(X), E[X], or EX, with E also often stylized as E or \n  \n    \n      \n        \n          E\n        \n        .\n      \n    \n    {\\displaystyle \\mathbb {E} .}"
    },
    "P": {
        "url": "https://en.wikipedia.org/wiki/P",
        "summary": "P, or p, is the sixteenth letter of the Latin alphabet, used in the modern English alphabet, the alphabets of other western European languages and others worldwide. Its name in English is pee (pronounced ), plural pees.\n\n"
    },
    "Bernoulli distribution": {
        "url": "https://en.wikipedia.org/wiki/Bernoulli_distribution",
        "summary": "In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli, is the discrete probability distribution of a random variable which takes the value 1 with probability \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   and the value 0 with probability \n  \n    \n      \n        q\n        =\n        1\n        \u2212\n        p\n      \n    \n    {\\displaystyle q=1-p}\n  . Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes\u2013no question. Such questions lead to outcomes that are boolean-valued: a single bit whose value is success/yes/true/one with probability p and failure/no/false/zero with probability q. It can be used to represent a (possibly biased) coin toss where 1 and 0 would represent \"heads\" and \"tails\", respectively, and p would be the probability of the coin landing on heads (or vice versa where 1 would represent tails and p would be the probability of tails).  In particular, unfair coins would have \n  \n    \n      \n        p\n        \u2260\n        1\n        \n          /\n        \n        2.\n      \n    \n    {\\displaystyle p\\neq 1/2.}\n  \nThe Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution). It is also a special case of the two-point distribution, for which the possible outcomes need not be 0 and 1."
    },
    "Sigma": {
        "url": "https://en.wikipedia.org/wiki/Sigma",
        "summary": "Sigma  ( SIG-m\u0259; uppercase \u03a3, lowercase \u03c3, lowercase in word-final position \u03c2; Greek: \u03c3\u03af\u03b3\u03bc\u03b1) is the eighteenth letter of the Greek alphabet. In the system of Greek numerals, it has a value of 200. In general mathematics, uppercase \u03a3 is used as an operator for summation. When used at the end of a letter-case word (one that does not use all caps), the final form (\u03c2) is used. In \u1f48\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2 (Odysseus), for example, the two lowercase sigmas (\u03c3) in the center of the name are distinct from the word-final sigma (\u03c2) at the end. The Latin letter S derives from sigma while the Cyrillic letter Es derives from a lunate form of this letter.\n\n"
    },
    "Parametric equation": {
        "url": "https://en.wikipedia.org/wiki/Parametric_equation",
        "summary": "In mathematics, a parametric equation defines a group of quantities as functions of one or more independent variables called parameters. Parametric equations are commonly used to express the coordinates of the points that make up a geometric object such as a curve or surface, called a  parametric curve and parametric surface, respectively. In such cases, the equations are collectively called a parametric representation, or parametric system, or parameterization (alternatively spelled as parametrisation) of the object.For example, the equations\n\nform a parametric representation of the unit circle, where t is the parameter: A point (x, y) is on the unit circle if and only if there is a value of t such that these two equations generate that point. Sometimes the parametric equations for the individual scalar output variables are combined into a single parametric equation in vectors:\n\nParametric representations are generally nonunique (see the \"Examples in two dimensions\" section below), so the same quantities may be expressed by a number of different parameterizations.In addition to curves and surfaces, parametric equations can describe manifolds and algebraic varieties of higher dimension, with the number of parameters being equal to the dimension of the manifold or variety, and the number of equations being equal to the dimension of the space in which the manifold or variety is considered (for curves the dimension is one and one parameter is used,  for surfaces dimension two and two parameters, etc.).\nParametric equations are commonly used in kinematics, where the trajectory of an object is represented by equations depending on time as the parameter. Because of this application, a single parameter is often labeled t; however, parameters can represent other physical quantities (such as geometric variables) or can be selected arbitrarily for convenience. Parameterizations are non-unique; more than one set of parametric equations can specify the same curve."
    },
    "Beta": {
        "url": "https://en.wikipedia.org/wiki/Beta",
        "summary": "Beta (UK: , US: ; uppercase \u0392, lowercase \u03b2, or cursive \u03d0; Ancient Greek: \u03b2\u1fc6\u03c4\u03b1, romanized: b\u0113\u0302ta or Greek: \u03b2\u03ae\u03c4\u03b1, romanized: v\u00edta) is the second letter of the Greek alphabet. In the system of Greek numerals, it has a value of 2. In Ancient Greek, beta represented the voiced bilabial plosive IPA: [b]. In Modern Greek, it represents the voiced labiodental fricative IPA: [v] while IPA: [b] in borrowed words is instead commonly transcribed as \u03bc\u03c0. Letters that arose from beta include the Roman letter \u27e8B\u27e9 and the Cyrillic letters \u27e8\u0411\u27e9 and \u27e8\u0412\u27e9."
    },
    "Phi": {
        "url": "https://en.wikipedia.org/wiki/Phi",
        "summary": "Phi (; uppercase \u03a6, lowercase \u03c6 or \u03d5; Ancient Greek: \u03d5\u03b5\u1fd6 phe\u00ee [p\u02b0\u00e9\u00ee\u032f]; Modern Greek: \u03c6\u03b9 fi [fi]) is the twenty-first letter of the Greek alphabet.\nIn Archaic and Classical Greek (c. 9th century BC to 4th century BC), it represented an aspirated voiceless bilabial plosive ([p\u02b0]), which was the origin of its usual romanization as \u27e8ph\u27e9. During the later part of Classical Antiquity, in Koine Greek (c. 4th century BC to 4th century AD), its pronunciation shifted to that of a voiceless bilabial fricative ([\u0278]), and by the Byzantine Greek period (c. 4th century AD to 15th century AD) it developed its modern pronunciation as a voiceless labiodental fricative ([f]).\nThe romanization of the Modern Greek phoneme is therefore usually \u27e8f\u27e9.\nIt may be that phi originated as the letter qoppa (\u03d8, \u03d9), and initially represented the sound /k\u02b7\u02b0/ before shifting to Classical Greek [p\u02b0]. In traditional Greek numerals, phi has a value of 500 (\u03c6\u02b9) or 500,000 (\u0375\u03c6). The Cyrillic letter Ef (\u0424, \u0444) descends from phi.\nAs with other Greek letters, lowercase phi (encoded as the Unicode character U+03C6 \u03c6 GREEK SMALL LETTER PHI) is used as a mathematical or scientific symbol. Some uses require the old-fashioned 'closed' glyph, which is separately encoded as the Unicode character U+03D5 \u03d5 GREEK PHI SYMBOL."
    },
    "6": {
        "url": "https://en.wikipedia.org/wiki/6",
        "summary": "6 (six) is the natural number following 5 and preceding 7. It is a composite number and the smallest perfect number."
    },
    "9": {
        "url": "https://en.wikipedia.org/wiki/9",
        "summary": "9 (nine) is the natural number following 8 and preceding 10."
    },
    "Logit": {
        "url": "https://en.wikipedia.org/wiki/Logit",
        "summary": "In statistics, the logit ( LOH-jit) function is the quantile function associated with the standard logistic distribution. It has many uses in data analysis and machine learning, especially in data transformations.\nMathematically, the logit is the inverse of the standard logistic function \n  \n    \n      \n        \u03c3\n        (\n        x\n        )\n        =\n        1\n        \n          /\n        \n        (\n        1\n        +\n        \n          e\n          \n            \u2212\n            x\n          \n        \n        )\n      \n    \n    {\\displaystyle \\sigma (x)=1/(1+e^{-x})}\n  , so the logit is defined as\n\n  \n    \n      \n        logit\n        \u2061\n        p\n        =\n        \n          \u03c3\n          \n            \u2212\n            1\n          \n        \n        (\n        p\n        )\n        =\n        ln\n        \u2061\n        \n          \n            p\n            \n              1\n              \u2212\n              p\n            \n          \n        \n        \n        \n          for\n        \n        \n        p\n        \u2208\n        (\n        0\n        ,\n        1\n        )\n        .\n      \n    \n    {\\displaystyle \\operatorname {logit} p=\\sigma ^{-1}(p)=\\ln {\\frac {p}{1-p}}\\quad {\\text{for}}\\quad p\\in (0,1).}\n  Because of this, the logit is also called the log-odds since it is equal to the logarithm of the odds \n  \n    \n      \n        \n          \n            p\n            \n              1\n              \u2212\n              p\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {p}{1-p}}}\n   where p is a probability. Thus, the logit is a type of function that maps probability values from \n  \n    \n      \n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle (0,1)}\n   to real numbers in \n  \n    \n      \n        (\n        \u2212\n        \u221e\n        ,\n        +\n        \u221e\n        )\n      \n    \n    {\\displaystyle (-\\infty ,+\\infty )}\n  , akin to the probit function."
    },
    "Statistics": {
        "url": "https://en.wikipedia.org/wiki/Statistics",
        "summary": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems."
    },
    "Line code": {
        "url": "https://en.wikipedia.org/wiki/Line_code",
        "summary": "In telecommunication, a line code is a pattern of voltage, current, or photons used to represent digital data transmitted down a communication channel or written to a storage medium. This repertoire of signals is usually called a constrained code in data storage systems. \nSome signals are more prone to error than others as the physics of the communication channel or storage medium constrains the repertoire of signals that can be used reliably.Common line encodings are unipolar, polar, bipolar, and Manchester code."
    },
    "Normalizing constant": {
        "url": "https://en.wikipedia.org/wiki/Normalizing_constant",
        "summary": "In probability theory, a normalizing constant or normalizing factor is used to reduce any probability function to a probability density function with total probability of one.\nFor example, a Gaussian function can be normalized into a probability density function, which gives the standard normal distribution. In Bayes' theorem, a normalizing constant is used to ensure that the sum of all possible hypotheses equals 1. Other uses of normalizing constants include making the value of a Legendre polynomial at 1 and in the orthogonality of orthonormal functions.\nA similar concept has been used in areas other than probability, such as for polynomials."
    },
    "Factorization": {
        "url": "https://en.wikipedia.org/wiki/Factorization",
        "summary": "In mathematics, factorization (or factorisation, see English spelling differences) or factoring consists of writing a number or another mathematical object as a product of several factors, usually smaller or simpler objects of the same kind. For example, 3 \u00d7 5 is an integer factorization of 15, and (x \u2013 2)(x + 2) is a polynomial factorization of x2 \u2013 4.\nFactorization is not usually considered meaningful within number systems possessing division, such as the real or complex numbers, since any \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   can be trivially written as \n  \n    \n      \n        (\n        x\n        y\n        )\n        \u00d7\n        (\n        1\n        \n          /\n        \n        y\n        )\n      \n    \n    {\\displaystyle (xy)\\times (1/y)}\n   whenever \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   is not zero. However, a meaningful factorization for a rational number or a rational function can be obtained by writing it in lowest terms and separately factoring its numerator and denominator.\nFactorization was first considered by ancient Greek mathematicians in the case of integers. They proved the fundamental theorem of arithmetic, which asserts that every positive integer may be factored into a product of prime numbers, which cannot be further factored into integers greater than 1. Moreover, this factorization is unique up to the order of the factors. Although integer factorization is a sort of inverse to multiplication, it is much more difficult algorithmically, a fact which is exploited in the RSA cryptosystem to implement public-key cryptography.\nPolynomial factorization has also been studied for centuries. In elementary algebra, factoring a polynomial reduces the problem of finding its roots to finding the roots of the factors. Polynomials with coefficients in the integers or in a field possess the unique factorization property, a version of the fundamental theorem of arithmetic with prime numbers replaced by irreducible polynomials. In particular, a univariate polynomial with complex coefficients admits a unique (up to ordering) factorization into linear polynomials: this is a version of the fundamental theorem of algebra. In this case, the factorization can be done with root-finding algorithms. The case of polynomials with integer coefficients is fundamental for computer algebra. There are efficient computer algorithms for computing (complete) factorizations within the ring of polynomials with rational number coefficients (see factorization of polynomials).\nA commutative ring possessing the unique factorization property is called a unique factorization domain. There are number systems, such as certain rings of algebraic integers, which are not unique factorization domains. However, rings of algebraic integers satisfy the weaker property of Dedekind domains: ideals factor uniquely into prime ideals.\nFactorization may also refer to more general decompositions of a mathematical object into the product of smaller or simpler objects. For example, every function may be factored into the composition of a surjective function with an injective function. Matrices possess many kinds of matrix factorizations. For example, every matrix has a unique LUP factorization as a product of a lower triangular matrix L with all diagonal entries equal to one, an upper triangular matrix U, and a permutation matrix P; this is a matrix formulation of Gaussian elimination."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input."
    },
    "Mathematics": {
        "url": "https://en.wikipedia.org/wiki/Mathematics",
        "summary": "Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory, algebra, geometry, and analysis, respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.\nMost mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or\u2014in modern mathematics\u2014entities that are stipulated to have certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and\u2014in case of abstraction from nature\u2014some basic properties that are considered true starting points of the theory under consideration.Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.\nHistorically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was essentially divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new areas. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid lockstep increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics."
    }
}
{
    "Boltzmann machine": {
        "url": "https://en.wikipedia.org/wiki/Boltzmann_machine",
        "summary": "A Boltzmann machine (also called Sherrington\u2013Kirkpatrick model with external field or stochastic Ising\u2013Lenz\u2013Little model) is a stochastic spin-glass model with an external field, i.e., a Sherrington\u2013Kirkpatrick model, that is a stochastic Ising model. It is a statistical physics technique applied in the context of cognitive science. It is also classified as a Markov random field.Boltzmann machines are theoretically intriguing because of the locality and Hebbian nature of their training algorithm (being trained by Hebb's rule), and because of their parallelism and the resemblance of their dynamics to simple physical processes.  Boltzmann machines with unconstrained connectivity have not been proven useful for practical problems in machine learning or inference, but if the connectivity is properly constrained, the learning can be made efficient enough to be useful for practical problems.They are named after the Boltzmann distribution in statistical mechanics, which is used in their sampling function.  They were heavily popularized and promoted by Geoffrey Hinton, Terry Sejnowski and Yann LeCun in cognitive sciences communities and in machine learning.  As a more general class within machine learning these models are called \"energy based models\" (EBM), because Hamiltonians of spin glasses are used as a starting point to define the learning task."
    },
    "Probability distribution": {
        "url": "https://en.wikipedia.org/wiki/Probability_distribution",
        "summary": "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).For instance, if X is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). More commonly, probability distributions are used to compare the relative occurrence of many different random values.\nProbability distributions can be defined in different ways and for discrete or for continuous variables. Distributions with special properties or for especially important applications are given specific names.\n\n"
    },
    "Statistical model": {
        "url": "https://en.wikipedia.org/wiki/Statistical_model",
        "summary": "A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process. When referring specifically to probabilities, the corresponding term is probabilistic model.\nA statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables.  As such, a statistical model is \"a formal representation of a theory\" (Herman Ad\u00e8r quoting Kenneth Bollen).All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference."
    },
    "Maximum likelihood estimation": {
        "url": "https://en.wikipedia.org/wiki/Maximum_likelihood_estimation",
        "summary": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.If the likelihood function is differentiable, the derivative test for finding maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved analytically; for instance, the ordinary least squares estimator for a linear regression model maximizes the likelihood when the random errors are assumed to have normal distributions with the same variance.From the perspective of Bayesian inference, MLE is generally equivalent to maximum a posteriori (MAP) estimation with uniform prior distributions (or a normal prior distribution with a standard deviation of infinity). In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood."
    },
    "Statistics": {
        "url": "https://en.wikipedia.org/wiki/Statistics",
        "summary": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems."
    },
    "Graphics processing unit": {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining."
    },
    "Wakefulness": {
        "url": "https://en.wikipedia.org/wiki/Wakefulness",
        "summary": "Wakefulness is a daily recurring brain state and state of consciousness in which an individual is conscious and engages in coherent cognitive and behavioral responses to the external world.\nBeing awake is the opposite of being asleep, in which most external inputs to the brain are excluded from neural processing."
    },
    "P": {
        "url": "https://en.wikipedia.org/wiki/P",
        "summary": "P, or p, is the sixteenth letter of the Latin alphabet, used in the modern English alphabet, the alphabets of other western European languages and others worldwide. Its name in English is pee (pronounced ), plural pees.\n\n"
    },
    "H": {
        "url": "https://en.wikipedia.org/wiki/H",
        "summary": "H, or h, is the eighth letter in the Latin alphabet, used in the modern English alphabet, including the alphabets of other western European languages and others worldwide. Its name in English is aitch (pronounced , plural aitches), or regionally haitch .\n\n"
    },
    "Dialectic": {
        "url": "https://en.wikipedia.org/wiki/Dialectic",
        "summary": "Dialectic (Greek: \u03b4\u03b9\u03b1\u03bb\u03b5\u03ba\u03c4\u03b9\u03ba\u03ae, dialektik\u1e17; German: Dialektik), also known as the dialectical method, refers originally to dialogue between people holding different points of view about a subject but wishing to arrive at the truth through reasoned argumentation. Dialectic resembles debate, but the concept excludes subjective elements such as emotional appeal and rhetoric. It has its origins in ancient philosophy and continued to be developed in the Middle Ages.\nIn the modern period, Hegelianism refigured \"dialectic\" to no longer refer to a literal dialogue. Instead, the term takes on the specialized meaning of development by way of overcoming internal contradictions. Dialectical materialism, a theory advanced by Karl Marx and Friedrich Engels, adapted the Hegelian dialectic into a materialist theory of history. The legacy of Hegelian and Marxian dialectics has been criticized by philosophers such as Karl Popper and Mario Bunge, who considered it unscientific.\nDialectic implies a developmental process and so does not naturally fit within classical logic. Nevertheless, some twentieth-century logicians have attempted to formalize it."
    },
    "Linguistics": {
        "url": "https://en.wikipedia.org/wiki/Linguistics",
        "summary": "Linguistics is the scientific study of language.Linguistics is based on theoretical as well as descriptive study of language and is also interlinked with the applied fields of language studies and language learning, which entails the study of specific languages. Before the 20th century, linguistics evolved in conjunction with literary study and did not exclusively employ scientific methods.Traditional areas of linguistic analysis correspond to syntax (rules governing the structure of sentences), semantics (meaning), morphology (structure of words), phonetics (speech sounds and equivalent gestures in sign languages), phonology (the abstract sound system of a particular language), and pragmatics (how social context contributes to meaning). Subdisciplines such as biolinguistics (the study of the biological variables and evolution of language) and psycholinguistics (the study of psychological factors in human language) bridge many of these divisions.Linguistics encompasses many branches and subfields that span both theoretical and practical applications. Theoretical linguistics (including traditional descriptive linguistics) is concerned with understanding the universal and fundamental nature of language and developing a general theoretical framework for describing it. Applied linguistics seeks to utilise the scientific findings of the study of language for practical purposes, such as developing methods of improving language education and literacy.Linguistic features may be studied through a variety of perspectives: synchronically (by describing the shifts in a language at a certain specific point of time) or diachronically (through the historical development of language over several periods of time), in monolinguals or in multilinguals, amongst children or amongst adults, in terms of how it is being learned or how it was acquired, as abstract objects or as cognitive structures, through written texts or through oral elicitation, and finally through mechanical data collection or through practical fieldwork.Linguistics emerged from the field of philology,  of which some branches are more qualitative and holistic in approach. Today, philology and linguistics are now variably described as related fields, subdisciplines, or separate fields of language study but, by and large, linguistics can be seen as an umbrella term. Linguistics is also related to the philosophy of language, stylistics, rhetoric, semiotics, lexicography, and translation."
    },
    "Generative model": {
        "url": "https://en.wikipedia.org/wiki/Generative_model",
        "summary": "In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following Jebara (2004):\n\nA generative model is a statistical model of the joint probability distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n   on given observable variable X and target variable Y;\nA discriminative model is a model of the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        \u2223\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y\\mid X=x)}\n   of the target Y, given an observation x; and\nClassifiers computed without using a probability model are also referred to loosely as \"discriminative\".The distinction between these last two classes is not consistently made; Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.\nStandard examples of each, all of which are linear classifiers, are:\n\ngenerative classifiers:\nnaive Bayes classifier and\nlinear discriminant analysis\ndiscriminative model:\nlogistic regressionIn application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n   (discriminative model), and base classification on that; or one can estimate the joint distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n   (generative model), from that compute the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n  , and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches."
    },
    "Bipartite graph": {
        "url": "https://en.wikipedia.org/wiki/Bipartite_graph",
        "summary": "In the mathematical field of graph theory, a bipartite graph (or bigraph) is a graph whose vertices can be divided into two disjoint and independent sets \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  , that is, every edge connects a vertex in \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   to one in \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  . Vertex sets \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n   are usually called the parts of the graph. Equivalently, a bipartite graph is a graph that does not contain any odd-length cycles.The two sets \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n   may be thought of as a coloring of the graph with two colors: if one colors all nodes in \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   blue, and all nodes in \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n   red, each edge has endpoints of differing colors, as is required in the graph coloring problem. In contrast, such a coloring is impossible in the case of a non-bipartite graph, such as a triangle: after one node is colored blue and another red, the third vertex of the triangle is connected to vertices of both colors, preventing it from being assigned either color.\nOne often writes \n  \n    \n      \n        G\n        =\n        (\n        U\n        ,\n        V\n        ,\n        E\n        )\n      \n    \n    {\\displaystyle G=(U,V,E)}\n   to denote a bipartite graph whose partition has the parts \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  , with \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   denoting the edges of the graph. If a bipartite graph is not connected, it may have more than one bipartition; in this case, the \n  \n    \n      \n        (\n        U\n        ,\n        V\n        ,\n        E\n        )\n      \n    \n    {\\displaystyle (U,V,E)}\n   notation is helpful in specifying one particular bipartition that may be of importance in an application.  If \n  \n    \n      \n        \n          |\n        \n        U\n        \n          |\n        \n        =\n        \n          |\n        \n        V\n        \n          |\n        \n      \n    \n    {\\displaystyle |U|=|V|}\n  , that is, if the two subsets have equal cardinality, then \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n   is called a balanced bipartite graph. If all vertices on the same side of the bipartition have the same degree, then \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n   is called biregular."
    },
    "Hierarchy": {
        "url": "https://en.wikipedia.org/wiki/Hierarchy",
        "summary": "A hierarchy (from Greek: \u1f31\u03b5\u03c1\u03b1\u03c1\u03c7\u03af\u03b1, hierarkhia, 'rule of a high priest', from hierarkhes, 'president of sacred rites') is an arrangement of items (objects, names, values, categories, etc.) that are represented as being \"above\", \"below\", or \"at the same level as\" one another. Hierarchy is an important concept in a wide variety of fields, such as architecture, philosophy, design, mathematics, computer science, organizational theory, systems theory, systematic biology, and the social sciences (especially political science).\nA hierarchy can link entities either directly or indirectly, and either vertically or diagonally. The only direct links in a hierarchy, insofar as they are hierarchical, are to one's immediate superior or to one of one's subordinates, although a system that is largely hierarchical can also incorporate alternative hierarchies. Hierarchical links can extend \"vertically\" upwards or downwards via multiple links in the same direction, following a path. All parts of the hierarchy that are not linked vertically to one another nevertheless can be \"horizontally\" linked through a path by traveling up the hierarchy to find a common direct or indirect superior, and then down again. This is akin to two co-workers or colleagues; each reports to a common superior, but they have the same relative amount of authority. Organizational forms exist that are both alternative and complementary to hierarchy. Heterarchy is one such form."
    },
    "Hyperparameter": {
        "url": "https://en.wikipedia.org/wiki/Hyperparameter",
        "summary": "In Bayesian statistics, a hyperparameter is a parameter of a prior distribution; the term is used to distinguish them from parameters of the model for the underlying system under analysis.\nFor example, if one is using a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then:\n\np is a parameter of the underlying system (Bernoulli distribution), and\n\u03b1 and \u03b2 are parameters of the prior distribution (beta distribution), hence hyperparameters.One may take a single value for a given hyperparameter, or one can iterate and take a probability distribution on the hyperparameter itself, called a hyperprior."
    },
    "Training": {
        "url": "https://en.wikipedia.org/wiki/Training",
        "summary": "Training is teaching, or developing in oneself or others, any skills and knowledge  or fitness that relate to specific useful competencies. Training has specific goals of improving one's capability, capacity, productivity and performance. It forms the core of apprenticeships and provides the backbone of content at institutes of technology (also known as technical colleges or polytechnics). In addition to the basic training required for a trade, occupation or profession, training may continue beyond initial competence to maintain, upgrade and update skills throughout working life. People within some professions and occupations may refer to this sort of training as professional development. Training also refers to the development of physical fitness related to a specific competence, such as sport, martial arts, military applications and some other occupations."
    },
    "COVID-19 pandemic": {
        "url": "https://en.wikipedia.org/wiki/COVID-19_pandemic",
        "summary": "The COVID-19 pandemic, also known as the coronavirus pandemic, is a global pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The novel virus was first identified in an outbreak in the Chinese city of Wuhan in December 2019. Attempts to contain it there failed, allowing the virus to spread to other areas of Asia and then worldwide in early 2020. The World Health Organization (WHO) declared the outbreak a public health emergency of international concern (PHEIC) on 30 January 2020. The WHO ended its PHEIC declaration on 5 May 2023. As of 2 December 2023, the pandemic had caused 772,051,988 cases and 6,985,265 confirmed deaths, ranking it fifth in the list of the deadliest epidemics and pandemics in history.\nCOVID-19 symptoms range from asymptomatic to deadly, but most commonly include fever, sore throat, nocturnal cough, and fatigue. Transmission of the virus is often through airborne particles. Mutations have produced many strains (variants) with varying degrees of infectivity and virulence.COVID-19 vaccines were widely deployed in various countries beginning in December 2020. Treatments include novel antiviral drugs and symptom control. Common mitigation measures during the public health emergency included travel restrictions, lockdowns, business restrictions and closures, workplace hazard controls, mask mandates, quarantines, testing systems, and contact tracing of the infected.\nThe pandemic caused severe social and economic disruption around the world, including the largest global recession since the Great Depression. Widespread supply shortages, including food shortages, were caused by supply chain disruptions and panic buying. Reduced human activity led to an unprecedented decrease in pollution. Educational institutions and public areas were partially or fully closed in many jurisdictions, and events were cancelled or postponed during 2020, 2021, and 2022. Many white-collar workers began working from home. Misinformation circulated through social media and mass media, and political tensions intensified. The pandemic raised issues of racial and geographic discrimination, health equity, and the balance between public health imperatives and individual rights."
    },
    "2020": {
        "url": "https://en.wikipedia.org/wiki/2020",
        "summary": "2020 (MMXX) was a leap year starting on Wednesday of the Gregorian calendar, the 2020th year of the Common Era (CE) and Anno Domini (AD) designations, the 20th  year of the 3rd millennium and the 21st century, and the  1st   year of the 2020s decade.  \n2020 was heavily defined by the COVID-19 pandemic, which led to global social and economic disruption, mass cancellations and postponements of events, worldwide lockdowns and the largest economic recession since the Great Depression in the 1930s. Geospatial World also called 2020 \"the worst year in terms of climate change\" in part due to major climate disasters worldwide, including major bushfires in Australia and the western United States, as well as extreme tropical cyclone activity affecting large parts of North America. A United Nations progress report published in December 2020 indicated that none of the international Sustainable Development Goals for 2020 were achieved. Time magazine used its fifth ever Red X cover to declare 2020 \"the worst year ever\", although the cover article itself did not go as far, instead saying \"There have been worse years in U.S. history, and certainly worse years in world history, but most of us alive today have seen nothing like this one.\" The Golden Raspberry Awards also awarded the year the Special Governor's Award for The Worst Calendar Year Ever! at their 41st ceremony."
    },
    "List of things named after Carl Friedrich Gauss": {
        "url": "https://en.wikipedia.org/wiki/List_of_things_named_after_Carl_Friedrich_Gauss",
        "summary": "Carl Friedrich Gauss (1777\u20131855) is the eponym of all of the topics listed below. \nThere are over 100 topics all named after this German mathematician and scientist, all in the fields of mathematics, physics, and astronomy. The English eponymous adjective Gaussian is pronounced .\n\n"
    },
    "Supine": {
        "url": "https://en.wikipedia.org/wiki/Supine",
        "summary": "In grammar, a supine is a form of verbal noun used in some languages. The term is most often used for Latin, where it is one of the four principal parts of a verb. The word refers to a position of lying on one's back (as opposed to 'prone', lying face downward), but there exists no widely accepted etymology that explains why or how the term came to be used to also describe this form of a verb."
    },
    "Convolutional neural network": {
        "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
        "summary": "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,  only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\nThey have applications in: \n\nimage and video recognition,\nrecommender systems,\n\nimage classification,\n\nimage segmentation,\n\nmedical image analysis,\n\nnatural language processing,\n\nbrain\u2013computer interfaces, and\n\nfinancial time series.CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increases the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage."
    },
    "Restricted Boltzmann machine": {
        "url": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine",
        "summary": "A restricted Boltzmann machine (RBM) (also called a restricted Sherrington\u2013Kirkpatrick model with external field or restricted stochastic Ising\u2013Lenz\u2013Little model) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.RBMs were initially proposed under the name Harmonium by Paul Smolensky in 1986, and rose to prominence after Geoffrey Hinton and collaborators used fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction, classification, collaborative filtering, feature learning, topic modelling and even many body quantum mechanics. They can be trained in either supervised or unsupervised ways, depending on the task.\nAs their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: \na pair of nodes from each of the two groups of units (commonly referred to as the \"visible\" and \"hidden\" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, \"unrestricted\" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by \"stacking\" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation."
    },
    "Backpropagation": {
        "url": "https://en.wikipedia.org/wiki/Backpropagation",
        "summary": "As a machine-learning algorithm, backpropagation is a crucial step in a common method used to iteratively train a neural network model. It is used to calculate the necessary parameter adjustments, to gradually minimize error.\nIn a multi-layered network, backpropagation is step 2.2 for training a neural network model:\n\nPropagate training data through the model from input to predicted output by computing the successive hidden layers' outputs and finally the final layer's output (the feedforward step).\nAdjust the model weights to reduce the error relative to the weights.\nThe error is typically the squared difference between prediction and target.\nFor each weight, the slope or derivative of the error is found, and the weight adjusted by a negative multiple of this derivative, so as to go downslope toward the minimum-error configuration.\nThis derivative is easy to calculate for final layer weights, and possible to calculate for one layer given the next layer's derivatives. Starting at the end, then, the derivatives are calculated layer by layer toward the beginning -- thus \"backpropagation\".\nRepeatedly update the weights until they converge or the model has undergone enough iterations.It is an efficient application of the Leibniz chain rule (1673) to such networks. It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).   The term \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory.Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input\u2013output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming. Gradient descent, or variants such as stochastic gradient descent, are commonly used.\nStrictly the term backpropagation refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm \u2013 including how the gradient is used, such as by stochastic gradient descent. In 1986 David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons."
    },
    "Gradient": {
        "url": "https://en.wikipedia.org/wiki/Gradient",
        "summary": "In vector calculus, the gradient of a scalar-valued differentiable function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   of several variables is the vector field (or vector-valued function) \n  \n    \n      \n        \u2207\n        f\n      \n    \n    {\\displaystyle \\nabla f}\n   whose value at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   gives the direction and the rate of fastest increase. The gradient transforms like a vector under change of basis of the space of variables of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  . If the gradient of a function is non-zero at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , the direction of the gradient is the direction in which the function increases most quickly from \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , and the magnitude of the gradient is the rate of increase in that direction, the greatest absolute directional derivative. Further, a point where the gradient is the zero vector is known as a stationary point. The gradient thus plays a fundamental role in optimization theory, where it is used to maximize a function by gradient ascent. In coordinate-free terms, the gradient of a function \n  \n    \n      \n        f\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {r} )}\n   may be defined by:\n\nwhere \n  \n    \n      \n        d\n        f\n      \n    \n    {\\displaystyle df}\n   is the total infinitesimal change in \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   for an infinitesimal displacement  \n  \n    \n      \n        d\n        \n          r\n        \n      \n    \n    {\\displaystyle d\\mathbf {r} }\n  , and is seen to be maximal when \n  \n    \n      \n        d\n        \n          r\n        \n      \n    \n    {\\displaystyle d\\mathbf {r} }\n   is in the direction of the gradient \n  \n    \n      \n        \u2207\n        f\n      \n    \n    {\\displaystyle \\nabla f}\n  . The nabla symbol \n  \n    \n      \n        \u2207\n      \n    \n    {\\displaystyle \\nabla }\n  , written as an upside-down triangle and pronounced \"del\", denotes the vector differential operator.\nWhen a coordinate system is used in which the basis vectors are not functions of position, the gradient is given by the vector whose components are the partial derivatives of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  . That is, for \n  \n    \n      \n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle f\\colon \\mathbb {R} ^{n}\\to \\mathbb {R} }\n  , its gradient \n  \n    \n      \n        \u2207\n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\nabla f\\colon \\mathbb {R} ^{n}\\to \\mathbb {R} ^{n}}\n   is defined at the point \n  \n    \n      \n        p\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle p=(x_{1},\\ldots ,x_{n})}\n   in n-dimensional space as the vector\nNote that the above definition for gradient is only defined for the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  , if it is differentiable at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  . There can be functions for which partial derivatives exist in every direction but still fail to be differentiable. For example, the function \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          \n            \n              \n                x\n                \n                  2\n                \n              \n              y\n            \n            \n              \n                x\n                \n                  2\n                \n              \n              +\n              \n                y\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x,y)={\\frac {x^{2}y}{x^{2}+y^{2}}}}\n   unless at origin where \n  \n    \n      \n        f\n        (\n        0\n        ,\n        0\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(0,0)=0}\n  , is not differentiable at origin as it does not have a well defined tangent plane despite having well defined partial derivatives in every direction at the origin. In the particular example, under rotation of x-y coordinate system, the above formula for gradient fails to transform like a vector (gradient becomes dependent on choice of basis for coordinate system) and also fails to point towards the steepest ascent in some orientations. For differentiable functions where the formula for gradient holds, it can be shown to always transform as a vector under transformation of the basis so as to always \"point towards the fastest increase\".\nThe gradient is dual to the total derivative \n  \n    \n      \n        d\n        f\n      \n    \n    {\\displaystyle df}\n  : the value of the gradient at a point is a tangent vector \u2013 a vector at each point; while the value of the derivative at a point is a cotangent vector \u2013 a linear functional on vectors. They are related in that the dot product of the gradient of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   with another tangent vector \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   equals the directional derivative of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   of the function along \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  ; that is, \n  \n    \n      \n        \u2207\n        f\n        (\n        p\n        )\n        \u22c5\n        \n          v\n        \n        =\n        \n          \n            \n              \u2202\n              f\n            \n            \n              \u2202\n              \n                v\n              \n            \n          \n        \n        (\n        p\n        )\n        =\n        d\n        \n          f\n          \n            p\n          \n        \n        (\n        \n          v\n        \n        )\n      \n    \n    {\\textstyle \\nabla f(p)\\cdot \\mathbf {v} ={\\frac {\\partial f}{\\partial \\mathbf {v} }}(p)=df_{p}(\\mathbf {v} )}\n  . \nThe gradient admits multiple generalizations to more general functions on manifolds; see \u00a7 Generalizations."
    },
    "Normal distribution": {
        "url": "https://en.wikipedia.org/wiki/Normal_distribution",
        "summary": "In statistics, a normal distribution or Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              \u03c3\n              \n                \n                  2\n                  \u03c0\n                \n              \n            \n          \n        \n        \n          e\n          \n            \u2212\n            \n              \n                1\n                2\n              \n            \n            \n              \n                (\n                \n                  \n                    \n                      x\n                      \u2212\n                      \u03bc\n                    \n                    \u03c3\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\n  The parameter \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   is the mean or expectation of the distribution (and also its median and mode), while the parameter \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   is its standard deviation. The variance of the distribution is \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  . A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate.\nNormal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable\u2014whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal.Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies. For instance, any linear combination of a fixed collection of independent normal deviates is a normal deviate. Many results and methods, such as propagation of uncertainty and least squares parameter fitting, can be derived analytically in explicit form when the relevant variables are normally distributed.\nA normal distribution is sometimes informally called a bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions). For other names, see Naming.\nThe univariate probability distribution is generalized for vectors in the multivariate normal distribution and for matrices in the matrix normal distribution."
    },
    "Reinforcement learning": {
        "url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
        "summary": "Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process and they target large Markov decision processes where exact methods become infeasible. \n\n"
    },
    "Neural coding": {
        "url": "https://en.wikipedia.org/wiki/Neural_coding",
        "summary": "Neural coding (or neural representation) is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among the electrical activity of the neurons in the ensemble. Based on the theory that\nsensory and other information is represented in the brain by networks of neurons, it is thought that neurons can encode both digital and analog information."
    },
    "Deep learning": {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "summary": "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog."
    },
    "Gradient descent": {
        "url": "https://en.wikipedia.org/wiki/Gradient_descent",
        "summary": "Gradient descent (also often called steepest descent) is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\nThe idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.\nIt is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although both are iterative methods for optimization.\nGradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.A simple extension of gradient descent, stochastic gradient descent, serves as the most basic algorithm used for training most deep networks today.\n\n"
    },
    "Supervised learning": {
        "url": "https://en.wikipedia.org/wiki/Supervised_learning",
        "summary": "Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error."
    },
    "Variational autoencoder": {
        "url": "https://en.wikipedia.org/wiki/Variational_autoencoder",
        "summary": "In machine learning, a variational autoencoder (VAE) is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling. It is part of the families of probabilistic graphical models and variational Bayesian methods.Variational autoencoders are often associated with the autoencoder model because of its architectural affinity, but with significant differences in the goal and mathematical formulation. Variational autoencoders are probabilistic generative models that require neural networks as only a part of their overall structure. The neural network components are typically referred to as the encoder and decoder for the first and second component respectively. The first neural network maps the input variable to a latent space that corresponds to the parameters of a variational distribution. In this way, the encoder can produce multiple different samples that all come from the same distribution. The decoder has the opposite function, which is to map from the latent space to the input space, in order to produce or generate data points. Both networks are typically trained together with the usage of the reparameterization trick, although the variance of the noise model can be learned separately.\nAlthough this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning.\n\n"
    },
    "Autoencoder": {
        "url": "https://en.wikipedia.org/wiki/Autoencoder",
        "summary": "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\nVariants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data)."
    },
    "Variational Bayesian methods": {
        "url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods",
        "summary": "Variational Bayesian methods are a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning.  They are typically used in complex statistical models consisting of observed variables (usually termed \"data\") as well as unknown parameters and latent variables, with various sorts of relationships among the three types of random variables, as might be described by a graphical model.  As typical in Bayesian inference, the parameters and latent variables are grouped together as \"unobserved variables\". Variational Bayesian methods are primarily used for two purposes:\n\nTo provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables.\nTo derive a lower bound for the marginal likelihood (sometimes called the evidence) of the observed data (i.e. the marginal probability of the data given the model, with marginalization performed over unobserved variables).  This is typically used for performing model selection, the general idea being that a higher marginal likelihood for a given model indicates a better fit of the data by that model and hence a greater probability that the model in question was the one that generated the data. (See also the Bayes factor article.)In the former purpose (that of approximating a posterior probability), variational Bayes is an alternative to Monte Carlo sampling methods\u2014particularly, Markov chain Monte Carlo methods such as Gibbs sampling\u2014for taking a fully Bayesian approach to statistical inference over complex distributions that are difficult to evaluate directly or sample. In particular, whereas Monte Carlo techniques provide a numerical approximation to the exact posterior using a set of samples, variational Bayes provides a locally-optimal, exact analytical solution to an approximation of the posterior.\nVariational Bayes can be seen as an extension of the expectation-maximization (EM) algorithm from maximum a posteriori estimation (MAP estimation) of the single most probable value of each parameter to fully Bayesian estimation which computes (an approximation to) the entire posterior distribution of the parameters and latent variables.  As in EM, it finds a set of optimal parameter values, and it has the same alternating structure as does EM, based on a set of interlocked (mutually dependent) equations that cannot be solved analytically.\nFor many applications, variational Bayes produces solutions of comparable accuracy to Gibbs sampling at greater speed.  However, deriving the set of equations used to update the parameters iteratively often requires a large amount of work compared with deriving the comparable Gibbs sampling equations.  This is the case even for many models that are conceptually quite simple, as is demonstrated below in the case of a basic non-hierarchical model with only two parameters and no latent variables."
    },
    "Neural network": {
        "url": "https://en.wikipedia.org/wiki/Neural_network",
        "summary": "A neural network is a neural circuit of biological neurons, sometimes also called a biological neural network, or a network of artificial neurons or nodes in the case of an artificial neural network.Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1.\nThese artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information."
    },
    "Classification": {
        "url": "https://en.wikipedia.org/wiki/Classification",
        "summary": "Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. \nClassification is the grouping of related facts into classes. \nIt may also refer to   a process which brings together like things and separates unlike things."
    },
    "Alpha": {
        "url": "https://en.wikipedia.org/wiki/Alpha",
        "summary": "Alpha  (uppercase \u0391, lowercase \u03b1; Ancient Greek: \u1f04\u03bb\u03c6\u03b1, \u00e1lpha, or Greek: \u03ac\u03bb\u03c6\u03b1, romanized: \u00e1lfa) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for \"ox\". Letters that arose from alpha include the Latin letter A and the Cyrillic letter \u0410."
    },
    "Prevalence": {
        "url": "https://en.wikipedia.org/wiki/Prevalence",
        "summary": "In epidemiology, prevalence is the proportion of a particular population found to be affected by a medical condition (typically a disease or a risk factor such as smoking or seatbelt use) at a specific time. It is derived by comparing the number of people found to have the condition with the total number of people studied and is usually expressed as a fraction, a percentage, or the number of cases per 10,000 or 100,000 people. Prevalence is most often used in questionnaire studies."
    },
    "Stochastic": {
        "url": "https://en.wikipedia.org/wiki/Stochastic",
        "summary": "Stochastic (; from Ancient Greek  \u03c3\u03c4\u03cc\u03c7\u03bf\u03c2 (st\u00f3khos) 'aim, guess') refers to the property of being well-described by a random probability distribution. Although stochasticity and randomness are distinct in that the former refers to a modeling approach and the latter refers to phenomena themselves, these two terms are often used synonymously. Furthermore, in probability theory, the formal concept of a stochastic process is also referred to as a random process.Stochasticity is used in many different fields, including the natural sciences such as biology, chemistry, ecology,  neuroscience, and physics, as well as technology and engineering fields such as image processing, signal processing, information theory, computer science, cryptography, and telecommunications. It is also used in finance, due to seemingly random changes in financial markets as well as in medicine, linguistics,  music, media, colour theory,  botany, manufacturing, and geomorphology."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input."
    },
    "List of things named after Thomas Bayes": {
        "url": "https://en.wikipedia.org/wiki/List_of_things_named_after_Thomas_Bayes",
        "summary": "Thomas Bayes (/be\u026az/; c. 1701 \u2013 1761) was an English statistician, philosopher, and Presbyterian minister.\nBayesian (/\u02c8be\u026a\u02cc\u0292\u0259n/ or /\u02c8be\u026a\u02ccz\u026a\u0259n/) refers either to a range of concepts and approaches that relate to statistical methods based on Bayes' theorem, or a follower of these methods.A number of things have been named after Thomas Bayes, including:"
    },
    "Machine learning": {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods."
    },
    "Overfitting": {
        "url": "https://en.wikipedia.org/wiki/Overfitting",
        "summary": "In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.:\u200a45\u200aUnderfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.\nThe possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend. \nAs an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. \nThe potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.\nTo lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.\n\n"
    },
    "Misfit stream": {
        "url": "https://en.wikipedia.org/wiki/Misfit_stream",
        "summary": "A misfit stream is  a river that is either too large or too small to have eroded the valley or cave passage in which it flows. This term is also used for a stream or river with meanders that obviously are not proportional in size to the meanders of the valley or meander scars cut into its valley walls. If the misfit stream is too large for either its valley or meanders, it is known as an overfit stream. If the misfit stream is too small for either its valley or meanders, it is known as an underfit stream.The term misfit stream is often incorrectly used as a synonym for an underfit stream. An underfit stream is a type of misfit stream whose discharge is too small to be correlated with either existing channel characteristics, i.e. meander radius, wavelength and channel width, or valley size"
    },
    "Learning": {
        "url": "https://en.wikipedia.org/wiki/Learning",
        "summary": "Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, animals, and some machines; there is also evidence for some kind of learning in certain plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulate from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be \"lost\" from that which cannot be retrieved.Human learning starts at birth (it might even start before in terms of an embryo's need for both interaction with, and freedom within its environment within the womb.) and continues until death as a consequence of ongoing interactions between people and their environment. The nature and processes involved in learning are studied in many established fields (including educational psychology, neuropsychology, experimental psychology, cognitive sciences, and pedagogy), as well as emerging fields of knowledge (e.g. with a shared interest in the topic of learning from safety events such as incidents/accidents, or in collaborative learning health systems). Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event cannot be avoided or escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development.Play has been approached by several theorists as a form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games. For Vygotsky, however, play is the first form of learning language and communication, and the stage where a child begins to understand rules and symbols. This has led to a view that learning in organisms is always related to semiosis, and is often associated with representational systems/activity."
    },
    "Intelligence": {
        "url": "https://en.wikipedia.org/wiki/Intelligence",
        "summary": "Intelligence has been defined in many ways: the capacity for abstraction, logic, understanding, self-awareness, learning, emotional knowledge, reasoning, planning, creativity, critical thinking, and problem-solving. It can be described as the ability to perceive or infer information; and to retain it as knowledge to be applied to adaptive behaviors within an environment or context.The term rose to prominence during the early 1900s. Most psychologists believe that intelligence can be divided into various domains or competencies.\nIntelligence is most often studied in humans but has also been observed in both non-human animals and in plants despite controversy as to whether some of these forms of life exhibit intelligence. Intelligence in computers or other machines is called artificial intelligence.\n\n"
    },
    "Data model": {
        "url": "https://en.wikipedia.org/wiki/Data_model",
        "summary": "A data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner.\nThe corresponding professional activity is called generally data modeling or, more specifically, database design.\nData models are typically specified by a data expert, data specialist, data scientist, data librarian, or a data scholar. \nA data modeling language and notation are often represented in graphical form as diagrams.A data model can sometimes be referred to as a data structure, especially in the context of programming languages. Data models are often complemented by function models, especially in the context of enterprise models.\nA data model explicitly determines the structure of data; conversely, structured data is data organized according to an explicit data model or data structure. Structured data is in contrast to unstructured data and semi-structured data."
    },
    "Conference on Neural Information Processing Systems": {
        "url": "https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems",
        "summary": "The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December. The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts."
    },
    "2015": {
        "url": "https://en.wikipedia.org/wiki/2015",
        "summary": "2015 (MMXV) was a common year starting on Thursday of the Gregorian calendar, the 2015th year of the Common Era (CE) and Anno Domini (AD) designations, the 15th  year of the 3rd millennium and the 21st century, and the  6th   year of the 2010s decade.  \n2015 was designated by the United Nations as:\n\nInternational Year of Light\nInternational Year of Soil\n\n"
    },
    "McGill University": {
        "url": "https://en.wikipedia.org/wiki/McGill_University",
        "summary": "McGill University is an English-language public research university located in Montreal, Quebec, Canada. Founded in 1821 by royal charter, the university bears the name of James McGill, a Scottish merchant whose bequest in 1813 established the University of McGill College. In 1885, the name was officially changed to McGill University. \nMcGill's main campus is on the slope of Mount Royal in downtown Montreal in the borough of Ville-Marie, with a second campus situated in Sainte-Anne-de-Bellevue, 30 kilometres (19 mi) west of the main campus on Montreal Island. The university is one of two members of the Association of American Universities located outside the United States, alongside the University of Toronto, and is the only Canadian member of the Global University Leaders Forum (GULF) within the World Economic Forum. The university offers degrees and diplomas in over 300 fields of study. Most students are enrolled in the six largest faculties: Arts, Science, Medicine, Education, Engineering, and Management.McGill alumni, faculty, and affiliates include 12 Nobel laureates and 147 Rhodes Scholars, as well as 159 Loran Scholars, 18 billionaires, the current prime minister and two former prime ministers of Canada, two Governors General of Canada, and 15 justices of the Supreme Court of Canada. McGill alumni also include 9 Academy Award winners, 13 Grammy Award winners, 13 Emmy Award winners, four Pulitzer Prize winners, and 121 Olympians with over 35 Olympic medals."
    },
    "Canada": {
        "url": "https://en.wikipedia.org/wiki/Canada",
        "summary": "Canada is a country in North America. Its ten provinces and three territories extend from the Atlantic Ocean to the Pacific Ocean and northward into the Arctic Ocean, making it the world's second-largest country by total area, with the world's longest coastline. Its border with the United States is the world's longest international land border. The country is characterized by a wide range of both meteorologic and geological regions. It is a sparsely inhabited country of 40 million people, the vast majority residing south of the 55th parallel in urban areas. Canada's capital is Ottawa and its three largest metropolitan areas are Toronto, Montreal, and Vancouver.\nIndigenous peoples have continuously inhabited what is now Canada for thousands of years. Beginning in the 16th century, British and French expeditions explored and later settled along the Atlantic coast. As a consequence of various armed conflicts, France ceded nearly all of its colonies in North America in 1763. In 1867, with the union of three British North American colonies through Confederation, Canada was formed as a federal dominion of four provinces. This began an accretion of provinces and territories and a process of increasing autonomy from the United Kingdom, highlighted by the Statute of Westminster, 1931, and culminating in the Canada Act 1982, which severed the vestiges of legal dependence on the Parliament of the United Kingdom.\nCanada is a parliamentary democracy and a constitutional monarchy in the Westminster tradition. The country's head of government is the prime minister, who holds office by virtue of their ability to command the confidence of the elected House of Commons and is \"called upon\" by the governor general, representing the monarch of Canada, the head of state. The country is a Commonwealth realm and is officially bilingual (English and French) in the federal jurisdiction. It is very highly ranked in international measurements of government transparency, quality of life, economic competitiveness, innovation, education and gender equality. It is one of the world's most ethnically diverse and multicultural nations, the product of large-scale immigration. Canada's long and complex relationship with the United States has had a significant impact on its history, economy, and culture.\nA developed country, Canada has a high nominal per capita income globally and its advanced economy ranks among the largest in the world, relying chiefly upon its abundant natural resources and well-developed international trade networks. Canada is recognized as a middle power for its role in international affairs, with a tendency to pursue multilateral solutions. Canada's peacekeeping role during the 20th century has had a significant influence on its global image. Canada is part of multiple major international and intergovernmental institutions."
    },
    "Montreal": {
        "url": "https://en.wikipedia.org/wiki/Montreal",
        "summary": "Montreal (  MUN-tree-AWL; French: Montr\u00e9al [m\u0254\u0303\u0281eal] ) is the second most populous city in Canada, the tenth most populous city in North America, and the most populous city in the province of Quebec. Founded in 1642 as Ville-Marie, or \"City of Mary\", it is named after Mount Royal, the triple-peaked hill around which the early city of Ville-Marie was built. The city is centred on the Island of Montreal, which obtained its name from the same origin as the city, and a few much smaller peripheral islands, the largest of which is \u00cele Bizard. The city is 196 km (122 mi) east of the national capital Ottawa, and 258 km (160 mi) southwest of the provincial capital, Quebec City.\nAs of 2021, the city has a population of 1,762,949, and a metropolitan  population of 4,291,732, making it the second-largest city, and second-largest metropolitan area in Canada. French is the city's official language. In 2021, 85.7% of the population of the city of Montreal considered themselves fluent in French while 90.2% could speak it in the metropolitan area. Montreal is one of the most bilingual cities in Quebec and Canada, with 58.5% of the population able to speak both English and French.Historically the commercial capital of Canada, Montreal was surpassed in population and economic strength by Toronto in the 1970s. Montreal remains an important centre of art, culture, literature, film and television, music, commerce, aerospace, transport, finance, pharmaceuticals, technology, design, education, tourism, food, fashion, video game development, and world affairs. Montreal is the location of the headquarters of the International Civil Aviation Organization, and was named a UNESCO City of Design in 2006. In 2017, Montreal was ranked the 12th-most liveable city in the world by the Economist Intelligence Unit in its annual Global Liveability Ranking, although it slipped to rank 40 in the 2021 index, primarily due to stress on the healthcare system from the COVID-19 pandemic. It is regularly ranked as a top ten city in the world to be a university student in the QS World University Rankings.Montreal has hosted multiple international conferences and events, including the 1967 International and Universal Exposition and the 1976 Summer Olympics. It is the only Canadian city to have held the Summer Olympics. In 2018, Montreal was ranked as a global city. The city hosts the Canadian Grand Prix of Formula One; the Montreal International Jazz Festival, the largest jazz festival in the world; the Just for Laughs festival, the largest comedy festival in the world; and Les Francos de Montr\u00e9al, the largest French-language music festival in the world. In sports, it is home to the Montreal Canadiens of the National Hockey League, who have won the Stanley Cup more times than any other team."
    },
    "MIT Press": {
        "url": "https://en.wikipedia.org/wiki/MIT_Press",
        "summary": "The MIT Press is a university press affiliated with the Massachusetts Institute of Technology (MIT) in Cambridge, Massachusetts. The Press has been a pioneer in the Open Access movement in academic publishing and publishes a number of academic journals. The organization also operates the MIT Press Bookstore, which is one of the few retail bookstores run by a university publisher.\n\n"
    },
    "Cambridge, Massachusetts": {
        "url": "https://en.wikipedia.org/wiki/Cambridge,_Massachusetts",
        "summary": "Cambridge ( KAYM-brij) is a city in Middlesex County, Massachusetts, in the United States. It is a major suburb in the Greater Boston metropolitan area, located directly across the Charles River from Boston. The city's population as of the 2020 U.S. census was 118,403, making it the most populous city in the county, the 4th most populous city in the Commonwealth, behind Boston, Worcester, and Springfield, and ninth most populous city in New England. It was named in honor of the University of Cambridge in England, which was an important center of the Puritan theology that was embraced by the town's founders.:\u200a18\u200aCambridge is known globally as home to two of the world's most prestigious universities. Harvard University, an Ivy League university founded in Cambridge in 1636, is the oldest institution of higher learning in the United States and has routinely been ranked as one of the best universities in the world. The Massachusetts Institute of Technology (MIT), founded in 1861, is also located in Cambridge and has been similarly ranked highly among the world's best universities. Lesley University and Hult International Business School also are based in Cambridge. Radcliffe College, an elite women's liberal arts college, also was based in Cambridge from its 1879 founding until its assimiliation into Harvard in 1999.\nKendall Square, near MIT in the eastern part of Cambridge, has been called \"the most innovative square mile on the planet\" due to the high concentration of startup companies that have emerged there since 2010."
    },
    "Journal of the Royal Statistical Society": {
        "url": "https://en.wikipedia.org/wiki/Journal_of_the_Royal_Statistical_Society",
        "summary": "The Journal of the Royal Statistical Society is a peer-reviewed scientific journal of statistics. It comprises three series and is published by Oxford University Press for the Royal Statistical Society."
    },
    "Journal of the ACM": {
        "url": "https://en.wikipedia.org/wiki/Journal_of_the_ACM",
        "summary": "The Journal of the ACM is a peer-reviewed scientific journal covering computer science in general, especially theoretical aspects. It is an official journal of the Association for Computing Machinery. Its current editor-in-chief is Venkatesan  Guruswami.\nThe journal was established in 1954 and \"computer scientists universally hold the Journal of the ACM in high esteem\".\n\n"
    },
    "International Conference on Computer Vision": {
        "url": "https://en.wikipedia.org/wiki/International_Conference_on_Computer_Vision",
        "summary": "The International Conference on Computer Vision (ICCV) is a research conference sponsored by the Institute of Electrical and Electronics Engineers (IEEE) held every other year. It is considered to be one of the top conferences in computer vision, alongside CVPR and ECCV,\nand it is held on years in which ECCV is not.\nThe conference is usually spread over four to five days. Typically, experts in the focus areas give tutorial talks on the first day, then the technical sessions (and poster sessions in parallel) follow. Recent conferences have also had an increasing number of focused workshops and a commercial exhibition."
    },
    "Institute of Electrical and Electronics Engineers": {
        "url": "https://en.wikipedia.org/wiki/Institute_of_Electrical_and_Electronics_Engineers",
        "summary": "The Institute of Electrical and Electronics Engineers (IEEE) is a 501(c)(3) professional association for electronics engineering, electrical engineering, and other related disciplines with its corporate office in New York City and its operations center in Piscataway, New Jersey. The IEEE was formed from the amalgamation of the American Institute of Electrical Engineers and the Institute of Radio Engineers in 1963."
    },
    "Miocene": {
        "url": "https://en.wikipedia.org/wiki/Miocene",
        "summary": "The Miocene ( MY-\u0259-seen, -\u2060oh-) is the first geological epoch of the Neogene Period and extends from about 23.03 to 5.333 million years ago (Ma). The Miocene was named by Scottish geologist Charles Lyell; the name comes from the Greek words \u03bc\u03b5\u03af\u03c9\u03bd (me\u00ed\u014dn, \"less\") and \u03ba\u03b1\u03b9\u03bd\u03cc\u03c2 (kain\u00f3s, \"new\") and means \"less recent\" because it has 18% fewer modern marine invertebrates than the Pliocene has. The Miocene is preceded by the Oligocene and is followed by the Pliocene.\nAs Earth went from the Oligocene through the Miocene and into the Pliocene, the climate slowly cooled towards a series of ice ages. The Miocene boundaries are not marked by a single distinct global event but consist rather of regionally defined boundaries between the warmer Oligocene and the cooler Pliocene Epoch.\nDuring the Early Miocene, Afro-Arabia collided with Eurasia, severing the connection between the Mediterranean and Indian Oceans, and allowing a faunal interchange to occur between Eurasia and Africa, including the dispersal of proboscideans into Eurasia. During the late Miocene, the connections between the Atlantic and Mediterranean closed, causing the Mediterranean Sea to nearly completely evaporate, in an event called the Messinian salinity crisis. The Strait of Gibraltar opened and the Mediterranean refilled at the Miocene\u2013Pliocene boundary, in an event called the Zanclean flood.\nThe apes first evolved, arose, and diversified during the early Miocene (Aquitanian and Burdigalian Stages), becoming widespread in the Old World. By the end of this epoch and the start of the following one, the ancestors of humans had split away from the ancestors of the chimpanzees to follow their own evolutionary path during the final Messinian Stage (7.5\u20135.3 Ma) of the Miocene. As in the Oligocene before it, grasslands continued to expand and forests to dwindle in extent. In the seas of the Miocene, kelp forests made their first appearance and soon became one of Earth's most productive ecosystems.The plants and animals of the Miocene were recognizably modern. Mammals and birds were well-established. Whales, pinnipeds, and kelp spread.\nThe Miocene is of particular interest to geologists and palaeoclimatologists as major phases of the geology of the Himalaya occurred during the Miocene, affecting monsoonal patterns in Asia, which were interlinked with glacial periods in the northern hemisphere."
    },
    "List of time periods": {
        "url": "https://en.wikipedia.org/wiki/List_of_time_periods",
        "summary": "The categorisation of the past into discrete, quantified named blocks of time is called periodization. This is a list of such named time periods as defined in various fields of study.\nThese can be divided broadly into prehistorical periods and historical periods (when written records began to be kept).\nIn archaeology and anthropology, prehistory is subdivided around the three-age system, this list includes the use of the three-age system as well as a number of various designation used in reference to sub-ages within the traditional three.\nThe dates for each age can vary by region. On the geologic time scale, the Holocene epoch starts at the end of the last glacial period of the current ice age (c. 10,000 BC) and continues to the present. The beginning of the Mesolithic is usually considered to correspond to the beginning of the Holocene epoch."
    },
    "International Conference on Learning Representations": {
        "url": "https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations",
        "summary": "The International Conference on Learning Representations (ICLR) is a machine learning conference typically held in late April or early May each year. The conference includes invited talks as well as oral and poster presentations of refereed papers. Since its inception in 2013, ICLR has employed an open peer review process to referee paper submissions (based on models proposed by Yann LeCun). In 2019, there were 1591 paper submissions, of which 500 accepted with poster presentations (31%) and 24 with oral presentations (1.5%).. In 2021, there were 2997 paper submissions, of which 860 were accepted (29%).."
    },
    "IEEE Transactions on Signal Processing": {
        "url": "https://en.wikipedia.org/wiki/IEEE_Transactions_on_Signal_Processing",
        "summary": "The IEEE Transactions on Signal Processing is a biweekly peer-reviewed scientific journal published by the Institute of Electrical and Electronics Engineers covering research on signal processing. It was established in 1953 as the IRE Transactions on Audio, renamed to IEEE Transactions on Audio and Electroacoustics in 1966 and to IEEE Transactions on Acoustics, Speech, and Signal Processing in 1974, before obtaining its current name in 1992. The journal is abstracted and indexed in MEDLINE/PubMed and the Science Citation Index Expanded. According to the Journal Citation Reports, the journal has a 2022 impact factor of 5.4. The editor-in-chief is Wing-Kin (Ken) Ma (Chinese University of Hong Kong)."
    }
}
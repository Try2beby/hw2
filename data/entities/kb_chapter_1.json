{
    "Cybernetics": {
        "url": "https://en.wikipedia.org/wiki/Cybernetics",
        "summary": "Cybernetics is a wide-ranging field concerned with circular causal processes such as feedback. The field is named after an example of circular causal feedback\u2014that of steering a ship (the ancient Greek \u03ba\u03c5\u03b2\u03b5\u03c1\u03bd\u03ae\u03c4\u03b7\u03c2 (kybern\u1e17t\u0113s) means \"helmsperson\"). In steering a ship, the helmsperson adjusts their steering in continual response to the effect it is observed as having, forming a feedback loop through which a steady course can be maintained in a changing environment, responding to disturbances from cross winds and tide. Cybernetics is concerned with the principles of circular causal processes such as steering however they are embodied, including in ecological, technological, biological, cognitive and social systems and also in the context of practical activities such as designing, learning, managing, etc. Cybernetics' transdisciplinary character has meant that it intersects with a number of other fields, leading to it having both wide influence and diverse interpretations.\n\n"
    },
    "Deep learning": {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "summary": "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog."
    },
    "Perceptron": {
        "url": "https://en.wikipedia.org/wiki/Perceptron",
        "summary": "In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector."
    },
    "Neural network": {
        "url": "https://en.wikipedia.org/wiki/Neural_network",
        "summary": "A neural network is a neural circuit of biological neurons, sometimes also called a biological neural network, or a network of artificial neurons or nodes in the case of an artificial neural network.Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1.\nThese artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information."
    },
    "Artificial neural network": {
        "url": "https://en.wikipedia.org/wiki/Artificial_neural_network",
        "summary": "Artificial neural networks (ANNs, also shortened to neural networks (NNs) or neural nets) are a branch of machine learning models that are built using principles of neuronal organization discovered by connectionism in the biological neural networks constituting animal brains.An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.\n\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times."
    },
    "Stochastic gradient descent": {
        "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent",
        "summary": "Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.While the basic idea behind stochastic approximation can be traced back to the Robbins\u2013Monro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Neuron": {
        "url": "https://en.wikipedia.org/wiki/Neuron",
        "summary": "Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \nThe neuron is the main component of nervous tissue in all animals except sponges and placozoa. Non-animals like plants and fungi do not have nerve cells. The ability to generate electric signals first appeared in evolution 700 million years ago. 800 million years ago, predecessors of neurons were the peptidergic secretory cells. They eventually gained new gene modules which enabled cells to create post-synaptic scaffolds and ion channels that generate fast electrical signals. The ability to generate electric signals was a key innovation in the evolution of the nervous system.Neurons are typically classified into three types based on their function. Sensory neurons respond to stimuli such as touch, sound, or light that affect the cells of the sensory organs, and they send signals to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to control everything from muscle contractions to glandular output. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord. When multiple neurons are functionally connected together, they form what is called a neural circuit.\nNeurons are special cells which are made up of some structures that are common to all other eukaryotic cells such as the cell body (soma), a nucleus, smooth and rough endoplasmic reticulum, Golgi apparatus, mitochondria, and other cellular components. Additionally, neurons have other unique structures  such as dendrites, and a single axon. The soma is a compact structure, and the axon and dendrites are filaments extruding from the soma. Dendrites typically branch profusely and extend a few hundred micrometers from the soma. The axon leaves the soma at a swelling called the axon hillock and travels for as far as 1 meter in humans or more in other species. It branches but usually maintains a constant diameter. At the farthest tip of the axon's branches are axon terminals, where the neuron can transmit a signal across the synapse to another cell. Neurons may lack dendrites or have no axon. The term neurite is used to describe either a dendrite or an axon, particularly when the cell is undifferentiated.\nMost neurons receive signals via the dendrites and soma and send out signals down the axon. At the majority of synapses, signals cross from the axon of one neuron to a dendrite of another. However, synapses can connect an axon to another axon or a dendrite to another dendrite.\nThe signaling process is partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage gradients across their membranes. If the voltage changes by a large enough amount over a short interval, the neuron generates an all-or-nothing electrochemical pulse called an action potential. This potential travels rapidly along the axon and activates synaptic connections as it reaches them. Synaptic signals may be excitatory or inhibitory, increasing or reducing the net voltage that reaches the soma.\nIn most cases, neurons are generated by neural stem cells during brain development and childhood. Neurogenesis largely ceases during adulthood in most areas of the brain."
    },
    "Neuroscience": {
        "url": "https://en.wikipedia.org/wiki/Neuroscience",
        "summary": "Neuroscience is the scientific study of the nervous system (the brain, spinal cord, and peripheral nervous system), its functions and disorders. It is a multidisciplinary science that combines physiology, anatomy, molecular biology, developmental biology, cytology, psychology, physics, computer science, chemistry, medicine, statistics, and mathematical modeling to understand the fundamental and emergent properties of neurons, glia and neural circuits. The understanding of the biological basis of learning, memory, behavior, perception, and consciousness has been described by Eric Kandel as the \"epic challenge\" of the biological sciences.The scope of neuroscience has broadened over time to include different approaches used to study the nervous system at different scales. The techniques used by neuroscientists have expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory, motor and cognitive tasks in the brain.\n\n"
    },
    "Rodent": {
        "url": "https://en.wikipedia.org/wiki/Rodent",
        "summary": "Rodents (from Latin rodere, 'to gnaw') are mammals of the order Rodentia (), which are characterized by a single pair of continuously growing incisors in each of the upper and lower jaws. About 40% of all mammal species are rodents. They are native to all major land masses except for New Zealand, Antarctica, and several oceanic islands, though they have subsequently been introduced to most of these land masses by human activity.\nRodents are extremely diverse in their ecology and lifestyles and can be found in almost every terrestrial habitat, including human-made environments. Species can be arboreal, fossorial (burrowing), saltatorial/richochetal (leaping on their hind legs), or semiaquatic. However, all rodents share several morphological features, including having only a single upper and lower pair of ever-growing incisors. Well-known rodents include mice, rats, squirrels, prairie dogs, porcupines, beavers, guinea pigs, and hamsters. Rabbits, hares, and pikas, who also have incisors that grow continuously (but have two pairs of upper incisors instead of one), were once included with them, but are now considered to be in a separate order, the Lagomorpha. Nonetheless, Rodentia and Lagomorpha are sister groups, sharing a single common ancestor and forming the clade of Glires.\nMost rodents are small animals with robust bodies, short limbs, and long tails. They use their sharp incisors to gnaw food, excavate burrows, and defend themselves. Most eat seeds or other plant material, but some have more varied diets. They tend to be social animals and many species live in societies with complex ways of communicating with each other. Mating among rodents can vary from monogamy, to polygyny, to promiscuity. Many have litters of underdeveloped, altricial young, while others are precocial (relatively well developed) at birth.\nThe rodent fossil record dates back to the Paleocene on the supercontinent of Laurasia. Rodents greatly diversified in the Eocene, as they spread across continents, sometimes even crossing oceans. Rodents reached both South America and Madagascar from Africa and, until the arrival of Homo sapiens, were the only terrestrial placental mammals to reach and colonize Australia.\nRodents have been used as food, for clothing, as pets, and as laboratory animals in research. Some species, in particular, the brown rat, the black rat, and the house mouse, are serious pests, eating and spoiling food stored by humans and spreading diseases. Accidentally introduced species of rodents are often considered to be invasive and have caused the extinction of numerous species, such as island birds, the dodo being an example, previously isolated from land-based predators."
    },
    "Retinal": {
        "url": "https://en.wikipedia.org/wiki/Retinal",
        "summary": "Retinal (also known as retinaldehyde) is a polyene chromophore. Retinal, bound to proteins called opsins, is the chemical basis of visual phototransduction, the light-detection stage of visual perception (vision).\nSome microorganisms use retinal to convert light into metabolic energy. In fact, a recent study suggests most living organisms on our planet ~3 billion years ago used retinal to convert sunlight into energy rather than chlorophyll. Since retinal absorbs mostly green light and transmits purple light, this gave rise to the Purple Earth Hypothesis.There are many forms of vitamin A \u2014 all of which are converted to retinal, which cannot be made without them. Retinal itself is considered to be a form of vitamin A when eaten by an animal. The number of different molecules that can be converted to retinal varies from species to species. Retinal was originally called retinene, and was renamed after it was discovered to be vitamin A aldehyde.Vertebrate animals ingest retinal directly from meat, or they produce retinal from carotenoids \u2014 either from \u03b1-carotene or \u03b2-carotene \u2014 both of which are carotenes. They also produce it from \u03b2-cryptoxanthin, a type of xanthophyll. These carotenoids must be obtained from plants or other photosynthetic organisms. No other carotenoids can be converted by animals to retinal. Some carnivores cannot convert any carotenoids at all. The other main forms of vitamin A \u2014 retinol and a partially active form, retinoic acid \u2014 may both be produced from retinal.\nInvertebrates such as insects and squid use hydroxylated forms of retinal in their visual systems, which derive from conversion from other xanthophylls."
    },
    "Retina": {
        "url": "https://en.wikipedia.org/wiki/Retina",
        "summary": "The retina (from Latin  rete 'net'; pl.\u2009retinae or retinas) is the innermost, light-sensitive layer of tissue of the eye of most vertebrates and some molluscs. The optics of the eye create a focused two-dimensional image of the visual world on the retina, which then processes that image within the retina and sends nerve impulses along the optic nerve to the visual cortex to create visual perception. The retina serves a function which is in many ways analogous to that of the film or image sensor in a camera.\nThe neural retina consists of several layers of neurons interconnected by synapses and is supported by an outer layer of pigmented epithelial cells. The primary light-sensing cells in the retina are the photoreceptor cells, which are of two types: rods and cones. Rods function mainly in dim light and provide monochromatic vision. Cones function in well-lit conditions and are responsible for the perception of colour through the use of a range of opsins, as well as high-acuity vision used for tasks such as reading. A third type of light-sensing cell, the photosensitive ganglion cell, is important for entrainment of circadian rhythms and reflexive responses such as the pupillary light reflex.\nLight striking the retina initiates a cascade of chemical and electrical events that ultimately trigger nerve impulses that are sent to various visual centres of the brain through the fibres of the optic nerve. Neural signals from the rods and cones undergo processing by other neurons, whose output takes the form of action potentials in retinal ganglion cells whose axons form the optic nerve.In vertebrate embryonic development, the retina and the optic nerve originate as outgrowths of the developing brain, specifically the embryonic diencephalon; thus, the retina is considered part of the central nervous system (CNS) and is actually brain tissue. It is the only part of the CNS that can be visualized noninvasively. Like most of the brain, the retina is isolated from the vascular system by the blood\u2013brain barrier. The retina is the part of the body with the greatest continuous energy demand."
    },
    "Cognitive science": {
        "url": "https://en.wikipedia.org/wiki/Cognitive_science",
        "summary": "Cognitive science is the interdisciplinary, scientific study of the mind and its processes with input from linguistics, psychology, neuroscience, philosophy, computer science/artificial intelligence, and anthropology. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology. The typical analysis of cognitive science spans many levels of organization, from learning and decision to logic and planning; from neural circuitry to modular brain organization. One of the fundamental concepts of cognitive science is that \"thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures.\"The goal of cognitive science is to understand and formulate the principles of intelligence with the hope that this will lead to a better comprehension of the mind and of learning.\nThe cognitive sciences began as an intellectual movement in the 1950s often referred to as the cognitive revolution.\n\n"
    },
    "Geoffrey Hinton": {
        "url": "https://en.wikipedia.org/wiki/Geoffrey_Hinton",
        "summary": "Geoffrey Everest Hinton  (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing his departure from Google in May 2023, citing concerns about the risks of artificial intelligence (AI) technology. In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularised the backpropagation algorithm for training multi-layer neural networks, although they were not the first to propose the approach. Hinton is viewed as a leading figure in the deep learning community. The dramatic image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky and Ilya Sutskever for the ImageNet challenge 2012 was a breakthrough in the field of computer vision.Hinton received the 2018 Turing Award, often referred to as the \"Nobel Prize of Computing\", together with Yoshua Bengio and Yann LeCun, for their work on deep learning. They are sometimes referred to as the \"Godfathers of Deep Learning\", and have continued to give public talks together.In May 2023, Hinton announced his resignation from Google to be able to \"freely speak out about the risks of A.I.\" He has voiced concerns about deliberate misuse by malicious actors, technological unemployment, and existential risk from artificial general intelligence.\n\n"
    },
    "University of Toronto": {
        "url": "https://en.wikipedia.org/wiki/University_of_Toronto",
        "summary": "The University of Toronto (UToronto or U of T) is a public research university in Toronto, Ontario, Canada, located on the grounds that surround Queen's Park. It was founded by royal charter in 1827 as King's College, the first institution of higher learning in Upper Canada. Originally controlled by the Church of England, the university assumed its present name in 1850 upon becoming a secular institution. As a collegiate university, it comprises 11 colleges each with substantial autonomy on financial and institutional affairs and significant differences in character and history. The university maintains three campuses, the oldest of which is St. George, located in downtown Toronto. The other two satellite campuses are located in Scarborough and Mississauga.\nThe University of Toronto offers over 700 undergraduate and 200 graduate programs. The university receives the most annual scientific research funding and endowment of any Canadian university and is one of two members of the Association of American Universities outside the United States, alongside McGill University. Academically, the University of Toronto is noted for influential movements and curricula in literary criticism and communication theory, known collectively as the Toronto School. \nThe university was the birthplace of insulin and stem cell research, the first artificial cardiac pacemaker, and the site of the first successful lung transplant and nerve transplant. The university was also home to the first electron microscope, the development of deep learning, neural network, multi-touch technology, the identification of the first black hole Cygnus X-1, and the development of the theory of NP-completeness. The University of Toronto is the recipient of both the single largest philanthropic gift in Canadian history, a $250 million donation from James and Louise Temerty in 2020, and the largest ever research grant in Canada, a $200 million grant from the Government of Canada in 2023.The Varsity Blues are the athletic teams that represent the university in intercollegiate league matches, primarily within U Sports, with ties to gridiron football, rowing and ice hockey. The earliest recorded instance of gridiron football occurred at University of Toronto's University College in November 1861. The university's Hart House is an early example of the North American student centre, simultaneously serving cultural, intellectual, and recreational interests within its large Gothic-revival complex.\nUniversity of Toronto alumni include five Prime Ministers of Canada (including William Lyon Mackenzie King and Lester B. Pearson), three Governors Generals of Canada, nine foreign leaders, and 17 justices of the Supreme Court of Canada. As of 2019, 12 Nobel laureates, six Turing Award winners, 94 Rhodes Scholars, and one Fields Medalist have been affiliated with the university."
    },
    "Yoshua Bengio": {
        "url": "https://en.wikipedia.org/wiki/Yoshua_Bengio",
        "summary": "Yoshua Bengio  (born March 5, 1964) is a Canadian computer scientist, most noted for his work on artificial neural networks and deep learning. He is a professor at the Department of Computer Science and Operations Research at the Universit\u00e9 de Montr\u00e9al and scientific director of the Montreal Institute for Learning Algorithms (MILA).Bengio received the 2018 ACM A.M. Turing Award (often referred to as the \"Nobel Prize of Computing\"), together with Geoffrey Hinton and Yann LeCun, for their work on deep learning. Bengio, Hinton, and LeCun are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\". As of May 2023, he has the highest h-index of any computer scientist."
    },
    "Universit\u00e9 de Montr\u00e9al": {
        "url": "https://en.wikipedia.org/wiki/Universit%C3%A9_de_Montr%C3%A9al",
        "summary": "The Universit\u00e9 de Montr\u00e9al (UdeM; French pronunciation: [yniv\u025b\u0281site d\u0259 m\u0254\u0303\u0281eal]; translates to University of Montreal) is a French-language public research university in Montreal, Quebec, Canada. The university's main campus is located  in the C\u00f4te-des-Neiges neighborhood of C\u00f4te-des-Neiges\u2013Notre-Dame-de-Gr\u00e2ce on Mount Royal near the Outremont Summit (also called Mount Murray), in the borough of Outremont. The institution comprises thirteen faculties, more than sixty departments and two affiliated schools: the Polytechnique Montr\u00e9al (School of Engineering; formerly the \u00c9cole polytechnique de Montr\u00e9al) and HEC Montr\u00e9al (School of Business). It offers more than 650 undergraduate programmes and graduate programmes, including 71 doctoral programmes.\nThe university was founded as a satellite campus of the Universit\u00e9 Laval in 1878. It became an independent institution after it was issued a papal charter in 1919 and a provincial charter in 1920. Universit\u00e9 de Montr\u00e9al moved from Montreal's Quartier Latin to its present location at Mount Royal in 1942. It was made a secular institution with the passing of another provincial charter in 1967.\nThe school is co-educational, and has 34,335 undergraduate and 11,925 post-graduate students (excluding affiliated schools). Alumni and former students reside across Canada and around the world, with notable alumni serving as government officials, academics, and business leaders."
    },
    "Unsupervised": {
        "url": "https://en.wikipedia.org/wiki/Unsupervised",
        "summary": "Unsupervised is an American adult animated sitcom created by David Hornsby, Rob Rosell, and Scott Marder which ran on FX from January 19 to December 20, 2012. The show was created, and for the most part, written by David Hornsby, Scott Marder, and Rob Rosell.On November 17, 2012, the series was canceled after one season."
    },
    "Machine learning": {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods."
    },
    "Nervous system": {
        "url": "https://en.wikipedia.org/wiki/Nervous_system",
        "summary": "In biology, the nervous system is the highly complex part of an animal that coordinates its actions and sensory information by transmitting signals to and from different parts of its body. The nervous system detects environmental changes that impact the body, then works in tandem with the endocrine system to respond to such events. Nervous tissue first arose in wormlike organisms about 550 to 600 million years ago. In vertebrates it consists of two main parts, the central nervous system (CNS) and the peripheral nervous system (PNS). The CNS consists of the brain and spinal cord. The PNS consists mainly of nerves, which are enclosed bundles of the long fibers, or axons, that connect the CNS to every other part of the body. Nerves that transmit signals from the brain are called motor nerves or efferent nerves, while those nerves that transmit information from the body to the CNS are called sensory nerves or afferent. Spinal nerves are mixed nerves that serve both functions. The PNS is divided into three separate subsystems, the somatic, autonomic, and enteric nervous systems. Somatic nerves mediate voluntary movement. The autonomic nervous system is further subdivided into the sympathetic and the parasympathetic nervous systems. The sympathetic nervous system is activated in cases of emergencies to mobilize energy, while the parasympathetic nervous system is activated when organisms are in a relaxed state. The enteric nervous system functions to control the gastrointestinal system. Both autonomic and enteric nervous systems function involuntarily. Nerves that exit from the cranium are called cranial nerves while those exiting from the spinal cord are called spinal nerves.\nAt the cellular level, the nervous system is defined by the presence of a special type of cell, called the neuron. Neurons have special structures that allow them to send signals rapidly and precisely to other cells. They send these signals in the form of electrochemical impulses traveling along thin fibers called axons, which can be directly transmitted to neighboring cells through electrical synapses or cause chemicals called neurotransmitters to be released at chemical synapses. A cell that receives a synaptic signal from a neuron may be excited, inhibited, or otherwise modulated. The connections between neurons can form neural pathways, neural circuits, and larger networks that generate an organism's perception of the world and determine its behavior. Along with neurons, the nervous system contains other specialized cells called glial cells (or simply glia), which provide structural and metabolic support. Many of the cells and vasculature channels within the nervous system make up the neurovascular unit, which regulates cerebral blood flow in order to rapidly satisfy the high energy demands of activated neurons.Nervous systems are found in most multicellular animals, but vary greatly in complexity. The only multicellular animals that have no nervous system at all are sponges, placozoans, and mesozoans, which have very simple body plans. The nervous systems of the radially symmetric organisms ctenophores (comb jellies) and cnidarians (which include anemones, hydras, corals and jellyfish) consist of a diffuse nerve net. All other animal species, with the exception of a few types of worm, have a nervous system containing a brain, a central cord (or two cords running in parallel), and nerves radiating from the brain and central cord. The size of the nervous system ranges from a few hundred cells in the simplest worms, to around 300 billion cells in African elephants.The central nervous system functions to send signals from one cell to others, or from one part of the body to others and to receive feedback. Malfunction of the nervous system can occur as a result of genetic defects, physical damage due to trauma or toxicity, infection, or simply senescence. The medical specialty of neurology studies disorders of the nervous system and looks for interventions that can prevent or treat them. In the peripheral nervous system, the most common problem is the failure of nerve conduction, which can be due to different causes including diabetic neuropathy and demyelinating disorders such as multiple sclerosis and amyotrophic lateral sclerosis. Neuroscience is the field of science that focuses on the study of the nervous system."
    },
    "Graphics processing unit": {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\n"
    },
    "Reinforcement learning": {
        "url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
        "summary": "Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process and they target large Markov decision processes where exact methods become infeasible."
    },
    "Google DeepMind": {
        "url": "https://en.wikipedia.org/wiki/Google_DeepMind",
        "summary": "DeepMind Technologies Limited, doing business as Google DeepMind, is a British-American artificial intelligence research laboratory which serves as a subsidiary of Google. Founded in the UK in 2010, it was acquired by Google in 2014, The company is based in London, with research centres in Canada, France, Germany and the United States.\nGoogle DeepMind has created neural network models that learn how to play video games in a fashion similar to that of humans, as well as Neural Turing machines (neural networks that can access external memory like a conventional Turing machine), resulting in a computer that loosely resembles short-term memory in the human brain.DeepMind made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, a world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning. In 2020, DeepMind made significant advances in the problem of protein folding with AlphaFold. In July 2022, it was announced that over 200 million predicted protein structures, representing virtually all known proteins, would be released on the AlphaFold database.DeepMind posted a blog post on 28 April 2022 on a single visual language model (VLM) named Flamingo that can accurately describe a picture of something with just a few training images. In July 2022, DeepMind announced the development of DeepNash, a model-free multi-agent reinforcement learning system capable of playing the board game Stratego at the level of a human expert. The company merged with Google AI's Google Brain division to become Google DeepMind in April 2023."
    }
}
{
    "Deep learning": {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "summary": "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog."
    },
    "Machine learning": {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods."
    },
    "Array": {
        "url": "https://en.wikipedia.org/wiki/Array",
        "summary": "An array is a systematic arrangement of similar objects, usually in rows and columns.\n\nThings called an array include:\n\n"
    },
    "Tensor": {
        "url": "https://en.wikipedia.org/wiki/Tensor",
        "summary": "In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors. There are many types of tensors, including scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system; those components form an array, which can be thought of as a high-dimensional matrix. \nTensors have become important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as mechanics (stress, elasticity, fluid mechanics, moment of inertia, ...), electrodynamics (electromagnetic tensor, Maxwell tensor, permittivity, magnetic susceptibility, ...), general relativity (stress\u2013energy tensor, curvature tensor, ...) and others. In applications, it is common to study situations in which a different tensor can occur at each point of an object; for example the stress within an object may vary from one location to another. This leads to the concept of a tensor field. In some areas, tensor fields are so ubiquitous that they are often simply called \"tensors\".\nTullio Levi-Civita and Gregorio Ricci-Curbastro popularised tensors in 1900 \u2013 continuing the earlier work of Bernhard Riemann, Elwin Bruno Christoffel, and others \u2013 as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.\n\n"
    },
    "Alpha": {
        "url": "https://en.wikipedia.org/wiki/Alpha",
        "summary": "Alpha  (uppercase \u0391, lowercase \u03b1; Ancient Greek: \u1f04\u03bb\u03c6\u03b1, \u00e1lpha, or Greek: \u03ac\u03bb\u03c6\u03b1, romanized: \u00e1lfa) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for \"ox\". Letters that arose from alpha include the Latin letter A and the Cyrillic letter \u0410."
    },
    "Y": {
        "url": "https://en.wikipedia.org/wiki/Y",
        "summary": "Y, or y, is the 25th and penultimate letter of the Latin alphabet, used in the modern English alphabet, the alphabets of other western European languages and others worldwide. According to some authorities, it is the sixth (or seventh if including W) vowel letter of the English alphabet.  In the English writing system, it mostly represents a vowel and seldom a consonant, and in other orthographies it may represent a vowel or a consonant. Its name in English is wye (pronounced ), plural wyes."
    },
    "Apathy": {
        "url": "https://en.wikipedia.org/wiki/Apathy",
        "summary": "Apathy is a lack of feeling, emotion, interest, or concern about something. It is a state of indifference, or the suppression of emotions such as concern, excitement, motivation, or passion. An apathetic individual has an absence of interest in or concern about emotional, social, spiritual, philosophical, virtual, or physical life and the world. Apathy can also be defined as a person's lack of goal orientation. Apathy falls in the less extreme spectrum of diminished motivation, with abulia in the middle and akinetic mutism being more extreme than both apathy and abulia.The apathetic may lack a sense of purpose, worth, or meaning in their life. People with severe apathy tend to have a lower quality of life and are at a higher risk for mortality and early institutionalization. They may also exhibit insensibility or sluggishness. In positive psychology, apathy is described as a result of the individuals' feeling they do not possess the level of skill required to confront a challenge (i.e. \"flow\"). It may also be a result of perceiving no challenge at all (e.g., the challenge is irrelevant to them, or conversely, they have learned helplessness). Apathy is usually felt only in the short term, but sometimes it becomes a long-term or even lifelong state, often leading to deeper social and psychological issues.Apathy should be distinguished from reduced affect display, which refers to reduced emotional expression but not necessarily reduced emotion.\nPathological apathy, characterized by extreme forms of apathy, is now known to occur in many different brain disorders, including neurodegenerative conditions often associated with dementia such as Alzheimer's disease, Parkinson's disease, and psychiatric disorders such as schizophrenia. Although many patients with pathological apathy also have depression, several studies have shown that the two syndromes are dissociable: apathy can occur independent of depression and vice versa."
    },
    "2": {
        "url": "https://en.wikipedia.org/wiki/2",
        "summary": "2 (two) is a number, numeral and digit. It is the natural number following 1 and preceding 3. It is the smallest and only even prime number. Because it forms the basis of a duality, it has religious and spiritual significance in many cultures."
    },
    "3": {
        "url": "https://en.wikipedia.org/wiki/3",
        "summary": "3 (three) is a number, numeral and digit. It is the natural number following 2 and preceding 4, and is the smallest odd prime number and the only prime preceding a square number. It has religious or cultural significance in many societies."
    },
    "Antipathy": {
        "url": "https://en.wikipedia.org/wiki/Antipathy",
        "summary": "Antipathy is a dislike for something or somebody, the opposite of sympathy. While antipathy may be induced by experience, it sometimes exists without a rational cause-and-effect explanation being present to the individuals involved.Thus, the origin of antipathy has been subject to various philosophical and psychological explanations, which some people find convincing and others regard as highly speculative. The exploration of a philosophical aspect for antipathy has been found in an essay by John Locke, an early modern  17th century philosopher."
    },
    "Eigenvalues and eigenvectors": {
        "url": "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors",
        "summary": "In linear algebra, it is often important to know which vectors have their directions unchanged by a linear transformation.  An eigenvector () or characteristic vector is such a vector.  Thus an eigenvector \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   of a linear transformation \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is scaled by a constant factor \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   when the linear transformation is applied to it: \n  \n    \n      \n        T\n        \n          v\n        \n        =\n        \u03bb\n        \n          v\n        \n      \n    \n    {\\displaystyle T\\mathbf {v} =\\lambda \\mathbf {v} }\n  .  The corresponding eigenvalue, characteristic value, or characteristic root is the multiplying factor \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  .\nGeometrically, vectors are multi-dimensional quantities with magnitude and direction, often pictured as arrows. A linear transformation rotates, stretches, or shears the vectors it acts upon. Its eigenvectors are those vectors that are only stretched, with no rotation or shear. The corresponding eigenvalue is the factor by which an eigenvector is stretched or squished. If the eigenvalue is negative, the eigenvector's direction is reversed.The eigenvectors and eigenvalues of a transformation serve to characterize it, and so they play important roles in all the areas where linear algebra is applied, from geology to quantum mechanics. In particular, it is often the case that a system is represented by a linear transformation whose outputs are fed as inputs to the same inputs (feedback).  In such an application, the largest eigenvalue is of particular importance, because it governs the long-term behavior of the system, after many applications of the linear transformation, and the associated eigenvector is the steady state of the system.\n\n"
    },
    "Definite quadratic form": {
        "url": "https://en.wikipedia.org/wiki/Definite_quadratic_form",
        "summary": "In mathematics, a definite quadratic form is a quadratic form over some real vector space V that has the same sign (always positive or always negative) for every non-zero vector of V. According to that sign, the quadratic form is called positive-definite or negative-definite.\nA semidefinite (or semi-definite) quadratic form is defined in much the same way, except that \"always positive\" and \"always negative\" are replaced by \"never negative\" and \"never positive\", respectively. In other words, it may take on zero values for some non-zero vectors of V.\nAn indefinite quadratic form takes on both positive and negative values and is called an isotropic quadratic form.\nMore generally, these definitions apply to any vector space over an ordered field."
    },
    "Positive definiteness": {
        "url": "https://en.wikipedia.org/wiki/Positive_definiteness",
        "summary": "In mathematics, positive definiteness is a property of any object to which a bilinear form or a sesquilinear form may be naturally associated, which is positive-definite. See, in particular:\n\nPositive-definite bilinear form\nPositive-definite function\nPositive-definite function on a group\nPositive-definite functional\nPositive-definite kernel\nPositive-definite matrix\nPositive-definite quadratic form"
    },
    "Negative definiteness": {
        "url": "https://en.wikipedia.org/wiki/Negative_definiteness",
        "summary": "In mathematics, negative definiteness is a property of any object to which a bilinear form may be naturally associated, which is negative-definite. See, in particular:\n\nNegative-definite bilinear form\nNegative-definite quadratic form\nNegative-definite matrix\nNegative-definite function"
    },
    "Eigendecomposition of a matrix": {
        "url": "https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix",
        "summary": "In linear algebra, eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way. When the matrix being factorized is a normal or real symmetric matrix, the decomposition is called \"spectral decomposition\", derived from the spectral theorem."
    },
    "Singular value decomposition": {
        "url": "https://en.wikipedia.org/wiki/Singular_value_decomposition",
        "summary": "In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any \n  \n    \n      \n         \n        m\n        \u00d7\n        n\n         \n      \n    \n    {\\displaystyle \\ m\\times n\\ }\n   matrix. It is related to the polar decomposition.\nSpecifically, the singular value decomposition of an \n  \n    \n      \n         \n        m\n        \u00d7\n        n\n         \n      \n    \n    {\\displaystyle \\ m\\times n\\ }\n   complex matrix M is a factorization of the form \n  \n    \n      \n         \n        \n          M\n        \n        =\n        \n          U\n          \u03a3\n          \n            V\n            \n              \u2217\n            \n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle \\ \\mathbf {M} =\\mathbf {U\\Sigma V^{*}} \\ ,}\n   where U is an \n  \n    \n      \n         \n        m\n        \u00d7\n        m\n         \n      \n    \n    {\\displaystyle \\ m\\times m\\ }\n   complex unitary matrix, \n  \n    \n      \n         \n        \n          \u03a3\n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {\\Sigma } \\ }\n   is an \n  \n    \n      \n         \n        m\n        \u00d7\n        n\n         \n      \n    \n    {\\displaystyle \\ m\\times n\\ }\n   rectangular diagonal matrix with non-negative real numbers on the diagonal, V is an \n  \n    \n      \n        n\n        \u00d7\n        n\n      \n    \n    {\\displaystyle n\\times n}\n   complex unitary matrix, and \n  \n    \n      \n         \n        \n          \n            V\n            \n              \u2217\n            \n          \n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {V^{*}} \\ }\n   is the conjugate transpose of V. Such decomposition always exists for any complex matrix.  If M is real, then U and V can be guaranteed to be real orthogonal matrices; in such contexts, the SVD is often denoted \n  \n    \n      \n         \n        \n          \n            U\n            \u03a3\n            V\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\ \\mathbf {U\\Sigma V} ^{\\operatorname {T} }}\n  .\nThe diagonal entries \n  \n    \n      \n         \n        \n          \u03c3\n          \n            i\n          \n        \n        =\n        \n          \u03a3\n          \n            i\n            i\n          \n        \n         \n      \n    \n    {\\displaystyle \\ \\sigma _{i}=\\Sigma _{ii}\\ }\n   of \n  \n    \n      \n         \n        \n          \u03a3\n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {\\Sigma } \\ }\n   are uniquely determined by M and are known as the singular values of M. The number of non-zero singular values is equal to the rank of M. The columns of U and the columns of V are called left-singular vectors and right-singular vectors of M, respectively. They form two sets of orthonormal bases u1, ..., um  and v1, ..., vn , and if they are sorted so that the singular values \n  \n    \n      \n         \n        \n          \u03c3\n          \n            i\n          \n        \n         \n      \n    \n    {\\displaystyle \\ \\sigma _{i}\\ }\n   with value zero are all in the highest-numbered columns (or rows), the singular value decomposition can be written as \n  \n    \n      \n         \n        \n          M\n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            r\n          \n        \n        \n          \u03c3\n          \n            i\n          \n        \n        \n          \n            u\n          \n          \n            i\n          \n        \n        \n          \n            v\n          \n          \n            i\n          \n          \n            \u2217\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle \\ \\mathbf {M} =\\sum _{i=1}^{r}\\sigma _{i}\\mathbf {u} _{i}\\mathbf {v} _{i}^{*}\\ ,}\n   where \n  \n    \n      \n         \n        r\n        \u2264\n        min\n        {\n        m\n        ,\n        n\n        }\n         \n      \n    \n    {\\displaystyle \\ r\\leq \\min\\{m,n\\}\\ }\n   is the rank of M.\nThe SVD is not unique. It is always possible to choose the decomposition so that the singular values \n  \n    \n      \n        \n          \u03a3\n          \n            i\n            i\n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{ii}}\n   are in descending order. In this case, \n  \n    \n      \n        \n          \u03a3\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma } }\n   (but not U and V) is uniquely determined by M.\nThe term sometimes refers to the compact SVD, a similar decomposition \n  \n    \n      \n         \n        \n          M\n        \n        =\n        \n          U\n          \u03a3\n          \n            V\n            \n              \u2217\n            \n          \n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {M} =\\mathbf {U\\Sigma V^{*}} \\ }\n   in which \n  \n    \n      \n         \n        \n          \u03a3\n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {\\Sigma } \\ }\n   is square diagonal of size \n  \n    \n      \n        r\n        \u00d7\n        r\n      \n    \n    {\\displaystyle r\\times r}\n  , where \n  \n    \n      \n         \n        r\n        \u2264\n        min\n        {\n        m\n        ,\n        n\n        }\n         \n      \n    \n    {\\displaystyle \\ r\\leq \\min\\{m,n\\}\\ }\n   is the rank of M, and has only the non-zero singular values. In this variant, U is an \n  \n    \n      \n        m\n        \u00d7\n        r\n      \n    \n    {\\displaystyle m\\times r}\n   semi-unitary matrix and \n  \n    \n      \n         \n        \n          V\n        \n         \n      \n    \n    {\\displaystyle \\ \\mathbf {V} \\ }\n   is an \n  \n    \n      \n        n\n        \u00d7\n        r\n      \n    \n    {\\displaystyle n\\times r}\n    semi-unitary matrix, such that \n  \n    \n      \n         \n        \n          \n            U\n            \n              \u2217\n            \n          \n          U\n        \n        =\n        \n          \n            V\n            \n              \u2217\n            \n          \n          V\n        \n        =\n        \n          \n            I\n          \n          \n            r\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\ \\mathbf {U^{*}U} =\\mathbf {V^{*}V} =\\mathbf {I} _{r}\\ .}\n  \nMathematical applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix.  The SVD is also extremely useful in all areas of science, engineering, and statistics, such as signal processing, least squares fitting of data, and process control."
    },
    "Principal component analysis": {
        "url": "https://en.wikipedia.org/wiki/Principal_component_analysis",
        "summary": "Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.The principal components of a collection of points in a real coordinate space are a sequence of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   unit vectors, where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th vector is the direction of a line that best fits the data while being orthogonal to the first \n  \n    \n      \n        i\n        \u2212\n        1\n      \n    \n    {\\displaystyle i-1}\n   vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.\nIn data analysis, the first principal component of a set of  \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   variables, presumed to be jointly normally distributed, is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through  \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set.\nPCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th principal component can be taken as a direction orthogonal to the first \n  \n    \n      \n        i\n        \u2212\n        1\n      \n    \n    {\\displaystyle i-1}\n   principal components that maximizes the variance of the projected data.\nFor either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain-specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Norm (mathematics)": {
        "url": "https://en.wikipedia.org/wiki/Norm_(mathematics)",
        "summary": "In mathematics, a norm is a function from a real or complex vector space to the non-negative real numbers that behaves in certain ways like the distance from the origin: it commutes with scaling, obeys a form of the triangle inequality, and is zero only at the origin. In particular, the Euclidean distance in a Euclidean space is defined by a norm on the associated Euclidean vector space, called the Euclidean norm, the 2-norm, or, sometimes, the magnitude of the vector. This norm can be defined as the square root of the inner product of a vector with itself.\nA seminorm satisfies the first two properties of a norm, but may be zero for vectors other than the origin. A vector space with a specified norm is called a normed vector space. In a similar manner, a vector space with a seminorm is called a seminormed vector space.\nThe term pseudonorm has been used for several related meanings. It may be a synonym of \"seminorm\".\nA pseudonorm may satisfy the same axioms as a norm, with the equality replaced by an inequality \"\n  \n    \n      \n        \n        \u2264\n        \n      \n    \n    {\\displaystyle \\,\\leq \\,}\n  \" in the homogeneity axiom.\nIt can also refer to a norm that can take infinite values, or to certain functions parametrised by a directed set."
    },
    "Distributive property": {
        "url": "https://en.wikipedia.org/wiki/Distributive_property",
        "summary": "In mathematics, the distributive property of binary operations is a generalization of the distributive law, which asserts that the equality\n\nis always true in elementary algebra.\nFor example, in elementary arithmetic, one has\n\nTherefore, one would say that multiplication distributes over addition.\nThis basic property of numbers is part of the definition of most algebraic structures that have two operations called addition and multiplication, such as complex numbers, polynomials, matrices, rings, and fields. It is also encountered in Boolean algebra and mathematical logic, where each of the logical and (denoted \n  \n    \n      \n        \n        \u2227\n        \n      \n    \n    {\\displaystyle \\,\\land \\,}\n  ) and the logical or (denoted \n  \n    \n      \n        \n        \u2228\n        \n      \n    \n    {\\displaystyle \\,\\lor \\,}\n  ) distributes over the other."
    },
    "X": {
        "url": "https://en.wikipedia.org/wiki/X",
        "summary": "X, or x, is the 24th letter in the Latin alphabet, used in the modern English alphabet, the alphabets of other western European languages and others worldwide. Its  name in English is \"ex\" (pronounced ), plural exes."
    },
    "Linear algebra": {
        "url": "https://en.wikipedia.org/wiki/Linear_algebra",
        "summary": "Linear algebra is the branch of mathematics concerning linear equations such as: \n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        ,\n      \n    \n    {\\displaystyle a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,}\n  linear maps such as:\n\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        \u21a6\n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle (x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},}\n  and their representations in vector spaces and through matrices.Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.\nLinear algebra is also used in most sciences and fields of engineering, because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point.\n\n"
    },
    "Mathematics": {
        "url": "https://en.wikipedia.org/wiki/Mathematics",
        "summary": "Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory, algebra, geometry, and analysis, respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.\nMost mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or\u2014in modern mathematics\u2014entities that are stipulated to have certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and\u2014in case of abstraction from nature\u2014some basic properties that are considered true starting points of the theory under consideration.Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.\nHistorically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was essentially divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new areas. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid lockstep increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics.\n\n"
    }
}
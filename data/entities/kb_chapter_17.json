{
    "Las Vegas algorithm": {
        "url": "https://en.wikipedia.org/wiki/Las_Vegas_algorithm",
        "summary": "In computing, a Las Vegas algorithm is a randomized algorithm that always gives correct results; that is, it always produces the correct result or it informs about the failure. However, the runtime of a Las Vegas algorithm differs depending on the input. The usual definition of a Las Vegas algorithm includes the restriction that the expected runtime be finite, where the expectation is carried out over the space of random information, or entropy, used in the algorithm. An alternative definition requires that a Las Vegas algorithm always terminates (is effective), but may output a symbol not part of the solution space to indicate failure in finding a solution. The nature of Las Vegas algorithms makes them suitable in situations where the number of possible solutions is limited, and where verifying the correctness of a candidate solution is relatively easy while finding a solution is complex.\nLas Vegas algorithms are prominent in the field of artificial intelligence, and in other areas of computer science and operations research. In AI, stochastic local search (SLS) algorithms are considered to be of Las Vegas type. SLS algorithms have been used to address NP-complete decision problems and NP-hard combinatorial optimization problems. However, some systematic search methods, such as modern variants of the Davis\u2013Putnam algorithm for propositional satisfiability (SAT), also utilize non-deterministic decisions, and can thus also be considered Las Vegas algorithms."
    },
    "Randomized algorithm": {
        "url": "https://en.wikipedia.org/wiki/Randomized_algorithm",
        "summary": "A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.\nOne has to distinguish between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite (Las Vegas algorithms, for example Quicksort), and algorithms which have a chance of producing an incorrect result (Monte Carlo algorithms, for example the Monte Carlo algorithm for the MFAS problem) or fail to produce a result either by signaling a failure or failing to terminate. In some cases, probabilistic algorithms are the only practical means of solving a problem.In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator.\n\n"
    },
    "Monte Carlo algorithm": {
        "url": "https://en.wikipedia.org/wiki/Monte_Carlo_algorithm",
        "summary": "In computing, a Monte Carlo algorithm is a randomized algorithm whose output may be incorrect with a certain (typically small) probability. Two examples of such algorithms are the Karger\u2013Stein algorithm and the Monte Carlo algorithm for minimum feedback arc set.The name refers to the Monte Carlo casino in the Principality of Monaco, which is well-known around the world as an icon of gambling. The term \"Monte Carlo\" was first introduced in 1947 by Nicholas Metropolis.Las Vegas algorithms are a dual of Monte Carlo algorithms and never return an incorrect answer. However, they may make random choices as part of their work. As a result, the time taken might vary between runs, even with the same input.\nIf there is a procedure for verifying whether the answer given by a Monte Carlo algorithm is correct, and the probability of a correct answer is bounded above zero, then with probability one, running the algorithm repeatedly while testing the answers will eventually give a correct answer. Whether this process is a Las Vegas algorithm depends on whether halting with probability one is considered to satisfy the definition."
    },
    "Minimally invasive procedure": {
        "url": "https://en.wikipedia.org/wiki/Minimally_invasive_procedure",
        "summary": "Minimally invasive procedures (also known as minimally invasive surgeries) encompass surgical techniques that limit the size of incisions needed, thereby reducing wound healing time, associated pain, and risk of infection. Surgery by definition is invasive and many operations requiring incisions of some size are referred to as open surgery. Incisions made during open surgery can sometimes leave large wounds that may be painful and take a long time to heal. Advancements in medical technologies have enabled the development and regular use of minimally invasive procedures. For example, endovascular aneurysm repair, a minimally invasive surgery, has become the most common method of repairing abdominal aortic aneurysms in the US as of 2003. The procedure involves much smaller incisions than the corresponding open surgery procedure of open aortic surgery.Interventional radiologists were the forerunners of minimally invasive procedures. Using imaging techniques, radiologists were able to direct interventional instruments through the body by way of catheters instead of the large incisions needed in traditional surgery. As a result, many conditions once requiring surgery can now be treated non-surgically.Diagnostic techniques that do not involve incisions, puncturing the skin, or the introduction of foreign objects or materials into the body are known as non-invasive procedures. Several treatment procedures are classified as non-invasive. A major example of a non-invasive alternative treatment to surgery is radiation therapy, also called radiotherapy."
    },
    "Markov chain Monte Carlo": {
        "url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo",
        "summary": "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains, including the Metropolis\u2013Hastings algorithm."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Allele": {
        "url": "https://en.wikipedia.org/wiki/Allele",
        "summary": "An allele, or allelomorph, is a variant of the sequence of nucleotides at a particular location, or locus, on a DNA molecule.Alleles can differ at a single position through single nucleotide polymorphisms (SNP), but they can also have insertions and deletions of up to several thousand base pairs.Most alleles observed result in little or no change in the function of the gene product it codes for. However, sometimes different alleles can result in different observable phenotypic traits, such as different pigmentation. A notable example of this is Gregor Mendel's discovery that the white and purple flower colors in pea plants were the result of a single gene with two alleles.\nNearly all multicellular organisms have two sets of chromosomes at some point in their biological life cycle; that is, they are diploid. In this case, the chromosomes can be paired. Each chromosome in the pair contains the same genes in the same order, and place, along the length of the chromosome. For a given gene, if the two chromosomes contain the same allele, they, and the organism, are homozygous with respect to that gene. If the alleles are different, they, and the organism, are heterozygous with respect to that gene.\nPopular definitions of 'allele' typically refer only to different alleles within genes. For example, the ABO blood grouping is controlled by the ABO gene, which has six common alleles (variants). In population genetics, nearly every living human's phenotype for the ABO gene is some combination of just these six alleles.\n\n"
    },
    "Harris chain": {
        "url": "https://en.wikipedia.org/wiki/Harris_chain",
        "summary": "In the mathematical study of stochastic processes, a Harris chain is a Markov chain where the chain returns to a particular part of the state space an unbounded number of times. Harris chains are regenerative processes and are named after Theodore Harris. The theory of Harris chains and Harris recurrence is useful for treating Markov chains on general (possibly uncountably infinite) state spaces."
    },
    "Markov chain": {
        "url": "https://en.wikipedia.org/wiki/Markov_chain",
        "summary": "A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \"What happens next depends only on the state of affairs now.\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). It is named after the Russian mathematician Andrey Markov.\nMarkov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics.Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing.The adjectives Markovian and Markov are used to describe something that is related to a Markov process."
    },
    "Metropolis\u2013Hastings algorithm": {
        "url": "https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm",
        "summary": "In statistics and statistical physics, the Metropolis\u2013Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. This sequence can be used to approximate the distribution (e.g. to generate a histogram) or to compute an integral (e.g. an expected value). Metropolis\u2013Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high.  For single-dimensional distributions, there are usually other methods (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and these are free from the problem of autocorrelated samples that is inherent in MCMC methods."
    },
    "Deep learning": {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "summary": "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog."
    },
    "Alpha": {
        "url": "https://en.wikipedia.org/wiki/Alpha",
        "summary": "Alpha  (uppercase \u0391, lowercase \u03b1; Ancient Greek: \u1f04\u03bb\u03c6\u03b1, \u00e1lpha, or Greek: \u03ac\u03bb\u03c6\u03b1, romanized: \u00e1lfa) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for \"ox\". Letters that arose from alpha include the Latin letter A and the Cyrillic letter \u0410."
    },
    "Beta": {
        "url": "https://en.wikipedia.org/wiki/Beta",
        "summary": "Beta (UK: , US: ; uppercase \u0392, lowercase \u03b2, or cursive \u03d0; Ancient Greek: \u03b2\u1fc6\u03c4\u03b1, romanized: b\u0113\u0302ta or Greek: \u03b2\u03ae\u03c4\u03b1, romanized: v\u00edta) is the second letter of the Greek alphabet. In the system of Greek numerals, it has a value of 2. In Ancient Greek, beta represented the voiced bilabial plosive IPA: [b]. In Modern Greek, it represents the voiced labiodental fricative IPA: [v] while IPA: [b] in borrowed words is instead commonly transcribed as \u03bc\u03c0. Letters that arose from beta include the Roman letter \u27e8B\u27e9 and the Cyrillic letters \u27e8\u0411\u27e9 and \u27e8\u0412\u27e9."
    }
}
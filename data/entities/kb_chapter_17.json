{
    "Las Vegas algorithm": {
        "url": "https://en.wikipedia.org/wiki/Las_Vegas_algorithm",
        "summary": "In computing, a Las Vegas algorithm is a randomized algorithm that always gives correct results; that is, it always produces the correct result or it informs about the failure. However, the runtime of a Las Vegas algorithm differs depending on the input. The usual definition of a Las Vegas algorithm includes the restriction that the expected runtime be finite, where the expectation is carried out over the space of random information, or entropy, used in the algorithm. An alternative definition requires that a Las Vegas algorithm always terminates (is effective), but may output a symbol not part of the solution space to indicate failure in finding a solution. The nature of Las Vegas algorithms makes them suitable in situations where the number of possible solutions is limited, and where verifying the correctness of a candidate solution is relatively easy while finding a solution is complex.\nLas Vegas algorithms are prominent in the field of artificial intelligence, and in other areas of computer science and operations research. In AI, stochastic local search (SLS) algorithms are considered to be of Las Vegas type. SLS algorithms have been used to address NP-complete decision problems and NP-hard combinatorial optimization problems. However, some systematic search methods, such as modern variants of the Davis\u2013Putnam algorithm for propositional satisfiability (SAT), also utilize non-deterministic decisions, and can thus also be considered Las Vegas algorithms."
    },
    "Randomized algorithm": {
        "url": "https://en.wikipedia.org/wiki/Randomized_algorithm",
        "summary": "A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.\nOne has to distinguish between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite (Las Vegas algorithms, for example Quicksort), and algorithms which have a chance of producing an incorrect result (Monte Carlo algorithms, for example the Monte Carlo algorithm for the MFAS problem) or fail to produce a result either by signaling a failure or failing to terminate. In some cases, probabilistic algorithms are the only practical means of solving a problem.In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator."
    },
    "Allele": {
        "url": "https://en.wikipedia.org/wiki/Allele",
        "summary": "An allele, or allelomorph, is a variant of the sequence of nucleotides at a particular location, or locus, on a DNA molecule.Alleles can differ at a single position through single nucleotide polymorphisms (SNP), but they can also have insertions and deletions of up to several thousand base pairs.Most alleles observed result in little or no change in the function of the gene product it codes for. However, sometimes different alleles can result in different observable phenotypic traits, such as different pigmentation. A notable example of this is Gregor Mendel's discovery that the white and purple flower colors in pea plants were the result of a single gene with two alleles.\nNearly all multicellular organisms have two sets of chromosomes at some point in their biological life cycle; that is, they are diploid. In this case, the chromosomes can be paired. Each chromosome in the pair contains the same genes in the same order, and place, along the length of the chromosome. For a given gene, if the two chromosomes contain the same allele, they, and the organism, are homozygous with respect to that gene. If the alleles are different, they, and the organism, are heterozygous with respect to that gene.\nPopular definitions of 'allele' typically refer only to different alleles within genes. For example, the ABO blood grouping is controlled by the ABO gene, which has six common alleles (variants). In population genetics, nearly every living human's phenotype for the ABO gene is some combination of just these six alleles."
    },
    "Metropolis\u2013Hastings algorithm": {
        "url": "https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm",
        "summary": "In statistics and statistical physics, the Metropolis\u2013Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. This sequence can be used to approximate the distribution (e.g. to generate a histogram) or to compute an integral (e.g. an expected value). Metropolis\u2013Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high.  For single-dimensional distributions, there are usually other methods (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and these are free from the problem of autocorrelated samples that is inherent in MCMC methods."
    },
    "Deep learning": {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "summary": "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog."
    },
    "Alpha": {
        "url": "https://en.wikipedia.org/wiki/Alpha",
        "summary": "Alpha  (uppercase \u0391, lowercase \u03b1; Ancient Greek: \u1f04\u03bb\u03c6\u03b1, \u00e1lpha, or Greek: \u03ac\u03bb\u03c6\u03b1, romanized: \u00e1lfa) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for \"ox\". Letters that arose from alpha include the Latin letter A and the Cyrillic letter \u0410."
    },
    "Beta": {
        "url": "https://en.wikipedia.org/wiki/Beta",
        "summary": "Beta (UK: , US: ; uppercase \u0392, lowercase \u03b2, or cursive \u03d0; Ancient Greek: \u03b2\u1fc6\u03c4\u03b1, romanized: b\u0113\u0302ta or Greek: \u03b2\u03ae\u03c4\u03b1, romanized: v\u00edta) is the second letter of the Greek alphabet. In the system of Greek numerals, it has a value of 2. In Ancient Greek, beta represented the voiced bilabial plosive IPA: [b]. In Modern Greek, it represents the voiced labiodental fricative IPA: [v] while IPA: [b] in borrowed words is instead commonly transcribed as \u03bc\u03c0. Letters that arose from beta include the Roman letter \u27e8B\u27e9 and the Cyrillic letters \u27e8\u0411\u27e9 and \u27e8\u0412\u27e9."
    },
    "Temperature": {
        "url": "https://en.wikipedia.org/wiki/Temperature",
        "summary": "Temperature is a physical quantity that expresses quantitatively the attribute of hotness or coldness. Temperature is measured with a thermometer. It reflects the kinetic energy of the vibrating and colliding atoms making up a substance.\nThermometers are calibrated in various temperature scales that historically have relied on various reference points and thermometric substances for definition. The most common scales are the Celsius scale with the unit symbol \u00b0C (formerly called centigrade), the Fahrenheit scale (\u00b0F), and the Kelvin scale (K), the latter being used predominantly for scientific purposes. The kelvin is one of the seven base units in the International System of Units (SI).\nAbsolute zero, i.e., zero kelvin or \u2212273.15 \u00b0C, is the lowest point in the thermodynamic temperature scale. Experimentally, it can be approached very closely but not actually reached, as recognized in the third law of thermodynamics. It would be impossible to extract energy as heat from a body at that temperature.\nTemperature is important in all fields of natural science, including physics, chemistry, Earth science, astronomy, medicine, biology, ecology, material science, metallurgy, mechanical engineering and geography as well as most aspects of daily life."
    },
    "Autoencoder": {
        "url": "https://en.wikipedia.org/wiki/Autoencoder",
        "summary": "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\nVariants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data)."
    },
    "Feature learning": {
        "url": "https://en.wikipedia.org/wiki/Feature_learning",
        "summary": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform  a specific task.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\nFeature learning can be either supervised, unsupervised or self-supervised.\n\nIn supervised feature learning, features are learned using labeled input data. Labeled data includes input-label pairs where the input is given to the model and it must produce the ground truth label as the correct answer. This can be leveraged to generate feature representations with the model which result in high label prediction accuracy. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.\nIn unsupervised feature learning, features are learned with unlabeled input data by analyzing the relationship between points in the dataset.  Examples include dictionary learning, independent component analysis, matrix factorization and various forms of clustering.\nIn self-supervised feature learning, features are learned using unlabeled data like unsupervised learning, however input-label pairs are constructed from each data point, which enables learning the structure of the data through supervised methods such as gradient descent. Classical examples include word embeddings and autoencoders. SSL has since been applied to many modalities through the use of deep neural network architectures such as CNNs and transformers."
    },
    "Representations": {
        "url": "https://en.wikipedia.org/wiki/Representations",
        "summary": "Representations is an interdisciplinary journal in the humanities published quarterly by the University of California Press. The journal was established in 1983 and is the founding publication of the New Historicism movement of the 1980s. It covers topics including literary, historical, and cultural studies. The founding editorial board was chaired by Stephen Greenblatt and Svetlana Alpers. Representations frequently publishes thematic special issues, for example, the 2007 issue on the legacies of American Orientalism, the 2006 issue on cross-cultural mimesis, and the 2005 issue on political and intellectual redress."
    }
}
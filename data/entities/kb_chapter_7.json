{
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Machine learning": {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods."
    },
    "Overfitting": {
        "url": "https://en.wikipedia.org/wiki/Overfitting",
        "summary": "In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.:\u200a45\u200aUnderfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.\nThe possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend. \nAs an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. \nThe potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.\nTo lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter."
    },
    "Bias": {
        "url": "https://en.wikipedia.org/wiki/Bias",
        "summary": "Bias is a disproportionate weight in favor of or against an idea or thing, usually in a way that is closed-minded, prejudicial, or unfair. Biases can be innate or learned. People may develop biases for or against an individual, a group, or a belief. In science and engineering, a bias is a systematic error. Statistical bias results from an unfair sampling of a population, or from an estimation process that does not give accurate results on average."
    },
    "Alpha": {
        "url": "https://en.wikipedia.org/wiki/Alpha",
        "summary": "Alpha  (uppercase \u0391, lowercase \u03b1; Ancient Greek: \u1f04\u03bb\u03c6\u03b1, \u00e1lpha, or Greek: \u03ac\u03bb\u03c6\u03b1, romanized: \u00e1lfa) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for \"ox\". Letters that arose from alpha include the Latin letter A and the Cyrillic letter \u0410."
    },
    "Lambda": {
        "url": "https://en.wikipedia.org/wiki/Lambda",
        "summary": "Lambda (; uppercase \u039b, lowercase \u03bb; Greek: \u03bb\u03ac\u03bc(\u03b2)\u03b4\u03b1, l\u00e1m(b)da) is the eleventh letter of the Greek alphabet, representing the voiced alveolar lateral approximant IPA: [l]. In the system of Greek numerals, lambda has a value of 30. Lambda is derived from the Phoenician Lamed . Lambda gave rise to the Latin L and the Cyrillic El (\u041b). The ancient grammarians and dramatists give evidence to the pronunciation as [la\u02d0bda\u02d0] (\u03bb\u03ac\u03b2\u03b4\u03b1) in Classical Greek times. In Modern Greek, the name of the letter, \u039b\u03ac\u03bc\u03b4\u03b1, is pronounced [\u02c8lam.\u00f0a].\nIn early Greek alphabets, the shape and orientation of lambda varied. Most variants consisted of two straight strokes, one longer than the other, connected at their ends. The angle might be in the upper-left, lower-left (\"Western\" alphabets) or top (\"Eastern\" alphabets). Other variants had a vertical line with a horizontal or sloped stroke running to the right. With the general adoption of the Ionic alphabet, Greek settled on an angle at the top; the Romans put the angle at the lower-left.\nThe HTML 4 character entity references for the Greek capital and small letter lambda are &#923; and &#955; respectively. The Unicode code points for lambda are U+039B and U+03BB."
    },
    "Sign": {
        "url": "https://en.wikipedia.org/wiki/Sign",
        "summary": "A sign is an object, quality, event, or entity whose presence or occurrence indicates the probable presence or occurrence of something else. A natural sign bears a causal relation to its object\u2014for instance, thunder is a sign of storm, or medical symptoms a sign of disease. A conventional sign signifies by agreement, as a full stop signifies the end of a sentence; similarly the words and expressions of a language, as well as bodily gestures, can be regarded as signs, expressing particular meanings. The physical objects most commonly referred to as signs (notices, road signs, etc., collectively known as signage) generally inform or instruct using written text, symbols, pictures or a combination of these.\nThe philosophical study of signs and symbols is called semiotics; this includes the study of semiosis, which is the way in which signs (in the semiotic sense) operate."
    },
    "Lasso (statistics)": {
        "url": "https://en.wikipedia.org/wiki/Lasso_(statistics)",
        "summary": "In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model. It was originally introduced in geophysics, and later by Robert Tibshirani, who coined the term.\nLasso was originally formulated for linear regression models. This simple case reveals a substantial amount about the estimator. These include its relationship to ridge regression and best subset selection and the connections between lasso coefficient estimates and so-called soft thresholding. It also reveals that (like standard linear regression) the coefficient estimates do not need to be unique if covariates are collinear.\nThough originally defined for linear regression, lasso regularization is easily extended to other statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators. Lasso's ability to perform subset selection relies on the form of the constraint and has a variety of interpretations including in terms of geometry, Bayesian statistics and convex analysis.\nThe LASSO is closely related to basis pursuit denoising."
    },
    "Linear model": {
        "url": "https://en.wikipedia.org/wiki/Linear_model",
        "summary": "In statistics, the term linear model is used in different ways according to the context. The most common occurrence is in connection with regression models and the term is often taken as synonymous with linear regression model.  However, the term is also used in time series analysis with a different meaning. In each case, the designation \"linear\" is used to identify a subclass of models for which substantial reduction in the complexity of the related statistical theory is possible."
    },
    "Linear regression": {
        "url": "https://en.wikipedia.org/wiki/Linear_regression",
        "summary": "In statistics, linear regression is a linear approach for modelling a predictive relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables), which are measured without error. The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.If the explanatory variables are measured with error then errors-in-variables models are required, also known as measurement error models.\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n\nIf the goal is error reduction in prediction or forecasting, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\nIf the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Use of the Mean Squared Error(MSE) as the cost on a dataset that has many large outliers, can result in a model that fits the outliers more than the true data due to the higher importance assigned by MSE to large errors. So a cost functions that are robust to outliers should be used if the dataset has many large outliers.  Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous."
    },
    "Linear algebra": {
        "url": "https://en.wikipedia.org/wiki/Linear_algebra",
        "summary": "Linear algebra is the branch of mathematics concerning linear equations such as: \n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        ,\n      \n    \n    {\\displaystyle a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,}\n  linear maps such as:\n\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        \u21a6\n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle (x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},}\n  and their representations in vector spaces and through matrices.Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.\nLinear algebra is also used in most sciences and fields of engineering, because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point.\n\n"
    },
    "Convolution": {
        "url": "https://en.wikipedia.org/wiki/Convolution",
        "summary": "In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function (\n  \n    \n      \n        f\n        \u2217\n        g\n      \n    \n    {\\displaystyle f*g}\n  ) that expresses how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. The choice of which function is reflected and shifted before the integral does not change the integral result (see commutativity). The integral is evaluated for all values of shift, producing the convolution function.\nSome features of convolution are similar to cross-correlation: for real-valued functions, of a continuous or discrete variable, convolution (\n  \n    \n      \n        f\n        \u2217\n        g\n      \n    \n    {\\displaystyle f*g}\n  ) differs from cross-correlation (\n  \n    \n      \n        f\n        \u22c6\n        g\n      \n    \n    {\\displaystyle f\\star g}\n  ) only in that either f(x) or g(x) is reflected about the y-axis in convolution; thus it is a cross-correlation of g(\u2212x) and f(x), or f(\u2212x) and g(x). For complex-valued functions, the cross-correlation operator is the adjoint of the convolution operator.\nConvolution has applications that include probability, statistics, acoustics, spectroscopy, signal processing and image processing, geophysics, engineering, physics, computer vision and differential equations.The convolution can be defined for functions on Euclidean space and other groups (as algebraic structures). For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by periodic convolution. (See row 18 at DTFT \u00a7 Properties.) A discrete convolution can be defined for functions on the set of integers.\nGeneralizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.Computing the inverse of the convolution operation is known as deconvolution."
    },
    "Unsupervised": {
        "url": "https://en.wikipedia.org/wiki/Unsupervised",
        "summary": "Unsupervised is an American adult animated sitcom created by David Hornsby, Rob Rosell, and Scott Marder which ran on FX from January 19 to December 20, 2012. The show was created, and for the most part, written by David Hornsby, Scott Marder, and Rob Rosell.On November 17, 2012, the series was canceled after one season."
    },
    "Supervised learning": {
        "url": "https://en.wikipedia.org/wiki/Supervised_learning",
        "summary": "Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n\n"
    },
    "Early stopping": {
        "url": "https://en.wikipedia.org/wiki/Early_stopping",
        "summary": "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation."
    },
    "Metaheuristic": {
        "url": "https://en.wikipedia.org/wiki/Metaheuristic",
        "summary": "In computer science and mathematical optimization, a metaheuristic is a higher-level procedure or heuristic designed to find, generate, tune, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem or a machine learning problem, especially with incomplete or imperfect information or limited computation capacity. Metaheuristics sample a subset of solutions which is otherwise too large to be completely enumerated or otherwise explored. Metaheuristics may make relatively few assumptions about the optimization problem being solved and so may be usable for a variety of problems.Compared to optimization algorithms and iterative methods, metaheuristics do not guarantee that a globally optimal solution can be found on some class of problems. Many metaheuristics implement some form of stochastic optimization, so that the solution found is dependent on the set of random variables generated. In combinatorial optimization, by searching over a large set of feasible solutions, metaheuristics can often find good solutions with less computational effort than optimization algorithms, iterative methods, or simple heuristics. As such, they are useful  approaches for optimization problems. Several books and survey papers have been published on the subject. Literature review on metaheuristic optimization, suggested that it was Fred Glover who coined the word metaheuristics.Most literature on metaheuristics is experimental in nature, describing empirical results based on computer experiments with the algorithms. But some formal theoretical results are also available, often on convergence and the possibility of finding the global optimum. Many metaheuristic methods have been published with claims of novelty and practical efficacy. While the field also features high-quality research, many of the publications have been of poor quality; flaws include vagueness, lack of conceptual elaboration, poor experiments, and ignorance of previous literature."
    },
    "Graphics processing unit": {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\n"
    },
    "Central processing unit": {
        "url": "https://en.wikipedia.org/wiki/Central_processing_unit",
        "summary": "A central processing unit (CPU)\u2014also called a central processor or main processor\u2014is the most important processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry, and specialized coprocessors such as graphics processing units (GPUs).\nThe form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic\u2013logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components.\nMost modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to support CPU-level multithreading. Most modern CPUs have privileged mode to support operating systems and hypervisor mode to support virtualization.\nAn IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC)."
    },
    "Memory": {
        "url": "https://en.wikipedia.org/wiki/Memory",
        "summary": "Memory is the faculty of the mind by which data or information is encoded, stored, and retrieved when needed. It is the retention of information over time for the purpose of influencing future action. If past events could not be remembered, it would be impossible for language, relationships, or personal identity to develop. Memory loss is usually described as forgetfulness or amnesia.Memory is often understood as an informational processing system with explicit and implicit functioning that is made up of a sensory processor, short-term (or working) memory, and long-term memory. This can be related to the neuron.\nThe sensory processor allows information from the outside world to be sensed in the form of chemical and physical stimuli and attended to various levels of focus and intent. Working memory serves as an encoding and retrieval processor. Information in the form of stimuli is encoded in accordance with explicit or implicit functions by the working memory processor. The working memory also retrieves information from previously stored material. Finally, the function of long-term memory is to store through various categorical models or systems.Declarative, or explicit, memory is the conscious storage and recollection of data. Under declarative memory resides semantic and episodic memory. Semantic memory refers to memory that is encoded with specific meaning. Meanwhile, episodic memory refers to information that is encoded along a spatial and temporal plane. Declarative memory is usually the primary process thought of when referencing memory. Non-declarative, or implicit, memory is the unconscious storage and recollection of information. An example of a non-declarative process would be the unconscious learning or retrieval of information by way of procedural memory, or a priming phenomenon. Priming is the process of subliminally arousing specific responses from memory and shows that not all memory is consciously activated, whereas procedural memory is the slow and gradual learning of skills that often occurs without conscious attention to learning.Memory is not a perfect processor, and is affected by many factors. The ways by which information is encoded, stored, and retrieved can all be corrupted. Pain, for example, has been identified as a physical condition that impairs memory, and has been noted in animal models as well as chronic pain patients. The amount of attention given new stimuli can diminish the amount of information that becomes encoded for storage. Also, the storage process can become corrupted by physical damage to areas of the brain that are associated with memory storage, such as the hippocampus. Finally, the retrieval of information from long-term memory can be disrupted because of decay within long-term memory. Normal functioning, decay over time, and brain damage all affect the accuracy and capacity of the memory."
    },
    "Minimally invasive procedure": {
        "url": "https://en.wikipedia.org/wiki/Minimally_invasive_procedure",
        "summary": "Minimally invasive procedures (also known as minimally invasive surgeries) encompass surgical techniques that limit the size of incisions needed, thereby reducing wound healing time, associated pain, and risk of infection. Surgery by definition is invasive and many operations requiring incisions of some size are referred to as open surgery. Incisions made during open surgery can sometimes leave large wounds that may be painful and take a long time to heal. Advancements in medical technologies have enabled the development and regular use of minimally invasive procedures. For example, endovascular aneurysm repair, a minimally invasive surgery, has become the most common method of repairing abdominal aortic aneurysms in the US as of 2003. The procedure involves much smaller incisions than the corresponding open surgery procedure of open aortic surgery.Interventional radiologists were the forerunners of minimally invasive procedures. Using imaging techniques, radiologists were able to direct interventional instruments through the body by way of catheters instead of the large incisions needed in traditional surgery. As a result, many conditions once requiring surgery can now be treated non-surgically.Diagnostic techniques that do not involve incisions, puncturing the skin, or the introduction of foreign objects or materials into the body are known as non-invasive procedures. Several treatment procedures are classified as non-invasive. A major example of a non-invasive alternative treatment to surgery is radiation therapy, also called radiotherapy."
    },
    "Backpropagation": {
        "url": "https://en.wikipedia.org/wiki/Backpropagation",
        "summary": "As a machine-learning algorithm, backpropagation is a crucial step in a common method used to iteratively train a neural network model. It is used to calculate the necessary parameter adjustments, to gradually minimize error.\nIn a multi-layered network, backpropagation is step 2.2 for training a neural network model:\n\nPropagate training data through the model from input to predicted output by computing the successive hidden layers' outputs and finally the final layer's output (the feedforward step).\nAdjust the model weights to reduce the error relative to the weights.\nThe error is typically the squared difference between prediction and target.\nFor each weight, the slope or derivative of the error is found, and the weight adjusted by a negative multiple of this derivative, so as to go downslope toward the minimum-error configuration.\nThis derivative is easy to calculate for final layer weights, and possible to calculate for one layer given the next layer's derivatives. Starting at the end, then, the derivatives are calculated layer by layer toward the beginning -- thus \"backpropagation\".\nRepeatedly update the weights until they converge or the model has undergone enough iterations.It is an efficient application of the Leibniz chain rule (1673) to such networks. It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).   The term \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory.Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input\u2013output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming. Gradient descent, or variants such as stochastic gradient descent, are commonly used.\nStrictly the term backpropagation refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm \u2013 including how the gradient is used, such as by stochastic gradient descent. In 1986 David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.\n\n"
    },
    "Arithmetic mean": {
        "url": "https://en.wikipedia.org/wiki/Arithmetic_mean",
        "summary": "In mathematics and statistics, the arithmetic mean (  arr-ith-MET-ik), arithmetic average, or just the mean or average (when the context is clear) is the sum of a collection of numbers divided by the count of numbers in the collection. The collection is often a set of results from an experiment, an observational study, or a survey. The term \"arithmetic mean\" is preferred in some mathematics and statistics contexts because it helps distinguish it from other types of means, such as geometric and harmonic.\nIn addition to mathematics and statistics, the arithmetic mean is frequently used in economics, anthropology, history, and almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population.\nWhile the arithmetic mean is often used to report central tendencies, it is not a robust statistic: it is greatly influenced by outliers (values much larger or smaller than most others). For skewed distributions, such as the distribution of income for which a few people's incomes are substantially higher than most people's, the arithmetic mean may not coincide with one's notion of \"middle\". In that case, robust statistics, such as the median, may provide a better description of central tendency."
    },
    "Probability distribution": {
        "url": "https://en.wikipedia.org/wiki/Probability_distribution",
        "summary": "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).For instance, if X is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). More commonly, probability distributions are used to compare the relative occurrence of many different random values.\nProbability distributions can be defined in different ways and for discrete or for continuous variables. Distributions with special properties or for especially important applications are given specific names."
    },
    "2": {
        "url": "https://en.wikipedia.org/wiki/2",
        "summary": "2 (two) is a number, numeral and digit. It is the natural number following 1 and preceding 3. It is the smallest and only even prime number. Because it forms the basis of a duality, it has religious and spiritual significance in many cultures."
    },
    "7": {
        "url": "https://en.wikipedia.org/wiki/7",
        "summary": "7 (seven) is the natural number following 6 and preceding 8. It is the only prime number preceding a cube.\nAs an early prime number in the series of positive integers, the number seven has greatly symbolic associations in religion, mythology, superstition and philosophy. The seven Classical planets resulted in seven being the number of days in a week. It is often considered lucky in Western culture and is often seen as highly symbolic. Unlike Western culture, in Vietnamese culture, the number seven is sometimes considered unlucky."
    },
    "Restricted Boltzmann machine": {
        "url": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine",
        "summary": "A restricted Boltzmann machine (RBM) (also called a restricted Sherrington\u2013Kirkpatrick model with external field or restricted stochastic Ising\u2013Lenz\u2013Little model) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.RBMs were initially proposed under the name Harmonium by Paul Smolensky in 1986, and rose to prominence after Geoffrey Hinton and collaborators used fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction, classification, collaborative filtering, feature learning, topic modelling and even many body quantum mechanics. They can be trained in either supervised or unsupervised ways, depending on the task.\nAs their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: \na pair of nodes from each of the two groups of units (commonly referred to as the \"visible\" and \"hidden\" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, \"unrestricted\" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by \"stacking\" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation."
    },
    "Neural network": {
        "url": "https://en.wikipedia.org/wiki/Neural_network",
        "summary": "A neural network is a neural circuit of biological neurons, sometimes also called a biological neural network, or a network of artificial neurons or nodes in the case of an artificial neural network.Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1.\nThese artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information."
    },
    "Sexual reproduction": {
        "url": "https://en.wikipedia.org/wiki/Sexual_reproduction",
        "summary": "Sexual reproduction is a type of reproduction that involves a complex life cycle in which a gamete (haploid reproductive cells, such as a sperm or egg cell) with a single set of chromosomes combines with another gamete to produce a zygote that develops into an organism composed of cells with two sets of chromosomes (diploid). This is typical in animals, though the number of chromosome sets and how that number changes in sexual reproduction varies, especially among plants, fungi, and other eukaryotes.Sexual reproduction is the most common life cycle in multicellular eukaryotes, such as animals, fungi and plants. Sexual reproduction also occurs in some unicellular eukaryotes. Sexual reproduction does not occur in prokaryotes, unicellular organisms without cell nuclei, such as bacteria and archaea. However, some processes in bacteria, including bacterial conjugation, transformation and transduction, may be considered analogous to sexual reproduction in that they incorporate new genetic information. Some proteins and other features that are key for sexual reproduction may have arisen in bacteria, but sexual reproduction is believed to have developed in an ancient eukaryotic ancestor.In eukaryotes, diploid precursor cells divide to produce haploid cells in a process called meiosis. In meiosis, DNA is replicated to produce a total of four copies of each chromosome. This is followed by two cell divisions to generate haploid gametes. After the DNA is replicated in meiosis, the homologous chromosomes pair up so that their DNA sequences are aligned with each other. During this period before cell divisions, genetic information is exchanged between homologous chromosomes in genetic recombination. Homologous chromosomes contain highly similar but not identical information, and by exchanging similar but not identical regions, genetic recombination increases genetic diversity among future generations.During sexual reproduction, two haploid gametes combine into one diploid cell known as a zygote in a process called fertilization. The nuclei from the gametes fuse, and each gamete contributes half of the genetic material of the zygote. Multiple cell divisions by mitosis (without change in the number of chromosomes) then develop into a multicellular diploid phase or generation. In plants, the diploid phase, known as the sporophyte, produces spores by meiosis. These spores then germinate and divide by mitosis to form a haploid multicellular phase, the gametophyte, which produces gametes directly by mitosis. This type of life cycle, involving alternation between two multicellular phases, the sexual haploid gametophyte and asexual diploid sporophyte, is known as alternation of generations.\nThe evolution of sexual reproduction is considered paradoxical, because asexual reproduction should be able to outperform it as every young organism created can bear its own young. This implies that an asexual population has an intrinsic capacity to grow more rapidly with each generation. This 50% cost is a fitness disadvantage of sexual reproduction. The two-fold cost of sex includes this cost and the fact that any organism can only pass on 50% of its own genes to its offspring. However, one definite advantage of sexual reproduction is that it increases genetic diversity and impedes the accumulation of harmful genetic mutations.Sexual selection is a mode of natural selection in which some individuals out-reproduce others of a population because they are better at securing mates for sexual reproduction. It has been described as \"a powerful evolutionary force that does not exist in asexual populations\".\n\n"
    },
    "Biology": {
        "url": "https://en.wikipedia.org/wiki/Biology",
        "summary": "Biology is the scientific study of life. It is a natural science with a broad scope but has several unifying themes that tie it together as a single, coherent field. For instance, all organisms are made up of cells that process hereditary information encoded in genes, which can be transmitted to future generations. Another major theme is evolution, which explains the unity and diversity of life. Energy processing is also important to life as it allows organisms to move, grow, and reproduce. Finally, all organisms are able to regulate their own internal environments.Biologists are able to study life at multiple levels of organization, from the molecular biology of a cell to the anatomy and physiology of plants and animals, and evolution of populations. Hence, there are multiple subdisciplines within biology, each defined by the nature of their research questions and the tools that they use. Like other scientists, biologists use the scientific method to make observations, pose questions, generate hypotheses, perform experiments, and form conclusions about the world around them.Life on Earth, which emerged more than 3.7 billion years ago, is immensely diverse. Biologists have sought to study and classify the various forms of life, from prokaryotic organisms such as archaea and bacteria to eukaryotic organisms such as protists, fungi, plants, and animals. These various organisms contribute to the biodiversity of an ecosystem, where they play specialized roles in the cycling of nutrients and energy through their biophysical environment.\n\n"
    },
    "Autoencoder": {
        "url": "https://en.wikipedia.org/wiki/Autoencoder",
        "summary": "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\nVariants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).\n\n"
    },
    "Mathematical optimization": {
        "url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
        "summary": "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.\n\n"
    }
}
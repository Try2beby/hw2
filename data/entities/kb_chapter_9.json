{
    "Convolution": {
        "url": "https://en.wikipedia.org/wiki/Convolution",
        "summary": "In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function (\n  \n    \n      \n        f\n        \u2217\n        g\n      \n    \n    {\\displaystyle f*g}\n  ) that expresses how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. The choice of which function is reflected and shifted before the integral does not change the integral result (see commutativity). The integral is evaluated for all values of shift, producing the convolution function.\nSome features of convolution are similar to cross-correlation: for real-valued functions, of a continuous or discrete variable, convolution (\n  \n    \n      \n        f\n        \u2217\n        g\n      \n    \n    {\\displaystyle f*g}\n  ) differs from cross-correlation (\n  \n    \n      \n        f\n        \u22c6\n        g\n      \n    \n    {\\displaystyle f\\star g}\n  ) only in that either f(x) or g(x) is reflected about the y-axis in convolution; thus it is a cross-correlation of g(\u2212x) and f(x), or f(\u2212x) and g(x). For complex-valued functions, the cross-correlation operator is the adjoint of the convolution operator.\nConvolution has applications that include probability, statistics, acoustics, spectroscopy, signal processing and image processing, geophysics, engineering, physics, computer vision and differential equations.The convolution can be defined for functions on Euclidean space and other groups (as algebraic structures). For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by periodic convolution. (See row 18 at DTFT \u00a7 Properties.) A discrete convolution can be defined for functions on the set of integers.\nGeneralizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.Computing the inverse of the convolution operation is known as deconvolution."
    },
    "Neural network": {
        "url": "https://en.wikipedia.org/wiki/Neural_network",
        "summary": "A neural network is a neural circuit of biological neurons, sometimes also called a biological neural network, or a network of artificial neurons or nodes in the case of an artificial neural network.Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1.\nThese artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information."
    },
    "Neurotransmitter": {
        "url": "https://en.wikipedia.org/wiki/Neurotransmitter",
        "summary": "A neurotransmitter is a signaling molecule secreted by a neuron to affect another cell across a synapse. The cell receiving the signal, or target cell, may be another neuron, but could also be a gland or muscle cell.Neurotransmitters are released from synaptic vesicles into the synaptic cleft where they are able to interact with neurotransmitter receptors on the target cell. The neurotransmitter's effect on the target cell is determined by the receptor it binds to. Many neurotransmitters are synthesized from simple and plentiful precursors such as amino acids, which are readily available and often require a small number of biosynthetic steps for conversion.\nNeurotransmitters are essential to the function of complex neural systems. The exact number of unique neurotransmitters in humans is unknown, but more than 100 have been identified. Common neurotransmitters include glutamate, GABA, acetylcholine, glycine and norepinephrine.\n\n"
    },
    "Plane (mathematics)": {
        "url": "https://en.wikipedia.org/wiki/Plane_(mathematics)",
        "summary": "In mathematics, a plane is a two-dimensional space or flat surface that extends indefinitely. \nA plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. \nWhen working exclusively in two-dimensional Euclidean space, the definite article is used, so the Euclidean plane refers to the whole space. \nMany fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional or planar space."
    },
    "Array (data type)": {
        "url": "https://en.wikipedia.org/wiki/Array_(data_type)",
        "summary": "In computer science, array is a data type that represents a collection of elements (values or variables), each selected by one or more indices (identifying keys) that can be computed at run time during program execution.  Such a collection is usually called an array variable or array value.  By analogy with the mathematical concepts vector and matrix, array types with one and two indices are often called vector type and matrix type, respectively. More generally, a multidimensional array type can be called a tensor type, by analogy with the physical concept, tensor.Language support for array types may include certain built-in array data types, some syntactic constructions (array type constructors) that the programmer may use to define such types and declare array variables, and special notation for indexing array elements.  For example, in the Pascal programming language, the declaration type MyTable = array [1..4,1..2] of integer, defines a new array data type called MyTable. The declaration var A: MyTable then defines a variable A of that type, which is an aggregate of eight elements, each being an integer variable identified by two indices. In the Pascal program, those elements are denoted A[1,1], A[1,2], A[2,1], \u2026, A[4,2].  Special array types are often defined by the language's standard libraries.\nDynamic lists are also more common and easier to implement than dynamic arrays. Array types are distinguished from record types mainly because they allow the element indices to be computed at run time, as in the Pascal assignment A[I,J] := A[N-I,2*J].  Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array variable.\nIn more theoretical contexts, especially in type theory and in the description of abstract algorithms, the terms \"array\" and \"array type\" sometimes refer to an abstract data type (ADT) also called abstract array or may refer to an associative array, a mathematical model with the basic operations and behavior of a typical array type in most languages \u2013  basically, a collection of elements that are selected by indices computed at run-time.\nDepending on the language, array types may overlap (or be identified with) other data types that describe aggregates of values, such as lists and strings.  Array types are often implemented by array data structures, but sometimes by other means, such as hash tables, linked lists, or search trees."
    },
    "Machine learning": {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods."
    },
    "Three-dimensional space": {
        "url": "https://en.wikipedia.org/wiki/Three-dimensional_space",
        "summary": "In geometry, a three-dimensional space (3D space, 3-space or, rarely, tri-dimensional space) is a mathematical space in which three values (coordinates) are required to determine the position of a point. Most commonly, it is the three-dimensional Euclidean space, that is, the Euclidean space of dimension three, which models physical space. More general three-dimensional spaces are called 3-manifolds. \nThe term may also refer colloquially to a subset of space, a three-dimensional region (or 3D domain), a solid figure.\nTechnically, a tuple of n numbers can be understood as the Cartesian coordinates of a location in a n-dimensional Euclidean space. The set of these n-tuples is commonly denoted \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbb {R} ^{n},}\n   and can be identified to the pair formed by a n-dimensional Euclidean space and a Cartesian coordinate system.\nWhen n = 3, this space is called the three-dimensional Euclidean space (or simply \"Euclidean space\" when the context is clear). In classical physics, it serves as a model of the physical universe, in which all known matter exists. When relativity theory is considered, it can be considered a local subspace of space-time. While this space remains the most compelling and useful way to model the world as it is experienced, it is only one example of a large variety of spaces in three dimensions called 3-manifolds. In this classical example, when the three values refer to measurements in different directions (coordinates), any three directions can be chosen, provided that these directions do not lie in the same plane. Furthermore, if these directions are pairwise perpendicular, the three values are often labeled by the terms width/breadth, height/depth, and length.\n\n"
    },
    "Convolutional neural network": {
        "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
        "summary": "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,  only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\nThey have applications in: \n\nimage and video recognition,\nrecommender systems,\n\nimage classification,\n\nimage segmentation,\n\nmedical image analysis,\n\nnatural language processing,\n\nbrain\u2013computer interfaces, and\n\nfinancial time series.CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increases the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage."
    },
    "Matrix multiplication": {
        "url": "https://en.wikipedia.org/wiki/Matrix_multiplication",
        "summary": "In mathematics, particularly in linear algebra, matrix multiplication is a binary operation that produces a matrix from two matrices. For matrix multiplication, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The resulting matrix, known as the matrix product, has the number of rows of the first and the number of columns of the second matrix. The product of matrices A and B is denoted as AB.Matrix multiplication was first described by the French mathematician Jacques Philippe Marie Binet in 1812, to represent the composition of linear maps that are represented by matrices. Matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering.\nComputing matrix products is a central operation in all computational applications of linear algebra."
    },
    "Strong prior": {
        "url": "https://en.wikipedia.org/wiki/Strong_prior",
        "summary": "In Bayesian statistics, a strong prior is a preceding assumption, theory, concept or idea upon which, after taking account of new information, a current assumption, theory, concept or idea is founded. The term is used to contrast the case of a weak or uninformative prior probability.  A strong prior would be a type of informative prior in which the information contained in the prior distribution dominates the information contained in the data being analysed. The Bayesian analysis combines the information contained in the prior with that extracted from the data to produce the posterior distribution which, in the case of a \"strong prior\", would be little changed from the prior distribution."
    },
    "Prior probability": {
        "url": "https://en.wikipedia.org/wiki/Prior_probability",
        "summary": "A prior probability distribution of an uncertain quantity, often simply called the prior, is its assumed probability distribution before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election. The unknown quantity may be a parameter of the model or a latent variable rather than an observable variable.\nIn Bayesian statistics, Bayes' rule prescribes how to update the prior with new information to obtain the posterior probability distribution, which is the conditional distribution of the uncertain quantity given new data. Historically, the choice of priors was often constrained to a conjugate family of a given likelihood function, for that it would result in a tractable posterior of the same family. The widespread availability of Markov chain Monte Carlo methods, however, has made this less of a concern.\nThere are many ways to construct a prior distribution. In some cases, a prior may be determined from past information, such as previous experiments. A prior can also be elicited from the purely subjective assessment of an experienced expert. When no information is available, an uninformative prior may be adopted as justified by the principle of indifference. In modern applications, priors are also often chosen for their mechanical properties, such as regularization and feature selection.The prior distributions of model parameters will often depend on parameters of their own. Uncertainty about these hyperparameters can, in turn, be expressed as hyperprior probability distributions. For example, if one uses a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then:\n\np is a parameter of the underlying system (Bernoulli distribution), and\n\u03b1 and \u03b2 are parameters of the prior distribution (beta distribution); hence hyperparameters.In principle, priors can be decomposed into many conditional levels of distributions, so-called hierarchical priors."
    },
    "Graphics processing unit": {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\n"
    },
    "Central processing unit": {
        "url": "https://en.wikipedia.org/wiki/Central_processing_unit",
        "summary": "A central processing unit (CPU)\u2014also called a central processor or main processor\u2014is the most important processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry, and specialized coprocessors such as graphics processing units (GPUs).\nThe form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic\u2013logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components.\nMost modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to support CPU-level multithreading. Most modern CPUs have privileged mode to support operating systems and hypervisor mode to support virtualization.\nAn IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC)."
    },
    "COVID-19 pandemic": {
        "url": "https://en.wikipedia.org/wiki/COVID-19_pandemic",
        "summary": "The COVID-19 pandemic, also known as the coronavirus pandemic, is a global pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The novel virus was first identified in an outbreak in the Chinese city of Wuhan in December 2019. Attempts to contain it there failed, allowing the virus to spread to other areas of Asia and then worldwide in early 2020. The World Health Organization (WHO) declared the outbreak a public health emergency of international concern (PHEIC) on 30 January 2020. The WHO ended its PHEIC declaration on 5 May 2023. As of 5 December 2023, the pandemic has caused 772,051,988 cases and 6,985,265 confirmed deaths, ranking it fifth in the list of the deadliest epidemics and pandemics in history.\nCOVID-19 symptoms range from asymptomatic to deadly, but most commonly include fever, sore throat, nocturnal cough, and fatigue. Transmission of the virus is often through airborne particles. Mutations have produced many strains (variants) with varying degrees of infectivity and virulence.COVID-19 vaccines were widely deployed in various countries beginning in December 2020. Treatments include novel antiviral drugs and symptom control. Common mitigation measures during the public health emergency included travel restrictions, lockdowns, business restrictions and closures, workplace hazard controls, mask mandates, quarantines, testing systems, and contact tracing of the infected.\nThe pandemic caused severe social and economic disruption around the world, including the largest global recession since the Great Depression. Widespread supply shortages, including food shortages, were caused by supply chain disruptions and panic buying. Reduced human activity led to an unprecedented decrease in pollution. Educational institutions and public areas were partially or fully closed in many jurisdictions, and events were cancelled or postponed during 2020, 2021, and 2022. Many white-collar workers began working from home. Misinformation circulated through social media and mass media, and political tensions intensified. The pandemic raised issues of racial and geographic discrimination, health equity, and the balance between public health imperatives and individual rights."
    },
    "2020": {
        "url": "https://en.wikipedia.org/wiki/2020",
        "summary": "2020 (MMXX) was a leap year starting on Wednesday of the Gregorian calendar, the 2020th year of the Common Era (CE) and Anno Domini (AD) designations, the 20th  year of the 3rd millennium and the 21st century, and the  1st   year of the 2020s decade.  \n2020 was heavily defined by the COVID-19 pandemic, which led to global social and economic disruption, mass cancellations and postponements of events, worldwide lockdowns and the largest economic recession since the Great Depression in the 1930s. Geospatial World also called 2020 \"the worst year in terms of climate change\" in part due to major climate disasters worldwide, including major bushfires in Australia and the western United States, as well as extreme tropical cyclone activity affecting large parts of North America. A United Nations progress report published in December 2020 indicated that none of the international Sustainable Development Goals for 2020 were achieved. Time magazine used its sixth ever Red X cover to declare 2020 \"the worst year ever\", although the cover article itself did not go as far, instead saying \"There have been worse years in U.S. history, and certainly worse years in world history, but most of us alive today have seen nothing like this one.\" The Golden Raspberry Awards also awarded the year the Special Governor's Award for The Worst Calendar Year Ever! at their 41st ceremony."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Learning": {
        "url": "https://en.wikipedia.org/wiki/Learning",
        "summary": "Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, animals, and some machines; there is also evidence for some kind of learning in certain plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulate from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be \"lost\" from that which cannot be retrieved.Human learning starts at birth (it might even start before in terms of an embryo's need for both interaction with, and freedom within its environment within the womb.) and continues until death as a consequence of ongoing interactions between people and their environment. The nature and processes involved in learning are studied in many established fields (including educational psychology, neuropsychology, experimental psychology, cognitive sciences, and pedagogy), as well as emerging fields of knowledge (e.g. with a shared interest in the topic of learning from safety events such as incidents/accidents, or in collaborative learning health systems). Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event cannot be avoided or escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development.Play has been approached by several theorists as a form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games. For Vygotsky, however, play is the first form of learning language and communication, and the stage where a child begins to understand rules and symbols. This has led to a view that learning in organisms is always related to semiosis, and is often associated with representational systems/activity."
    },
    "David H. Hubel": {
        "url": "https://en.wikipedia.org/wiki/David_H._Hubel",
        "summary": "David Hunter Hubel  (February 27, 1926 \u2013 September 22, 2013) was an American Canadian neurophysiologist noted for his studies of the structure and function of the visual cortex. He was co-recipient with Torsten Wiesel of the 1981 Nobel Prize in Physiology or Medicine (shared with Roger W. Sperry), for their discoveries concerning information processing in the visual system. For much of his career, Hubel worked as the Professor of Neurobiology at Johns Hopkins University and Harvard Medical School. In 1978, Hubel and Wiesel were awarded the Louisa Gross Horwitz Prize from Columbia University. In 1983, Hubel received the Golden Plate Award of the American Academy of Achievement."
    },
    "Neuroscience": {
        "url": "https://en.wikipedia.org/wiki/Neuroscience",
        "summary": "Neuroscience is the scientific study of the nervous system (the brain, spinal cord, and peripheral nervous system), its functions and disorders. It is a multidisciplinary science that combines physiology, anatomy, molecular biology, developmental biology, cytology, psychology, physics, computer science, chemistry, medicine, statistics, and mathematical modeling to understand the fundamental and emergent properties of neurons, glia and neural circuits. The understanding of the biological basis of learning, memory, behavior, perception, and consciousness has been described by Eric Kandel as the \"epic challenge\" of the biological sciences.The scope of neuroscience has broadened over time to include different approaches used to study the nervous system at different scales. The techniques used by neuroscientists have expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory, motor and cognitive tasks in the brain.\n\n"
    },
    "Neuron": {
        "url": "https://en.wikipedia.org/wiki/Neuron",
        "summary": "Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \nThe neuron is the main component of nervous tissue in all animals except sponges and placozoa. Non-animals like plants and fungi do not have nerve cells. The ability to generate electric signals first appeared in evolution 700 million years ago. 800 million years ago, predecessors of neurons were the peptidergic secretory cells. They eventually gained new gene modules which enabled cells to create post-synaptic scaffolds and ion channels that generate fast electrical signals. The ability to generate electric signals was a key innovation in the evolution of the nervous system.Neurons are typically classified into three types based on their function. Sensory neurons respond to stimuli such as touch, sound, or light that affect the cells of the sensory organs, and they send signals to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to control everything from muscle contractions to glandular output. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord. When multiple neurons are functionally connected together, they form what is called a neural circuit.\nNeurons are special cells which are made up of some structures that are common to all other eukaryotic cells such as the cell body (soma), a nucleus, smooth and rough endoplasmic reticulum, Golgi apparatus, mitochondria, and other cellular components. Additionally, neurons have other unique structures  such as dendrites, and a single axon. The soma is a compact structure, and the axon and dendrites are filaments extruding from the soma. Dendrites typically branch profusely and extend a few hundred micrometers from the soma. The axon leaves the soma at a swelling called the axon hillock and travels for as far as 1 meter in humans or more in other species. It branches but usually maintains a constant diameter. At the farthest tip of the axon's branches are axon terminals, where the neuron can transmit a signal across the synapse to another cell. Neurons may lack dendrites or have no axon. The term neurite is used to describe either a dendrite or an axon, particularly when the cell is undifferentiated.\nMost neurons receive signals via the dendrites and soma and send out signals down the axon. At the majority of synapses, signals cross from the axon of one neuron to a dendrite of another. However, synapses can connect an axon to another axon or a dendrite to another dendrite.\nThe signaling process is partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage gradients across their membranes. If the voltage changes by a large enough amount over a short interval, the neuron generates an all-or-nothing electrochemical pulse called an action potential. This potential travels rapidly along the axon and activates synaptic connections as it reaches them. Synaptic signals may be excitatory or inhibitory, increasing or reducing the net voltage that reaches the soma.\nIn most cases, neurons are generated by neural stem cells during brain development and childhood. Neurogenesis largely ceases during adulthood in most areas of the brain."
    },
    "Torsten Wiesel": {
        "url": "https://en.wikipedia.org/wiki/Torsten_Wiesel",
        "summary": "Torsten Nils Wiesel (born 3 June 1924) is a Swedish neurophysiologist. With David H. Hubel, he received the 1981 Nobel Prize in Physiology or Medicine, for their discoveries concerning information processing in the visual system; the prize was shared with Roger W. Sperry for his independent research on the cerebral hemispheres."
    },
    "Inferior temporal gyrus": {
        "url": "https://en.wikipedia.org/wiki/Inferior_temporal_gyrus",
        "summary": "The inferior temporal gyrus is one of three gyri of the temporal lobe and is located below the middle temporal gyrus, connected behind with the inferior occipital gyrus; it also extends around the infero-lateral border on to the inferior surface of the temporal lobe, where it is limited by the inferior sulcus. This region is one of the higher levels of the ventral stream of visual processing, associated with the representation of objects, places, faces, and colors.  It may also be involved in face perception, and in the recognition of numbers and words.The inferior temporal gyrus is the anterior region of the temporal lobe located underneath the central temporal sulcus. The primary function of the occipital temporal gyrus \u2013 otherwise referenced as IT cortex \u2013 is associated with visual stimuli processing, namely visual object recognition, and has been suggested by recent experimental results as the final location of the ventral cortical visual system. The IT cortex in humans is also known as the Inferior Temporal Gyrus since it has been located to a specific region of the human temporal lobe. The IT processes visual stimuli of objects in our field of vision, and is involved with memory and memory recall to identify that object; it is involved with the processing and perception created by visual stimuli amplified in the V1, V2, V3, and V4 regions of the occipital lobe. This region processes the color and form of the object in the visual field and is responsible for producing the \"what\" from this visual stimuli, or in other words identifying the object based on the color and form of the object and comparing that processed information to stored memories of objects to identify that object.The IT cortex's neurological significance is not just its contribution to the processing of visual stimuli in object recognition but also has been found to be a vital area with regards to simple processing of the visual field, difficulties with perceptual tasks and spatial awareness, and the location of unique single cells that possibly explain the IT cortex's relation to memory."
    },
    "Human brain": {
        "url": "https://en.wikipedia.org/wiki/Human_brain",
        "summary": "The human brain is the central organ of the human nervous system, and with the spinal cord makes up the central nervous system. The brain consists of the cerebrum, the brainstem and the cerebellum. It controls most of the activities of the body, processing, integrating, and coordinating the information it receives from the sense organs, and making decisions as to the instructions sent to the rest of the body. The brain is contained in, and protected by, the skull bones of the head.\nThe cerebrum, the largest part of the human brain, consists of two cerebral hemispheres. Each hemisphere has an inner core composed of white matter, and an outer surface \u2013 the cerebral cortex \u2013 composed of grey matter. The cortex has an outer layer, the neocortex, and an inner allocortex. The neocortex is made up of six neuronal layers, while the allocortex has three or four. Each hemisphere is conventionally divided into four lobes \u2013 the frontal, temporal, parietal, and occipital lobes. The frontal lobe is associated with executive functions including self-control, planning, reasoning, and abstract thought, while the occipital lobe is dedicated to vision. Within each lobe, cortical areas are associated with specific functions, such as the sensory, motor and association regions. Although the left and right hemispheres are broadly similar in shape and function, some functions are associated with one side, such as language in the left and visual-spatial ability in the right. The hemispheres are connected by commissural nerve tracts, the largest being the corpus callosum.\nThe cerebrum is connected by the brainstem to the spinal cord. The brainstem consists of the midbrain, the pons, and the medulla oblongata. The cerebellum is connected to the brainstem by three pairs of nerve tracts called cerebellar peduncles. Within the cerebrum is the ventricular system, consisting of four interconnected ventricles in which cerebrospinal fluid is produced and circulated. Underneath the cerebral cortex are several important structures, including the thalamus, the epithalamus, the pineal gland, the hypothalamus, the pituitary gland, and the subthalamus; the limbic structures, including the amygdalae and the hippocampi, the claustrum, the various nuclei of the basal ganglia, the basal forebrain structures, and the three circumventricular organs. Brain structures that are not on the midplane exist in pairs, so there are for example two hippocampi and two amygdalae. The cells of the brain include neurons and supportive glial cells. There are more than 86 billion neurons in the brain, and a more or less equal number of other cells. Brain activity is made possible by the interconnections of neurons and their release of neurotransmitters in response to nerve impulses. Neurons connect to form neural pathways, neural circuits, and elaborate network systems. The whole circuitry is driven by the process of neurotransmission.\nThe brain is protected by the skull, suspended in cerebrospinal fluid, and isolated from the bloodstream by the blood\u2013brain barrier. However, the brain is still susceptible to damage, disease, and infection. Damage can be caused by trauma, or a loss of blood supply known as a stroke. The brain is susceptible to degenerative disorders, such as Parkinson's disease, dementias including Alzheimer's disease, and multiple sclerosis. Psychiatric conditions, including schizophrenia and clinical depression, are thought to be associated with brain dysfunctions. The brain can also be the site of tumours, both benign and malignant; these mostly originate from other sites in the body.\nThe study of the anatomy of the brain is neuroanatomy, while the study of its function is neuroscience. Numerous techniques are used to study the brain. Specimens from other animals, which may be examined microscopically, have traditionally provided much information. Medical imaging technologies such as functional neuroimaging, and electroencephalography (EEG) recordings are important in studying the brain. The medical history of people with brain injury has provided insight into the function of each part of the brain. Neuroscience research has expanded considerably, and research is ongoing. \nIn culture, the philosophy of mind has for centuries attempted to address the question of the nature of consciousness and the mind\u2013body problem. The pseudoscience of phrenology attempted to localise personality attributes to regions of the cortex in the 19th century. In science fiction, brain transplants are imagined in tales such as the 1942 Donovan's Brain."
    },
    "Politeness": {
        "url": "https://en.wikipedia.org/wiki/Politeness",
        "summary": "Politeness is the practical application of good manners or etiquette so as not to offend others and to put them at ease. It is a culturally defined phenomenon, and therefore what is considered polite in one culture can sometimes be quite rude or simply eccentric in another cultural context.\nWhile the goal of politeness is to refrain from behaving in an offensive way so as not to offend others, and to make all people feel relaxed and comfortable with one another, these culturally defined standards at times may be manipulated."
    },
    "Backpropagation": {
        "url": "https://en.wikipedia.org/wiki/Backpropagation",
        "summary": "As a machine-learning algorithm, backpropagation is a crucial step in a common method used to iteratively train a neural network model. It is used to calculate the necessary parameter adjustments, to gradually minimize error.\nIn a multi-layered network, backpropagation is step 2.2 for training a neural network model:\n\nPropagate training data through the model from input to predicted output by computing the successive hidden layers' outputs and finally the final layer's output (the feedforward step).\nAdjust the model weights to reduce the error relative to the weights.\nThe error is typically the squared difference between prediction and target.\nFor each weight, the slope or derivative of the error is found, and the weight adjusted by a negative multiple of this derivative, so as to go downslope toward the minimum-error configuration.\nThis derivative is easy to calculate for final layer weights, and possible to calculate for one layer given the next layer's derivatives. Starting at the end, then, the derivatives are calculated layer by layer toward the beginning -- thus \"backpropagation\".\nRepeatedly update the weights until they converge or the model has undergone enough iterations.It is an efficient application of the Leibniz chain rule (1673) to such networks. It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).   The term \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory.Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input\u2013output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming. Gradient descent, or variants such as stochastic gradient descent, are commonly used.\nStrictly the term backpropagation refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm \u2013 including how the gradient is used, such as by stochastic gradient descent. In 1986 David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.\n\n"
    },
    "Alpha": {
        "url": "https://en.wikipedia.org/wiki/Alpha",
        "summary": "Alpha  (uppercase \u0391, lowercase \u03b1; Ancient Greek: \u1f04\u03bb\u03c6\u03b1, \u00e1lpha, or Greek: \u03ac\u03bb\u03c6\u03b1, romanized: \u00e1lfa) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for \"ox\". Letters that arose from alpha include the Latin letter A and the Cyrillic letter \u0410."
    },
    "Sine and cosine": {
        "url": "https://en.wikipedia.org/wiki/Sine_and_cosine",
        "summary": "In mathematics, sine and cosine are trigonometric functions of an angle. The sine and cosine of an acute angle are defined in the context of a right triangle: for the specified angle, its sine is the ratio of the length of the side that is opposite that angle to the length of the longest side of the triangle (the hypotenuse), and the cosine is the ratio of the length of the adjacent leg to that of the hypotenuse. For an angle \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , the sine and cosine functions are denoted simply as \n  \n    \n      \n        sin\n        \u2061\n        \u03b8\n      \n    \n    {\\displaystyle \\sin \\theta }\n   and \n  \n    \n      \n        cos\n        \u2061\n        \u03b8\n      \n    \n    {\\displaystyle \\cos \\theta }\n  .More generally, the definitions of sine and cosine can be extended to any real value in terms of the lengths of certain line segments in a unit circle. More modern definitions express the sine and cosine as infinite series, or as the solutions of certain differential equations, allowing their extension to arbitrary positive and negative values and even to complex numbers.\nThe sine and cosine functions are commonly used to model periodic phenomena such as sound and light waves, the position and velocity of harmonic oscillators, sunlight intensity and day length, and average temperature variations throughout the year. They can be traced to the jy\u0101 and ko\u1e6di-jy\u0101 functions used in Indian astronomy during the Gupta period."
    },
    "Beta": {
        "url": "https://en.wikipedia.org/wiki/Beta",
        "summary": "Beta (UK: , US: ; uppercase \u0392, lowercase \u03b2, or cursive \u03d0; Ancient Greek: \u03b2\u1fc6\u03c4\u03b1, romanized: b\u0113\u0302ta or Greek: \u03b2\u03ae\u03c4\u03b1, romanized: v\u00edta) is the second letter of the Greek alphabet. In the system of Greek numerals, it has a value of 2. In Ancient Greek, beta represented the voiced bilabial plosive IPA: [b]. In Modern Greek, it represents the voiced labiodental fricative IPA: [v] while IPA: [b] in borrowed words is instead commonly transcribed as \u03bc\u03c0. Letters that arose from beta include the Roman letter \u27e8B\u27e9 and the Cyrillic letters \u27e8\u0411\u27e9 and \u27e8\u0412\u27e9."
    },
    "Phi": {
        "url": "https://en.wikipedia.org/wiki/Phi",
        "summary": "Phi (; uppercase \u03a6, lowercase \u03c6 or \u03d5; Ancient Greek: \u03d5\u03b5\u1fd6 phe\u00ee [p\u02b0\u00e9\u00ee\u032f]; Modern Greek: \u03c6\u03b9 fi [fi]) is the twenty-first letter of the Greek alphabet.\nIn Archaic and Classical Greek (c. 9th century BC to 4th century BC), it represented an aspirated voiceless bilabial plosive ([p\u02b0]), which was the origin of its usual romanization as \u27e8ph\u27e9. During the later part of Classical Antiquity, in Koine Greek (c. 4th century BC to 4th century AD), its pronunciation shifted to that of a voiceless bilabial fricative ([\u0278]), and by the Byzantine Greek period (c. 4th century AD to 15th century AD) it developed its modern pronunciation as a voiceless labiodental fricative ([f]).\nThe romanization of the Modern Greek phoneme is therefore usually \u27e8f\u27e9.\nIt may be that phi originated as the letter qoppa (\u03d8, \u03d9), and initially represented the sound /k\u02b7\u02b0/ before shifting to Classical Greek [p\u02b0]. In traditional Greek numerals, phi has a value of 500 (\u03c6\u02b9) or 500,000 (\u0375\u03c6). The Cyrillic letter Ef (\u0424, \u0444) descends from phi.\nAs with other Greek letters, lowercase phi (encoded as the Unicode character U+03C6 \u03c6 GREEK SMALL LETTER PHI) is used as a mathematical or scientific symbol. Some uses require the old-fashioned 'closed' glyph, which is separately encoded as the Unicode character U+03D5 \u03d5 GREEK PHI SYMBOL."
    },
    "Neural coding": {
        "url": "https://en.wikipedia.org/wiki/Neural_coding",
        "summary": "Neural coding (or neural representation) is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among the electrical activity of the neurons in the ensemble. Based on the theory that\nsensory and other information is represented in the brain by networks of neurons, it is thought that neurons can encode both digital and analog information."
    },
    "Unsupervised learning": {
        "url": "https://en.wikipedia.org/wiki/Unsupervised_learning",
        "summary": "Unsupervised learning is a paradigm in machine learning where, in contrast to supervised learning and semi-supervised learning, algorithms learn patterns exclusively from unlabeled data.\n\n"
    },
    "ImageNet": {
        "url": "https://en.wikipedia.org/wiki/ImageNet",
        "summary": "The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories, with a typical category, such as \"balloon\" or \"strawberry\", consisting of several hundred images. The database of annotations of third-party image URLs is freely available directly from ImageNet, though the actual images are not owned by ImageNet. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. The challenge uses a \"trimmed\" list of one thousand non-overlapping classes."
    },
    "Microsoft": {
        "url": "https://en.wikipedia.org/wiki/Microsoft",
        "summary": "Microsoft Corporation is an American multinational technology corporation headquartered in Redmond, Washington. Microsoft's best-known software products are the Windows line of operating systems, the Microsoft 365 suite of productivity applications, and the Edge web browser. Its flagship hardware products are the Xbox video game consoles and the Microsoft Surface lineup of touchscreen personal computers. Microsoft ranked No. 14 in the 2022 Fortune 500 rankings of the largest United States corporations by total revenue; it was the world's largest software maker by revenue as of 2022. It is considered one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Apple, and Meta.\nMicrosoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Windows. The company's 1986 initial public offering (IPO) and subsequent rise in its share price created three billionaires and an estimated 12,000 millionaires among Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market and has made several corporate acquisitions, the largest being the acquisition of Activision Blizzard for $68.7 billion in October 2023, followed by its acquisition of LinkedIn for $26.2 billion in December 2016, and its acquisition of Skype Technologies for $8.5 billion in May 2011.As of 2015, Microsoft is market-dominant in the IBM PC compatible operating system market and the office software suite market, although it has lost the majority of the overall operating system market to Android. The company also produces a wide range of other consumer and enterprise software for desktops, laptops, tabs, gadgets, and servers, including Internet search (with Bing), the digital services market (through MSN), mixed reality (HoloLens), cloud computing (Azure), and software development (Visual Studio).\nSteve Ballmer replaced Gates as CEO in 2000 and later envisioned a \"devices and services\" strategy. This unfolded with Microsoft acquiring Danger Inc. in 2008, entering the personal computer production market for the first time in June 2012 with the launch of the Microsoft Surface line of tablet computers, and later forming Microsoft Mobile through the acquisition of Nokia's devices and services division. Since Satya Nadella took over as CEO in 2014, the company has scaled back on hardware and instead focused on cloud computing, a move that helped the company's shares reach their highest value since December 1999. Under Nadella's direction, the company has also heavily expanded its gaming business to support the Xbox brand, establishing the Microsoft Gaming division in 2022, dedicated to operating Xbox in addition to its three subsidiaries (publishers). Microsoft Gaming is the third-largest gaming company in the world by revenue as of 2023.Earlier dethroned by Apple in 2010, in 2018, Microsoft reclaimed its position as the most valuable publicly traded company in the world. In April 2019, Microsoft reached a trillion-dollar market cap, becoming the third U.S. public company to be valued at over $1 trillion after Apple and Amazon, respectively. As of 2023, Microsoft has the third-highest global brand valuation.\nMicrosoft has been criticized for its monopolistic practices and the company's software has been criticized for problems with ease of use, robustness, and security."
    }
}
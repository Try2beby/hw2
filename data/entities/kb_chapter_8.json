{
    "Gradient descent": {
        "url": "https://en.wikipedia.org/wiki/Gradient_descent",
        "summary": "Gradient descent (also often called steepest descent) is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\nThe idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.\nIt is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although both are iterative methods for optimization.\nGradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.A simple extension of gradient descent, stochastic gradient descent, serves as the most basic algorithm used for training most deep networks today.\n\n"
    },
    "Mathematical optimization": {
        "url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
        "summary": "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.\n\n"
    },
    "Loss function": {
        "url": "https://en.wikipedia.org/wiki/Loss_function",
        "summary": "In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function)  is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.\nIn statistics, typically a  loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century.  In the context of economics, for example, this is usually economic cost or regret.  In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cram\u00e9r in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Machine learning": {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods."
    },
    "Stochastic": {
        "url": "https://en.wikipedia.org/wiki/Stochastic",
        "summary": "Stochastic (; from Ancient Greek  \u03c3\u03c4\u03cc\u03c7\u03bf\u03c2 (st\u00f3khos) 'aim, guess') refers to the property of being well-described by a random probability distribution. Although stochasticity and randomness are distinct in that the former refers to a modeling approach and the latter refers to phenomena themselves, these two terms are often used synonymously. Furthermore, in probability theory, the formal concept of a stochastic process is also referred to as a random process.Stochasticity is used in many different fields, including the natural sciences such as biology, chemistry, ecology,  neuroscience, and physics, as well as technology and engineering fields such as image processing, signal processing, information theory, computer science, cryptography, and telecommunications. It is also used in finance, due to seemingly random changes in financial markets as well as in medicine, linguistics,  music, media, colour theory,  botany, manufacturing, and geomorphology.\n\n"
    },
    "Minimally invasive procedure": {
        "url": "https://en.wikipedia.org/wiki/Minimally_invasive_procedure",
        "summary": "Minimally invasive procedures (also known as minimally invasive surgeries) encompass surgical techniques that limit the size of incisions needed, thereby reducing wound healing time, associated pain, and risk of infection. Surgery by definition is invasive and many operations requiring incisions of some size are referred to as open surgery. Incisions made during open surgery can sometimes leave large wounds that may be painful and take a long time to heal. Advancements in medical technologies have enabled the development and regular use of minimally invasive procedures. For example, endovascular aneurysm repair, a minimally invasive surgery, has become the most common method of repairing abdominal aortic aneurysms in the US as of 2003. The procedure involves much smaller incisions than the corresponding open surgery procedure of open aortic surgery.Interventional radiologists were the forerunners of minimally invasive procedures. Using imaging techniques, radiologists were able to direct interventional instruments through the body by way of catheters instead of the large incisions needed in traditional surgery. As a result, many conditions once requiring surgery can now be treated non-surgically.Diagnostic techniques that do not involve incisions, puncturing the skin, or the introduction of foreign objects or materials into the body are known as non-invasive procedures. Several treatment procedures are classified as non-invasive. A major example of a non-invasive alternative treatment to surgery is radiation therapy, also called radiotherapy."
    },
    "Graphics processing unit": {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\n"
    },
    "Architecture": {
        "url": "https://en.wikipedia.org/wiki/Architecture",
        "summary": "Architecture is the art and technique of designing and building, as distinguished from the skills associated with construction. It is both the process and the product of sketching, conceiving, planning, designing, and constructing buildings or other structures. The term comes from Latin  architectura; from Ancient Greek  \u1f00\u03c1\u03c7\u03b9\u03c4\u03ad\u03ba\u03c4\u03c9\u03bd (arkhit\u00e9kt\u014dn) 'architect'; from  \u1f00\u03c1\u03c7\u03b9- (arkhi-) 'chief', and  \u03c4\u03ad\u03ba\u03c4\u03c9\u03bd (t\u00e9kt\u014dn) 'creator'. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements.The practice, which began in the prehistoric era, has been used as a way of expressing culture by civilizations on all seven continents. For this reason, architecture is considered to be a form of art. Texts on architecture have been written since ancient times. The earliest surviving text on architectural theories is the 1st century AD treatise De architectura by the Roman architect Vitruvius, according to whom a good building embodies firmitas, utilitas, and venustas (durability, utility, and beauty). Centuries later, Leon Battista Alberti developed his ideas further, seeing beauty as an objective quality of buildings to be found in their proportions. In the 19th century, Louis Sullivan declared that \"form follows function\". \"Function\" began to replace the classical \"utility\" and was understood to include not only practical but also aesthetic, psychological and cultural dimensions. The idea of sustainable architecture was introduced in the late 20th century.\nArchitecture began as rural, oral vernacular architecture that developed from trial and error to successful replication. Ancient urban architecture was preoccupied with building religious structures and buildings symbolizing the political power of rulers until Greek and Roman architecture shifted focus to civic virtues. Indian and Chinese architecture influenced forms all over Asia and Buddhist architecture in particular took diverse local flavors. During the Middle Ages, pan-European styles of Romanesque and Gothic cathedrals and abbeys emerged while the Renaissance favored Classical forms implemented by architects known by name. Later, the roles of architects and engineers became separated.\nModern architecture began after World War I as an avant-garde movement that sought to develop a completely new style appropriate for a new post-war social and economic order focused on meeting the needs of the middle and working classes. Emphasis was put on modern techniques, materials, and simplified geometric forms, paving the way for high-rise superstructures. Many architects became disillusioned with modernism which they perceived as ahistorical and anti-aesthetic, and postmodern and contemporary architecture developed. Over the years, the field of architectural construction has branched out to include everything from ship design to interior decorating."
    },
    "Central processing unit": {
        "url": "https://en.wikipedia.org/wiki/Central_processing_unit",
        "summary": "A central processing unit (CPU)\u2014also called a central processor or main processor\u2014is the most important processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry, and specialized coprocessors such as graphics processing units (GPUs).\nThe form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic\u2013logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components.\nMost modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to support CPU-level multithreading. Most modern CPUs have privileged mode to support operating systems and hypervisor mode to support virtualization.\nAn IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC)."
    },
    "0": {
        "url": "https://en.wikipedia.org/wiki/0",
        "summary": "0 (zero) is a number representing an empty quantity. Adding 0 to any number leaves that number unchanged. In mathematical terminology, 0 is the additive identity of the integers, rational numbers, real numbers, and complex numbers, as well as other algebraic structures. Multiplying any number by 0 has the result 0, and consequently, division by zero has no meaning in arithmetic.\nAs a numerical digit, 0 plays a crucial role in decimal notation: it indicates that the power of ten corresponding to the place containing a 0 does not contribute to the total. For example, \"205\" in decimal means two hundreds, no tens, and five ones. The same principle applies in place-value notations that uses a base other than ten, such as binary and hexadecimal. The modern use of 0 in this manner derives from Indian mathematics that was transmitted to Europe via medieval Islamic mathematicians and popularized by Fibonacci. It was independently used by the Maya.\nCommon names for the number 0 in English include zero, nought, naught (), and nil. In contexts where at least one adjacent digit distinguishes it from the letter O, the number is sometimes pronounced as oh or o (). Informal or slang terms for 0 include zilch and zip. Historically, ought, aught (), and cipher have also been used."
    },
    "Neuron": {
        "url": "https://en.wikipedia.org/wiki/Neuron",
        "summary": "Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \nThe neuron is the main component of nervous tissue in all animals except sponges and placozoa. Non-animals like plants and fungi do not have nerve cells. The ability to generate electric signals first appeared in evolution 700 million years ago. 800 million years ago, predecessors of neurons were the peptidergic secretory cells. They eventually gained new gene modules which enabled cells to create post-synaptic scaffolds and ion channels that generate fast electrical signals. The ability to generate electric signals was a key innovation in the evolution of the nervous system.Neurons are typically classified into three types based on their function. Sensory neurons respond to stimuli such as touch, sound, or light that affect the cells of the sensory organs, and they send signals to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to control everything from muscle contractions to glandular output. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord. When multiple neurons are functionally connected together, they form what is called a neural circuit.\nNeurons are special cells which are made up of some structures that are common to all other eukaryotic cells such as the cell body (soma), a nucleus, smooth and rough endoplasmic reticulum, Golgi apparatus, mitochondria, and other cellular components. Additionally, neurons have other unique structures  such as dendrites, and a single axon. The soma is a compact structure, and the axon and dendrites are filaments extruding from the soma. Dendrites typically branch profusely and extend a few hundred micrometers from the soma. The axon leaves the soma at a swelling called the axon hillock and travels for as far as 1 meter in humans or more in other species. It branches but usually maintains a constant diameter. At the farthest tip of the axon's branches are axon terminals, where the neuron can transmit a signal across the synapse to another cell. Neurons may lack dendrites or have no axon. The term neurite is used to describe either a dendrite or an axon, particularly when the cell is undifferentiated.\nMost neurons receive signals via the dendrites and soma and send out signals down the axon. At the majority of synapses, signals cross from the axon of one neuron to a dendrite of another. However, synapses can connect an axon to another axon or a dendrite to another dendrite.\nThe signaling process is partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage gradients across their membranes. If the voltage changes by a large enough amount over a short interval, the neuron generates an all-or-nothing electrochemical pulse called an action potential. This potential travels rapidly along the axon and activates synaptic connections as it reaches them. Synaptic signals may be excitatory or inhibitory, increasing or reducing the net voltage that reaches the soma.\nIn most cases, neurons are generated by neural stem cells during brain development and childhood. Neurogenesis largely ceases during adulthood in most areas of the brain."
    },
    "Neural network": {
        "url": "https://en.wikipedia.org/wiki/Neural_network",
        "summary": "A neural network is a neural circuit of biological neurons, sometimes also called a biological neural network, or a network of artificial neurons or nodes in the case of an artificial neural network.Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1.\nThese artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information."
    },
    "Eigenvalues and eigenvectors": {
        "url": "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors",
        "summary": "In linear algebra, it is often important to know which vectors have their directions unchanged by a linear transformation.  An eigenvector () or characteristic vector is such a vector.  Thus an eigenvector \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   of a linear transformation \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is scaled by a constant factor \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   when the linear transformation is applied to it: \n  \n    \n      \n        T\n        \n          v\n        \n        =\n        \u03bb\n        \n          v\n        \n      \n    \n    {\\displaystyle T\\mathbf {v} =\\lambda \\mathbf {v} }\n  .  The corresponding eigenvalue, characteristic value, or characteristic root is the multiplying factor \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  .\nGeometrically, vectors are multi-dimensional quantities with magnitude and direction, often pictured as arrows. A linear transformation rotates, stretches, or shears the vectors it acts upon. Its eigenvectors are those vectors that are only stretched, with no rotation or shear. The corresponding eigenvalue is the factor by which an eigenvector is stretched or squished. If the eigenvalue is negative, the eigenvector's direction is reversed.The eigenvectors and eigenvalues of a transformation serve to characterize it, and so they play important roles in all the areas where linear algebra is applied, from geology to quantum mechanics. In particular, it is often the case that a system is represented by a linear transformation whose outputs are fed as inputs to the same inputs (feedback).  In such an application, the largest eigenvalue is of particular importance, because it governs the long-term behavior of the system, after many applications of the linear transformation, and the associated eigenvector is the steady state of the system.\n\n"
    },
    "Hessian matrix": {
        "url": "https://en.wikipedia.org/wiki/Hessian_matrix",
        "summary": "In mathematics, the Hessian matrix, Hessian or (less commonly) Hesse matrix is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables. The Hessian matrix was developed in the 19th century by the German mathematician Ludwig Otto Hesse and later named after him. Hesse originally used the term \"functional determinants\". The Hessian is sometimes denoted by H or, ambiguously, by \u22072.\n\n"
    },
    "Saddle point": {
        "url": "https://en.wikipedia.org/wiki/Saddle_point",
        "summary": "In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function.  An example of a saddle point is when there is a critical point with a relative minimum along one axial direction (between peaks) and at a relative maximum along the crossing axis.  However, a saddle point need not be in this form.  For example, the function \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle f(x,y)=x^{2}+y^{3}}\n   has a critical point at \n  \n    \n      \n        (\n        0\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle (0,0)}\n   that is a saddle point since it is neither a relative maximum nor relative minimum, but it does not have a relative maximum or relative minimum in the \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  -direction.\nThe name derives from the fact that the prototypical example in two dimensions is a surface that curves up in one direction, and curves down in a different direction, resembling a riding saddle or a mountain pass between two peaks forming a landform saddle. In terms of contour lines, a saddle point in two dimensions gives rise to a contour map with a pair of lines intersecting at the point.  Such intersections are rare in actual ordnance survey maps, as the height of the saddle point is unlikely to coincide with the integer multiples used in such maps. Instead, the saddle point appears as a blank space in the middle of four sets of contour lines that approach and veer away from it. For a basic saddle point, these sets occur in pairs, with an opposing high pair and an opposing low pair positioned in orthogonal directions. The critical contour lines generally do not have to intersect orthogonally."
    },
    "Gradient": {
        "url": "https://en.wikipedia.org/wiki/Gradient",
        "summary": "In vector calculus, the gradient of a scalar-valued differentiable function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   of several variables is the vector field (or vector-valued function) \n  \n    \n      \n        \u2207\n        f\n      \n    \n    {\\displaystyle \\nabla f}\n   whose value at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   gives the direction and the rate of fastest increase. The gradient transforms like a vector under change of basis of the space of variables of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  . If the gradient of a function is non-zero at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , the direction of the gradient is the direction in which the function increases most quickly from \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , and the magnitude of the gradient is the rate of increase in that direction, the greatest absolute directional derivative. Further, a point where the gradient is the zero vector is known as a stationary point. The gradient thus plays a fundamental role in optimization theory, where it is used to maximize a function by gradient ascent. In coordinate-free terms, the gradient of a function \n  \n    \n      \n        f\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {r} )}\n   may be defined by:\n\nwhere \n  \n    \n      \n        d\n        f\n      \n    \n    {\\displaystyle df}\n   is the total infinitesimal change in \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   for an infinitesimal displacement  \n  \n    \n      \n        d\n        \n          r\n        \n      \n    \n    {\\displaystyle d\\mathbf {r} }\n  , and is seen to be maximal when \n  \n    \n      \n        d\n        \n          r\n        \n      \n    \n    {\\displaystyle d\\mathbf {r} }\n   is in the direction of the gradient \n  \n    \n      \n        \u2207\n        f\n      \n    \n    {\\displaystyle \\nabla f}\n  . The nabla symbol \n  \n    \n      \n        \u2207\n      \n    \n    {\\displaystyle \\nabla }\n  , written as an upside-down triangle and pronounced \"del\", denotes the vector differential operator.\nWhen a coordinate system is used in which the basis vectors are not functions of position, the gradient is given by the vector whose components are the partial derivatives of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  . That is, for \n  \n    \n      \n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle f\\colon \\mathbb {R} ^{n}\\to \\mathbb {R} }\n  , its gradient \n  \n    \n      \n        \u2207\n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\nabla f\\colon \\mathbb {R} ^{n}\\to \\mathbb {R} ^{n}}\n   is defined at the point \n  \n    \n      \n        p\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle p=(x_{1},\\ldots ,x_{n})}\n   in n-dimensional space as the vector\nNote that the above definition for gradient is only defined for the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  , if it is differentiable at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  . There can be functions for which partial derivatives exist in every direction but still fail to be differentiable. For example, the function \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          \n            \n              \n                x\n                \n                  2\n                \n              \n              y\n            \n            \n              \n                x\n                \n                  2\n                \n              \n              +\n              \n                y\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x,y)={\\frac {x^{2}y}{x^{2}+y^{2}}}}\n   unless at origin where \n  \n    \n      \n        f\n        (\n        0\n        ,\n        0\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(0,0)=0}\n  , is not differentiable at origin as it does not have a well defined tangent plane despite having well defined partial derivatives in every direction at the origin. In the particular example, under rotation of x-y coordinate system, the above formula for gradient fails to transform like a vector (gradient becomes dependent on choice of basis for coordinate system) and also fails to point towards the steepest ascent in some orientations. For differentiable functions where the formula for gradient holds, it can be shown to always transform as a vector under transformation of the basis so as to always \"point towards the fastest increase\".\nThe gradient is dual to the total derivative \n  \n    \n      \n        d\n        f\n      \n    \n    {\\displaystyle df}\n  : the value of the gradient at a point is a tangent vector \u2013 a vector at each point; while the value of the derivative at a point is a cotangent vector \u2013 a linear functional on vectors. They are related in that the dot product of the gradient of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   with another tangent vector \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   equals the directional derivative of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   of the function along \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  ; that is, \n  \n    \n      \n        \u2207\n        f\n        (\n        p\n        )\n        \u22c5\n        \n          v\n        \n        =\n        \n          \n            \n              \u2202\n              f\n            \n            \n              \u2202\n              \n                v\n              \n            \n          \n        \n        (\n        p\n        )\n        =\n        d\n        \n          f\n          \n            p\n          \n        \n        (\n        \n          v\n        \n        )\n      \n    \n    {\\textstyle \\nabla f(p)\\cdot \\mathbf {v} ={\\frac {\\partial f}{\\partial \\mathbf {v} }}(p)=df_{p}(\\mathbf {v} )}\n  . \nThe gradient admits multiple generalizations to more general functions on manifolds; see \u00a7 Generalizations."
    },
    "Atrophy": {
        "url": "https://en.wikipedia.org/wiki/Atrophy",
        "summary": "Atrophy is the partial or complete wasting away of a part of the body. Causes of atrophy include mutations (which can destroy the gene to build up the organ), poor nourishment, poor circulation, loss of hormonal support, loss of nerve supply to the target organ, excessive amount of apoptosis of cells, and disuse or lack of exercise or disease intrinsic to the tissue itself. In medical practice, hormonal and nerve inputs that maintain an organ or body part are said to have trophic effects. A diminished muscular trophic condition is designated as atrophy. Atrophy is reduction in size of cell, organ or tissue, after attaining its normal mature growth. In contrast, hypoplasia is the reduction in the cellular numbers of an organ, or tissue that has not attained normal maturity.\nAtrophy is the general physiological process of reabsorption and breakdown of tissues, involving apoptosis. When it occurs as a result of disease or loss of trophic support because of other diseases, it is termed pathological atrophy, although it can be a part of normal body development and homeostasis as well."
    },
    "Stochastic gradient descent": {
        "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent",
        "summary": "Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.While the basic idea behind stochastic approximation can be traced back to the Robbins\u2013Monro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning."
    },
    "Valley": {
        "url": "https://en.wikipedia.org/wiki/Valley",
        "summary": "A valley is an elongated low area often running between hills or mountains, which will typically contain a river or stream running from one end to the other. Most valleys are formed by erosion of the land surface by rivers or streams over a very long period. Some valleys are formed through erosion by glacial ice. These glaciers may remain present in valleys in high mountains or polar areas. \nAt lower latitudes and altitudes, these glacially formed valleys may have been created or enlarged during ice ages but now are ice-free and occupied by streams or rivers. In desert areas, valleys may be entirely dry or carry a watercourse only rarely. In areas of limestone bedrock, dry valleys may also result from drainage now taking place underground rather than at the surface. Rift valleys arise principally from earth movements, rather than erosion. Many different types of valleys are described by geographers, using terms that may be global in use or else applied only locally.\n\n"
    },
    "Canyon": {
        "url": "https://en.wikipedia.org/wiki/Canyon",
        "summary": "A canyon (from Spanish: ca\u00f1\u00f3n; archaic British English spelling: ca\u00f1on), gorge or chasm, is a deep cleft between escarpments or cliffs resulting from weathering and the erosive activity of a river over geologic time scales. Rivers have a natural tendency to cut through underlying surfaces, eventually wearing away rock layers as sediments are removed downstream. A river bed will gradually reach a baseline elevation, which is the same elevation as the body of water into which the river drains. The processes of weathering and erosion will form canyons when the river's headwaters and estuary are at significantly different elevations, particularly through regions where softer rock layers are intermingled with harder layers more resistant to weathering.\nA canyon may also refer to a rift between two mountain peaks, such as those in ranges including the Rocky Mountains, the Alps, the Himalayas or the Andes. Usually, a river or stream carves out such splits between mountains. Examples of mountain-type canyons are Provo Canyon in Utah or Yosemite Valley in California's Sierra Nevada. Canyons within mountains, or gorges that have an opening on only one side, are called box canyons. Slot canyons are very narrow canyons that often have smooth walls.\nSteep-sided valleys in the seabed of the continental slope are referred to as submarine canyons. Unlike canyons on land, submarine canyons are thought to be formed by turbidity currents and landslides."
    },
    "Alpha": {
        "url": "https://en.wikipedia.org/wiki/Alpha",
        "summary": "Alpha  (uppercase \u0391, lowercase \u03b1; Ancient Greek: \u1f04\u03bb\u03c6\u03b1, \u00e1lpha, or Greek: \u03ac\u03bb\u03c6\u03b1, romanized: \u00e1lfa) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for \"ox\". Letters that arose from alpha include the Latin letter A and the Cyrillic letter \u0410."
    },
    "Momentum": {
        "url": "https://en.wikipedia.org/wiki/Momentum",
        "summary": "In Newtonian mechanics, momentum (pl.: momenta or momentums; more specifically linear momentum or translational momentum) is the product of the mass and velocity of an object. It is a vector quantity, possessing a magnitude and a direction. If m is an object's mass and v is its velocity (also a vector quantity), then the object's momentum p (from Latin pellere \"push, drive\") is: \n  \n    \n      \n        \n          p\n        \n        =\n        m\n        \n          v\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {p} =m\\mathbf {v} .}\n  \nIn the International System of Units (SI), the unit of measurement of momentum is the kilogram metre per second (kg\u22c5m/s), which is equivalent to the newton-second.\nNewton's second law of motion states that the rate of change of a body's momentum is equal to the net force acting on it. Momentum depends on the frame of reference, but in any inertial frame it is a conserved quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity (with a modified formula) and, in a modified form,  in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.\nAdvanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is  generalized momentum, and in general this is different from the kinetic momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.\nIn continuous systems such as electromagnetic fields, fluid dynamics and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier\u2013Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids."
    },
    "Iteration": {
        "url": "https://en.wikipedia.org/wiki/Iteration",
        "summary": "Iteration is the repetition of a process in order to generate a (possibly unbounded) sequence of outcomes. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration. \nIn mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms."
    },
    "Determinism": {
        "url": "https://en.wikipedia.org/wiki/Determinism",
        "summary": "Determinism is the philosophical view that events are completely determined by previously existing causes. Deterministic theories throughout the history of philosophy have developed from diverse and sometimes overlapping motives and considerations. Like eternalism, determinism focuses on particular events rather than the future as a concept. The opposite of determinism is indeterminism, or the view that events are not deterministically caused but rather occur due to chance. Determinism is often contrasted with free will, although some philosophers claim that the two are compatible. Historically, debates about determinism have involved many philosophical positions and given rise to multiple varieties or interpretations of determinism. One topic of debate concerns the scope of determined systems. Some philosophers have maintained that the entire universe is a single determinate system, while others identify more limited determinate systems. Another common debate topic is whether determinism and free will can coexist; compatibilism and incompatibilism represent the opposing sides of this debate.\nDeterminism should not be confused with the self-determination of human actions by reasons, motives, and desires. Determinism is about interactions which affect cognitive processes in people's lives. It is about the cause and the result of what people have done. Cause and result are always bound together in cognitive processes. It assumes that if an observer has sufficient information about an object or human being, that such an observer might be able to predict every consequent move of that object or human being. Determinism rarely requires that perfect prediction be practically possible."
    },
    "Autoencoder": {
        "url": "https://en.wikipedia.org/wiki/Autoencoder",
        "summary": "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\nVariants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).\n\n"
    },
    "8\u00bd": {
        "url": "https://en.wikipedia.org/wiki/8%C2%BD",
        "summary": "8+1\u20442 (Italian title: Otto E Mezzo, pronounced [\u02c8\u0254tto e m\u02c8m\u025bddzo]) is an Italian 1963 avant-garde surrealist comedy-drama film directed and co-written (with Tullio Pinelli, Ennio Flaiano and Brunello Rondi) by Federico Fellini. The metafictional narrative centers on Guido Anselmi, played by Marcello Mastroianni, a famous Italian film director who suffers from stifled creativity as he attempts to direct an epic science fiction film. Claudia Cardinale, Anouk Aim\u00e9e, Sandra Milo, Rossella Falk, Barbara Steele, and Eddra Gale portray the various women in Guido's life. The film is shot in black and white by cinematographer Gianni Di Venanzo and features a soundtrack by Nino Rota, with costume and set designs by Piero Gherardi.\n8+1\u20442 was critically acclaimed and won the Academy Award for Best Foreign Language Film and Best Costume Design (black-and-white). It is acknowledged as an avant-garde film and a highly influential classic. It was ranked 10th on the British Film Institute's The Sight & Sound Greatest Films of All Time 2012 critics' poll and 4th by directors. It is listed on the Vatican's compilation of the 45 best films made before 1995, the 100th anniversary of cinema. The film ranked 7th in BBC's 2018 list of The 100 Greatest Foreign Language Films voted by 209 film critics from 43 countries around the world.The film was also included on the Italian Ministry of Cultural Heritage's 100 Italian films to be saved, a list of 100 films that \"have changed the collective memory of the country between 1942 and 1978\". It is considered to be one of the greatest and most influential films of all time."
    },
    "Levenberg\u2013Marquardt algorithm": {
        "url": "https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm",
        "summary": "In mathematics and computing, the Levenberg\u2013Marquardt algorithm (LMA or just LM), also known as the  damped least-squares (DLS) method, is used to solve non-linear least squares problems.  These minimization problems arise especially in least squares curve fitting.  The LMA interpolates between the Gauss\u2013Newton algorithm (GNA) and the method of gradient descent. The LMA is more robust than the GNA, which means that in many cases it finds a solution even if it starts very far off the final minimum. For well-behaved functions and reasonable starting parameters, the LMA tends to be slower than the GNA. LMA can also be viewed as Gauss\u2013Newton using a trust region approach.\nThe algorithm was first published in 1944 by Kenneth Levenberg, while working at the Frankford Army Arsenal. It was rediscovered in 1963 by Donald Marquardt, who worked as a statistician at DuPont, and independently by Girard, Wynne and Morrison.The LMA is used in many software applications for solving generic curve-fitting problems. By using the Gauss\u2013Newton algorithm it often converges faster than first-order methods. However, like other iterative optimization algorithms, the LMA finds only a local minimum, which is not necessarily the global minimum."
    },
    "Converse (logic)": {
        "url": "https://en.wikipedia.org/wiki/Converse_(logic)",
        "summary": "In logic and mathematics, the converse of a categorical or implicational statement is the result of reversing its two constituent statements. For the implication P \u2192 Q, the converse is Q \u2192 P. For the categorical proposition All S are P, the converse is All P are S. Either way, the truth of the converse is generally independent from that of the original statement."
    },
    "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm": {
        "url": "https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm",
        "summary": "In numerical optimization, the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems. Like the related Davidon\u2013Fletcher\u2013Powell method, BFGS determines the descent direction by preconditioning the gradient with curvature information. It does so by gradually improving an approximation to the Hessian matrix of the loss function, obtained only from gradient evaluations (or approximate gradient evaluations) via a generalized secant method.Since the updates of the BFGS curvature matrix do not require matrix inversion, its computational complexity is only \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(n^{2})}\n  , compared to \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(n^{3})}\n   in Newton's method. Also in common use is L-BFGS, which is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables (e.g., >1000). The BFGS-B variant handles simple box constraints.The algorithm is named after Charles George Broyden, Roger Fletcher, Donald Goldfarb and David Shanno."
    },
    "Batch normalization": {
        "url": "https://en.wikipedia.org/wiki/Batch_normalization",
        "summary": "Batch normalization (also known as batch norm) is a method used to make training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network. Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. However, at initialization, batch normalization in fact induces severe gradient explosion in deep networks, which is only alleviated by skip connections in residual networks. Others maintain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."
    },
    "2": {
        "url": "https://en.wikipedia.org/wiki/2",
        "summary": "2 (two) is a number, numeral and digit. It is the natural number following 1 and preceding 3. It is the smallest and only even prime number. Because it forms the basis of a duality, it has religious and spiritual significance in many cultures."
    },
    "3": {
        "url": "https://en.wikipedia.org/wiki/3",
        "summary": "3 (three) is a number, numeral and digit. It is the natural number following 2 and preceding 4, and is the smallest odd prime number and the only prime preceding a square number. It has religious or cultural significance in many societies."
    },
    "Long short-term memory": {
        "url": "https://en.wikipedia.org/wiki/Long_short-term_memory",
        "summary": "Long short-term memory (LSTM) network is a recurrent neural network (RNN), aimed to deal with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\". It is applicable to classification, processing and predicting data based on time series, such as in handwriting, speech recognition,  machine translation, speech activity detection, robot control, video games, and healthcare.A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. Forget gates decide what information to discard from a previous state by assigning a previous state, compared to a current input, a value between 0 and 1. A (rounded) value of 1 means to keep the information, and a value of 0 means to discard it. Input gates decide which pieces of new information to store in the current state, using the same system as forget gates. Output gates control which pieces of information in the current state to output by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.\n\n"
    },
    "Initial point": {
        "url": "https://en.wikipedia.org/wiki/Initial_point",
        "summary": "In surveying, an initial point is a datum (a specific point on the surface of the earth) that marks the beginning point for a cadastral survey. The initial point establishes a local geographic coordinate system for the surveys that refer to that point.\nAn initial point is defined by the intersection of a principal meridian and a base line."
    },
    "Recurrent neural network": {
        "url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
        "summary": "A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. The term \"recurrent neural network\" is used to refer to the class of networks with an infinite impulse response, whereas \"convolutional neural network\" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\nAdditional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs."
    }
}
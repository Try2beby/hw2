{
    "Arithmetic underflow": {
        "url": "https://en.wikipedia.org/wiki/Arithmetic_underflow",
        "summary": "The term arithmetic underflow (also floating point underflow, or just underflow) is a condition in a computer program where the result of a calculation is a number of more precise absolute value than the computer can actually represent in memory on its central processing unit (CPU).\nArithmetic underflow can occur when the true result of a floating point operation is smaller in magnitude (that is, closer to zero) than the smallest value representable as a normal floating point number in the target datatype. Underflow can in part be regarded as negative overflow of the exponent of the floating point value. For example, if the exponent part can represent values from \u2212128 to 127, then a result with a value less than \u2212128 may cause underflow.\nStoring values that are too low in an integer variable (e.g., attempting to store \u22121 in an unsigned integer) is properly referred to as integer overflow, or more broadly, integer wraparound. The term underflow normally refers to floating point numbers only, which is a separate issue. It is impossible in most floating-point designs to store a too-low value, as usually they are signed and have a negative infinity value."
    },
    "Round-off error": {
        "url": "https://en.wikipedia.org/wiki/Round-off_error",
        "summary": "In computing, a roundoff error, also called rounding error, is the difference between the result produced by a given algorithm using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic. Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them. This is a form of quantization error. When using approximation equations or algorithms, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits), one of the goals of numerical analysis is to estimate computation errors. Computation errors, also called numerical errors, include both truncation errors and roundoff errors.\nWhen a sequence of calculations with an input involving any roundoff error are made, errors may accumulate, sometimes dominating the calculation. In ill-conditioned problems, significant error may accumulate.In short, there are two major facets of roundoff errors involved in numerical calculations:\nThe ability of computers to represent both magnitude and precision of numbers is inherently limited.\nCertain numerical manipulations are highly sensitive to roundoff errors. This can result from both mathematical considerations as well as from the way in which computers perform arithmetic operations."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Mathematics": {
        "url": "https://en.wikipedia.org/wiki/Mathematics",
        "summary": "Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory, algebra, geometry, and analysis, respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.\nMost mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or\u2014in modern mathematics\u2014entities that are stipulated to have certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and\u2014in case of abstraction from nature\u2014some basic properties that are considered true starting points of the theory under consideration.Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.\nHistorically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was essentially divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new areas. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid lockstep increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics.\n\n"
    },
    "Library": {
        "url": "https://en.wikipedia.org/wiki/Library",
        "summary": "A library is a collection of books, and possibly other materials and media, that is accessible for use by its members and members of allied institutions. Libraries provide physical (hard copies) or digital (soft copies) materials, and may be a physical location, a virtual space, or both. \nA library's collection normally includes printed materials which may be borrowed, and usually also includes a reference section of publications which may only be utilized inside the premises. Resources such as commercial releases of films, television programmes, other video recordings, radio, music and audio recordings may be available in many formats. These include DVDs, Blu-rays, CDs, cassettes, or other applicable formats such as microform. They may also provide access to information, music or other content held on bibliographic databases.\nLibraries can vary widely in size and may be organised and maintained by a public body such as a government, an institution (such as a school or museum), a corporation, or a private individual. In addition to providing materials, libraries also provide the services of librarians who are trained experts in finding, selecting, circulating and organising information while interpreting information needs and navigating and analysing large amounts of information with a variety of resources.\nLibrary buildings often provide quiet areas for studying, as well as common areas for group study and collaboration, and may provide public facilities for access to their electronic resources, such as computers and access to the Internet. \nThe library's clientele and general services offered vary depending on its type: users of a public library have different needs from those of a special library or academic library, for example. Libraries may also be community hubs, where programmes are made available and people engage in lifelong learning. Modern libraries extend their services beyond the physical walls of the building by providing material accessible by electronic means, including from home via the Internet.\nThe services that libraries offer are variously described as library services, information services, or the combination \"library and information services\", although different institutions and sources define such terminology differently."
    },
    "Theano": {
        "url": "https://en.wikipedia.org/wiki/Theano",
        "summary": "In Greek mythology, Theano (; Ancient Greek: \u0398\u03b5\u03b1\u03bd\u03ce) may refer to the following personages:\n\nTheano, wife of Metapontus, king of Icaria. Metapontus demanded that she bear him children, or leave the kingdom. She presented the children of Melanippe to her husband, as if they were her own. Later Theano bore him two sons of her own and, wishing to leave the kingdom to her own children, sent them to kill Melanippe's. In the fight that ensued, her two sons were killed, and she committed suicide upon hearing the news.\nTheano, one of the Dana\u00efdes, daughter of Danaus and Polyxo. She married (and murdered) Phantes, son of Aegyptus and Caliadne.\nTheano, a priestess of Athena in Troy during the Trojan War. She was a daughter of King Cisseus of Thrace and wife of Antenor, one of the Trojan elders.\nTheano or Theona, a character appearing in the Aeneid, the consort of Amycus."
    },
    "0": {
        "url": "https://en.wikipedia.org/wiki/0",
        "summary": "0 (zero) is a number representing an empty quantity. Adding 0 to any number leaves that number unchanged. In mathematical terminology, 0 is the additive identity of the integers, rational numbers, real numbers, and complex numbers, as well as other algebraic structures. Multiplying any number by 0 has the result 0, and consequently, division by zero has no meaning in arithmetic.\nAs a numerical digit, 0 plays a crucial role in decimal notation: it indicates that the power of ten corresponding to the place containing a 0 does not contribute to the total. For example, \"205\" in decimal means two hundreds, no tens, and five ones. The same principle applies in place-value notations that uses a base other than ten, such as binary and hexadecimal. The modern use of 0 in this manner derives from Indian mathematics that was transmitted to Europe via medieval Islamic mathematicians and popularized by Fibonacci. It was independently used by the Maya.\nCommon names for the number 0 in English include zero, nought, naught (), and nil. In contexts where at least one adjacent digit distinguishes it from the letter O, the number is sometimes pronounced as oh or o (). Informal or slang terms for 0 include zilch and zip. Historically, ought, aught (), and cipher have also been used."
    },
    "Hill climbing": {
        "url": "https://en.wikipedia.org/wiki/Hill_climbing",
        "summary": "In numerical analysis, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by making an incremental change to the solution. If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found.\nFor example, hill climbing can be applied to the travelling salesman problem. It is easy to find an initial solution that visits all the cities but will likely be very poor compared to the optimal solution. The algorithm starts with such a solution and makes small improvements to it, such as switching the order in which two cities are visited. Eventually, a much shorter route is likely to be obtained.\nHill climbing finds optimal solutions for convex problems \u2013 for other problems it will find only local optima (solutions that cannot be improved upon by any neighboring configurations), which are not necessarily the best possible solution (the global optimum) out of all possible solutions (the search space). Examples of algorithms that solve convex problems by hill-climbing include the simplex algorithm for linear programming and binary search.:\u200a253\u200a To attempt to avoid getting stuck in local optima, one could use restarts (i.e. repeated local search), or more complex schemes based on iterations (like iterated local search), or on memory (like reactive search optimization and tabu search), or on memory-less stochastic modifications (like simulated annealing).\nThe relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. It is used widely in artificial intelligence, for reaching a goal state from a starting node. Different choices for next nodes and starting nodes are used in related algorithms. Although more advanced algorithms such as simulated annealing or tabu search may give better results, in some situations hill climbing works just as well. Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, such as with real-time systems, so long as a small number of increments typically converges on a good solution (the optimal solution or a close approximation). At the other extreme, bubble sort can be viewed as a hill climbing algorithm (every adjacent element exchange decreases the number of disordered element pairs), yet this approach is far from efficient for even modest N, as the number of exchanges required grows quadratically. \nHill climbing is an anytime algorithm: it can return a valid solution even if it's interrupted at any time before it ends."
    },
    "Gradient descent": {
        "url": "https://en.wikipedia.org/wiki/Gradient_descent",
        "summary": "Gradient descent (also often called steepest descent) is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\nThe idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.\nIt is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although both are iterative methods for optimization.\nGradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.A simple extension of gradient descent, stochastic gradient descent, serves as the most basic algorithm used for training most deep networks today.\n\n"
    },
    "Mathematical optimization": {
        "url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
        "summary": "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.\n\n"
    },
    "Derivative": {
        "url": "https://en.wikipedia.org/wiki/Derivative",
        "summary": "In mathematics, the derivative shows the sensitivity of change of a function's output with respect to the input. Derivatives are a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances.\nThe derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value.  For this reason, the derivative is often described as the \"instantaneous rate of change\", the ratio of the instantaneous change in the dependent variable to that of the independent variable.\nDerivatives can be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables.  It can be calculated in terms of the partial derivatives with respect to the independent variables.  For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.\nThe process of finding a derivative is called differentiation. The reverse process is called antidifferentiation.  The fundamental theorem of calculus relates antidifferentiation with integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus."
    },
    "Curvature": {
        "url": "https://en.wikipedia.org/wiki/Curvature",
        "summary": "In mathematics, curvature is any of several strongly related concepts in geometry. Intuitively, the curvature is the amount by which a curve deviates from being a straight line, or a surface deviates from being a plane.\nFor curves, the canonical example is that of a circle, which has a curvature equal to the reciprocal of its radius. Smaller circles bend more sharply, and hence have higher curvature. The curvature at a point of a differentiable curve is the curvature of its osculating circle, that is the circle that best approximates the curve near this point. The curvature of a straight line is zero. In contrast to the tangent, which is a vector quantity, the curvature at a point is typically a scalar quantity, that is, it is expressed by a single real number.\nFor surfaces (and, more generally for higher-dimensional manifolds), that are embedded in a Euclidean space, the concept of curvature is more complex, as it depends on the choice of a direction on the surface or manifold. This leads to the concepts of maximal curvature, minimal curvature, and mean curvature.\nFor Riemannian manifolds (of dimension at least two) that are not necessarily embedded in a Euclidean space, one can define the curvature intrinsically, that is without referring to an external space. See Curvature of Riemannian manifolds for the definition, which is done in terms of lengths of curves traced on the manifold, and expressed, using linear algebra, by the Riemann curvature tensor."
    },
    "Second derivative": {
        "url": "https://en.wikipedia.org/wiki/Second_derivative",
        "summary": "In calculus, the second derivative, or the second-order derivative, of a function f is the derivative of the derivative of f. Informally, the second derivative can be phrased as \"the rate of change of the rate of change\"; for example, the second derivative of the position of an object with respect to time is the instantaneous acceleration of the object, or the rate at which the velocity of the object is changing with respect to time. In Leibniz notation:\n\nwhere a is acceleration, v is velocity, t is time, x is position, and d is the instantaneous \"delta\" or change. The last expression \n  \n    \n      \n        \n          \n            \n              \n                \n                  d\n                  \n                    2\n                  \n                \n                x\n              \n              \n                d\n                \n                  t\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {d^{2}x}{dt^{2}}}}\n   is the second derivative of position (x) with respect to time.\nOn the graph of a function, the second derivative corresponds to the curvature or concavity of the graph. The graph of a function with a positive second derivative is upwardly concave, while the graph of a function with a negative second derivative curves in the opposite way.\n\n"
    },
    "Gradient": {
        "url": "https://en.wikipedia.org/wiki/Gradient",
        "summary": "In vector calculus, the gradient of a scalar-valued differentiable function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   of several variables is the vector field (or vector-valued function) \n  \n    \n      \n        \u2207\n        f\n      \n    \n    {\\displaystyle \\nabla f}\n   whose value at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   gives the direction and the rate of fastest increase. The gradient transforms like a vector under change of basis of the space of variables of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  . If the gradient of a function is non-zero at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , the direction of the gradient is the direction in which the function increases most quickly from \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , and the magnitude of the gradient is the rate of increase in that direction, the greatest absolute directional derivative. Further, a point where the gradient is the zero vector is known as a stationary point. The gradient thus plays a fundamental role in optimization theory, where it is used to maximize a function by gradient ascent. In coordinate-free terms, the gradient of a function \n  \n    \n      \n        f\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {r} )}\n   may be defined by:\n\nwhere \n  \n    \n      \n        d\n        f\n      \n    \n    {\\displaystyle df}\n   is the total infinitesimal change in \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   for an infinitesimal displacement  \n  \n    \n      \n        d\n        \n          r\n        \n      \n    \n    {\\displaystyle d\\mathbf {r} }\n  , and is seen to be maximal when \n  \n    \n      \n        d\n        \n          r\n        \n      \n    \n    {\\displaystyle d\\mathbf {r} }\n   is in the direction of the gradient \n  \n    \n      \n        \u2207\n        f\n      \n    \n    {\\displaystyle \\nabla f}\n  . The nabla symbol \n  \n    \n      \n        \u2207\n      \n    \n    {\\displaystyle \\nabla }\n  , written as an upside-down triangle and pronounced \"del\", denotes the vector differential operator.\nWhen a coordinate system is used in which the basis vectors are not functions of position, the gradient is given by the vector whose components are the partial derivatives of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  . That is, for \n  \n    \n      \n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle f\\colon \\mathbb {R} ^{n}\\to \\mathbb {R} }\n  , its gradient \n  \n    \n      \n        \u2207\n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\nabla f\\colon \\mathbb {R} ^{n}\\to \\mathbb {R} ^{n}}\n   is defined at the point \n  \n    \n      \n        p\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle p=(x_{1},\\ldots ,x_{n})}\n   in n-dimensional space as the vector\nNote that the above definition for gradient is only defined for the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  , if it is differentiable at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  . There can be functions for which partial derivatives exist in every direction but still fail to be differentiable. For example, the function \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          \n            \n              \n                x\n                \n                  2\n                \n              \n              y\n            \n            \n              \n                x\n                \n                  2\n                \n              \n              +\n              \n                y\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x,y)={\\frac {x^{2}y}{x^{2}+y^{2}}}}\n   unless at origin where \n  \n    \n      \n        f\n        (\n        0\n        ,\n        0\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(0,0)=0}\n  , is not differentiable at origin as it does not have a well defined tangent plane despite having well defined partial derivatives in every direction at the origin. In the particular example, under rotation of x-y coordinate system, the above formula for gradient fails to transform like a vector (gradient becomes dependent on choice of basis for coordinate system) and also fails to point towards the steepest ascent in some orientations. For differentiable functions where the formula for gradient holds, it can be shown to always transform as a vector under transformation of the basis so as to always \"point towards the fastest increase\".\nThe gradient is dual to the total derivative \n  \n    \n      \n        d\n        f\n      \n    \n    {\\displaystyle df}\n  : the value of the gradient at a point is a tangent vector \u2013 a vector at each point; while the value of the derivative at a point is a cotangent vector \u2013 a linear functional on vectors. They are related in that the dot product of the gradient of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   with another tangent vector \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   equals the directional derivative of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   of the function along \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  ; that is, \n  \n    \n      \n        \u2207\n        f\n        (\n        p\n        )\n        \u22c5\n        \n          v\n        \n        =\n        \n          \n            \n              \u2202\n              f\n            \n            \n              \u2202\n              \n                v\n              \n            \n          \n        \n        (\n        p\n        )\n        =\n        d\n        \n          f\n          \n            p\n          \n        \n        (\n        \n          v\n        \n        )\n      \n    \n    {\\textstyle \\nabla f(p)\\cdot \\mathbf {v} ={\\frac {\\partial f}{\\partial \\mathbf {v} }}(p)=df_{p}(\\mathbf {v} )}\n  . \nThe gradient admits multiple generalizations to more general functions on manifolds; see \u00a7 Generalizations."
    },
    "Third derivative": {
        "url": "https://en.wikipedia.org/wiki/Third_derivative",
        "summary": "In calculus, a branch of mathematics, the third derivative or third-order derivative is the rate at which the second derivative, or the rate of change of the rate of change, is changing. The third derivative of a function \n  \n    \n      \n        y\n        =\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle y=f(x)}\n   can be denoted by\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  3\n                \n              \n              y\n            \n            \n              d\n              \n                x\n                \n                  3\n                \n              \n            \n          \n        \n        ,\n        \n        \n          f\n          \u2034\n        \n        (\n        x\n        )\n        ,\n        \n        \n          or \n        \n        \n          \n            \n              d\n              \n                3\n              \n            \n            \n              d\n              \n                x\n                \n                  3\n                \n              \n            \n          \n        \n        [\n        f\n        (\n        x\n        )\n        ]\n        .\n      \n    \n    {\\displaystyle {\\frac {d^{3}y}{dx^{3}}},\\quad f'''(x),\\quad {\\text{or }}{\\frac {d^{3}}{dx^{3}}}[f(x)].}\n  Other notations can be used, but the above are the most common."
    },
    "2": {
        "url": "https://en.wikipedia.org/wiki/2",
        "summary": "2 (two) is a number, numeral and digit. It is the natural number following 1 and preceding 3. It is the smallest and only even prime number. Because it forms the basis of a duality, it has religious and spiritual significance in many cultures."
    },
    "Saddle point": {
        "url": "https://en.wikipedia.org/wiki/Saddle_point",
        "summary": "In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function.  An example of a saddle point is when there is a critical point with a relative minimum along one axial direction (between peaks) and at a relative maximum along the crossing axis.  However, a saddle point need not be in this form.  For example, the function \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle f(x,y)=x^{2}+y^{3}}\n   has a critical point at \n  \n    \n      \n        (\n        0\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle (0,0)}\n   that is a saddle point since it is neither a relative maximum nor relative minimum, but it does not have a relative maximum or relative minimum in the \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  -direction.\nThe name derives from the fact that the prototypical example in two dimensions is a surface that curves up in one direction, and curves down in a different direction, resembling a riding saddle or a mountain pass between two peaks forming a landform saddle. In terms of contour lines, a saddle point in two dimensions gives rise to a contour map with a pair of lines intersecting at the point.  Such intersections are rare in actual ordnance survey maps, as the height of the saddle point is unlikely to coincide with the integer multiples used in such maps. Instead, the saddle point appears as a blank space in the middle of four sets of contour lines that approach and veer away from it. For a basic saddle point, these sets occur in pairs, with an opposing high pair and an opposing low pair positioned in orthogonal directions. The critical contour lines generally do not have to intersect orthogonally."
    },
    "Eigenvalues and eigenvectors": {
        "url": "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors",
        "summary": "In linear algebra, it is often important to know which vectors have their directions unchanged by a linear transformation.  An eigenvector () or characteristic vector is such a vector.  Thus an eigenvector \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   of a linear transformation \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is scaled by a constant factor \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   when the linear transformation is applied to it: \n  \n    \n      \n        T\n        \n          v\n        \n        =\n        \u03bb\n        \n          v\n        \n      \n    \n    {\\displaystyle T\\mathbf {v} =\\lambda \\mathbf {v} }\n  .  The corresponding eigenvalue, characteristic value, or characteristic root is the multiplying factor \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  .\nGeometrically, vectors are multi-dimensional quantities with magnitude and direction, often pictured as arrows. A linear transformation rotates, stretches, or shears the vectors it acts upon. Its eigenvectors are those vectors that are only stretched, with no rotation or shear. The corresponding eigenvalue is the factor by which an eigenvector is stretched or squished. If the eigenvalue is negative, the eigenvector's direction is reversed.The eigenvectors and eigenvalues of a transformation serve to characterize it, and so they play important roles in all the areas where linear algebra is applied, from geology to quantum mechanics. In particular, it is often the case that a system is represented by a linear transformation whose outputs are fed as inputs to the same inputs (feedback).  In such an application, the largest eigenvalue is of particular importance, because it governs the long-term behavior of the system, after many applications of the linear transformation, and the associated eigenvector is the steady state of the system.\n\n"
    },
    "Breast": {
        "url": "https://en.wikipedia.org/wiki/Breast",
        "summary": "The breast is one of two prominences located on the upper ventral region of a primate's torso. Both females and males develop breasts from the same embryological tissues.\nIn females, it serves as the mammary gland, which produces and secretes milk to feed infants. Subcutaneous fat covers and envelops a network of ducts that converge on the nipple, and these tissues give the breast its size and shape. At the ends of the ducts are lobules, or clusters of alveoli, where milk is produced and stored in response to hormonal signals. During pregnancy, the breast responds to a complex interaction of hormones, including estrogens, progesterone, and prolactin, that mediate the completion of its development, namely lobuloalveolar maturation, in preparation of lactation and breastfeeding.\nHumans are the only animals with permanent breasts. At puberty, estrogens, in conjunction with growth hormone, cause permanent breast growth in female humans. This happens only to a much lesser extent in other primates\u2014breast development in other primates generally only occurs with pregnancy. Along with their major function in providing nutrition for infants, female breasts have social and sexual characteristics. Breasts have been featured in ancient and modern sculpture, art, and photography. They can figure prominently in the perception of a woman's body and sexual attractiveness. A number of cultures associate breasts with sexuality and tend to regard bare breasts in public as immodest or indecent. Breasts, especially the nipples, are an erogenous zone."
    },
    "Convex optimization": {
        "url": "https://en.wikipedia.org/wiki/Convex_optimization",
        "summary": "Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets (or, equivalently, maximizing concave functions over convex sets).  Many classes of convex optimization problems admit polynomial-time algorithms, whereas mathematical optimization is in general NP-hard.Convex optimization has applications in a wide range of disciplines, such as automatic control systems, estimation and signal processing, communications and networks, electronic circuit design, data analysis and modeling, finance, statistics (optimal experimental design), and structural optimization, where the approximation concept has proven to be efficient. \nWith recent advancements in computing and optimization algorithms, convex programming is nearly as straightforward as linear programming."
    },
    "Deep learning": {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "summary": "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog."
    },
    "Optimization problem": {
        "url": "https://en.wikipedia.org/wiki/Optimization_problem",
        "summary": "In mathematics, engineering, computer science and economics, an optimization problem is the problem of finding the best solution from all feasible solutions.\nOptimization problems can be divided into two categories, depending on whether the variables are continuous or discrete: \n\nAn optimization problem with discrete variables is known as a discrete optimization, in which an object such as an integer, permutation or graph must be found from a countable set.\nA problem with continuous variables is known as a continuous optimization, in which an optimal value from a continuous function must be found. They can include constrained problems and multimodal problems.\n\n"
    },
    "Constrained optimization": {
        "url": "https://en.wikipedia.org/wiki/Constrained_optimization",
        "summary": "In mathematical optimization, constrained optimization (in some contexts called constraint optimization) is the process of optimizing an objective function with respect to some variables  in the presence of constraints on those variables. The objective function is either a cost function or energy function, which is to be minimized, or a reward function or utility function, which is to be maximized. Constraints can be either hard constraints, which set conditions for the variables that are required to be satisfied, or soft constraints, which have some variable values that are penalized in the objective function if, and based on the extent that, the conditions on the variables are not satisfied.\n\n"
    },
    "Graphics processing unit": {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\n"
    }
}
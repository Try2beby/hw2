{
    "Machine learning": {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods."
    },
    "Algorithm": {
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    "Google Street View": {
        "url": "https://en.wikipedia.org/wiki/Google_Street_View",
        "summary": "Google Street View is a technology featured in Google Maps and Google Earth that provides interactive panoramas from positions along many streets in the world. It was launched in 2007 in several cities in the United States, and has since expanded to include all of the country's major and minor cities, as well as cities and rural areas of many other countries worldwide. Streets with Street View imagery available are shown as blue lines on Google Maps.\nGoogle Street View displays interactively panoramas of stitched VR photographs. Most photography is done by car, but some is done by tricycle, camel, boat, snowmobile, underwater apparatus, and on foot."
    },
    "Google Maps": {
        "url": "https://en.wikipedia.org/wiki/Google_Maps",
        "summary": "Google Maps is a web mapping platform and consumer application offered by Google. It offers satellite imagery, aerial photography, street maps, 360\u00b0 interactive panoramic views of streets (Street View), real-time traffic conditions, and route planning for traveling by foot, car, bike, air (in beta) and public transportation. As of 2020, Google Maps was being used by over one billion people every month around the world.Google Maps began as a C++ desktop program developed by brothers Lars and Jens Rasmussen at Where 2 Technologies. In October 2004, the company was acquired by Google, which converted it into a web application. After additional acquisitions of a geospatial data visualization company and a real-time traffic analyzer, Google Maps was launched in February 2005. The service's front end utilizes JavaScript, XML, and Ajax. Google Maps offers an API that allows maps to be embedded on third-party websites, and offers a locator for businesses and other organizations in numerous countries around the world. Google Map Maker allowed users to collaboratively expand and update the service's mapping worldwide but was discontinued from March 2017. However, crowdsourced contributions to Google Maps were not discontinued as the company announced those features would be transferred to the Google Local Guides program.\nGoogle Maps' satellite view is a \"top-down\" or bird's-eye view; most of the high-resolution imagery of cities is aerial photography taken from aircraft flying at 800 to 1,500 feet (240 to 460 m), while most other imagery is from satellites. Much of the available satellite imagery is no more than three years old and is updated on a regular basis, according to a 2011 report. Google Maps previously used a variant of the Mercator projection, and therefore could not accurately show areas around the poles. In August 2018, the desktop version of Google Maps was updated to show a 3D globe. It is still possible to switch back to the 2D map in the settings.\nGoogle Maps for mobile devices were first released in 2006; the latest versions feature GPS turn-by-turn navigation along with dedicated parking assistance features. By 2013, it was found to be the world's most popular smartphone app, with over 54% of global smartphone owners using it. In 2017, the app was reported to have two billion users on Android, along with several other Google services including YouTube, Chrome, Gmail, Search, and Google Play."
    },
    "ImageNet": {
        "url": "https://en.wikipedia.org/wiki/ImageNet",
        "summary": "The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories, with a typical category, such as \"balloon\" or \"strawberry\", consisting of several hundred images. The database of annotations of third-party image URLs is freely available directly from ImageNet, though the actual images are not owned by ImageNet. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. The challenge uses a \"trimmed\" list of one thousand non-overlapping classes."
    },
    "Computer vision": {
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "summary": "Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images (the input to the retina in the human analog) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\nThe scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\nSub-domains of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. \nAdopting computer vision technology might be painstaking for organizations as there is no single point solution for it. There are very few companies that provide a unified and distributed platform or an Operating System where computer vision applications can be easily deployed and managed."
    },
    "Data": {
        "url": "https://en.wikipedia.org/wiki/Data",
        "summary": "In common usage and statistics, data (US: ; UK: ) is a collection of discrete or continuous values that convey information, describing the quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted formally.  A datum is an individual value in a collection of data. Data is usually organized into structures such as tables that provide additional context and meaning, and which may themselves be used as data in larger structures. Data may be used as variables in a computational process. Data may represent abstract ideas or concrete measurements.\nData is commonly used in scientific research, economics, and in virtually every other form of human organizational activity.  Examples of data sets include price indices (such as consumer price index), unemployment rates, literacy rates, and census data. In this context, data represents the raw facts and figures from which useful information can be extracted. \nData is collected using techniques such as measurement, observation, query, or analysis, and is typically represented as numbers or characters which may be further processed. Field data is data that is collected in an uncontrolled in-situ environment. Experimental data is data that is generated in the course of a controlled scientific experiment.  Data is analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis.  Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed and obvious instrument or data entry errors are corrected.\nData can be seen as the smallest units of factual information that can be used as a basis for calculation, reasoning, or discussion. Data can range from abstract ideas to concrete measurements, including, but not limited to, statistics. Thematically connected data presented in some relevant context can be viewed as information. Contextually connected pieces of information can then be described as data insights or intelligence. The stock of insights and intelligence that accumulates over time resulting from the synthesis of data into information, can then be described as knowledge. Data has been described as \"the new oil of the digital economy\". Data, as a general concept, refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.\nAdvances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible. (Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.) In response, the relatively new field of data science uses machine learning (and other artificial intelligence (AI)) methods that allow for efficient applications of analytic methods to big data."
    },
    "Computation": {
        "url": "https://en.wikipedia.org/wiki/Computation",
        "summary": "A computation is any type of arithmetic or non-arithmetic calculation that is well-defined. Common examples of computations are mathematical equations and computer algorithms.\nMechanical or electronic devices (or, historically, people) that perform computations are known as computers. The study of computation is the field of computability, itself a sub-field of computer science."
    },
    "Graphics processing unit": {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\n"
    },
    "Logistic regression": {
        "url": "https://en.wikipedia.org/wiki/Logistic_regression",
        "summary": "In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See \u00a7 Background and \u00a7 Definition for formal mathematics, and \u00a7 Example for a worked example.\nBinary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see \u00a7 Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See \u00a7 Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.\nAnalogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see \u00a7 Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the \"simplest\" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see \u00a7 Maximum entropy.\nThe parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see \u00a7 Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see \u00a7 Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined \"logit\"; see \u00a7 History."
    },
    "Neural network": {
        "url": "https://en.wikipedia.org/wiki/Neural_network",
        "summary": "A neural network is a neural circuit of biological neurons, sometimes also called a biological neural network, or a network of artificial neurons or nodes in the case of an artificial neural network.Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1.\nThese artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information."
    },
    "Computational resource": {
        "url": "https://en.wikipedia.org/wiki/Computational_resource",
        "summary": "In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems.\nThe simplest computational resources are computation time, the number of steps necessary to solve a problem, and memory space, the amount of storage needed while solving the problem, but many more complicated resources have been defined.A computational problem is generally defined in terms of its action on any valid input. Examples of problems might be \"given an integer n, determine whether n is prime\", or \"given two numbers x and y, calculate the product x*y\".  As the inputs get bigger, the amount of computational resources needed to solve a problem will increase.  Thus, the resources needed to solve a problem are described in terms of asymptotic analysis, by identifying the resources as a function of the length or size of the input.  Resource usage is often partially quantified using Big O notation.\nComputational resources are useful because we can study which problems can be computed in a certain amount of each computational resource.  In this way, we can determine whether algorithms for solving the problem are optimal and we can make statements about an algorithm's efficiency.  The set of all of the computational problems that can be solved using a certain amount of a certain computational resource is a complexity class, and relationships between different complexity classes are one of the most important topics in complexity theory."
    },
    "Spearmint": {
        "url": "https://en.wikipedia.org/wiki/Spearmint",
        "summary": "Spearmint, also known as garden mint, common mint, lamb mint and mackerel mint, is a species of mint, Mentha spicata (, native to Europe and southern temperate Asia, extending from Ireland in the west to southern China in the east. It is naturalized in many other temperate parts of the world, including northern and southern Africa, North America, and South America. It is used as a flavouring in food and herbal teas. The aromatic oil, called oil of spearmint, is also used as a flavoring and sometimes as a scent.\nThe species and its subspecies have many synonyms, including Mentha crispa, Mentha crispata, and Mentha viridis."
    },
    "Hyperparameter optimization": {
        "url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization",
        "summary": "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.  The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance, and therefore choose the set of values for hyperparameters that maximize it."
    },
    "Gradient descent": {
        "url": "https://en.wikipedia.org/wiki/Gradient_descent",
        "summary": "Gradient descent (also often called steepest descent) is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\nThe idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.\nIt is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although both are iterative methods for optimization.\nGradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.A simple extension of gradient descent, stochastic gradient descent, serves as the most basic algorithm used for training most deep networks today.\n\n"
    },
    "Software": {
        "url": "https://en.wikipedia.org/wiki/Software",
        "summary": "Software is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.\nAt the lowest programming level, executable code consists of machine language instructions supported by an individual processor\u2014typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer\u2014an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to \"jump\" to a different instruction or is interrupted by the operating system. As of 2023, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.\nThe majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler, an interpreter, or a combination of the two. Software may also be written in a low-level assembly language that has a strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.\n\n"
    },
    "Autoencoder": {
        "url": "https://en.wikipedia.org/wiki/Autoencoder",
        "summary": "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\nVariants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).\n\n"
    },
    "Gradient": {
        "url": "https://en.wikipedia.org/wiki/Gradient",
        "summary": "In vector calculus, the gradient of a scalar-valued differentiable function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   of several variables is the vector field (or vector-valued function) \n  \n    \n      \n        \u2207\n        f\n      \n    \n    {\\displaystyle \\nabla f}\n   whose value at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   gives the direction and the rate of fastest increase. The gradient transforms like a vector under change of basis of the space of variables of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  . If the gradient of a function is non-zero at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , the direction of the gradient is the direction in which the function increases most quickly from \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , and the magnitude of the gradient is the rate of increase in that direction, the greatest absolute directional derivative. Further, a point where the gradient is the zero vector is known as a stationary point. The gradient thus plays a fundamental role in optimization theory, where it is used to maximize a function by gradient ascent. In coordinate-free terms, the gradient of a function \n  \n    \n      \n        f\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {r} )}\n   may be defined by:\n\nwhere \n  \n    \n      \n        d\n        f\n      \n    \n    {\\displaystyle df}\n   is the total infinitesimal change in \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   for an infinitesimal displacement  \n  \n    \n      \n        d\n        \n          r\n        \n      \n    \n    {\\displaystyle d\\mathbf {r} }\n  , and is seen to be maximal when \n  \n    \n      \n        d\n        \n          r\n        \n      \n    \n    {\\displaystyle d\\mathbf {r} }\n   is in the direction of the gradient \n  \n    \n      \n        \u2207\n        f\n      \n    \n    {\\displaystyle \\nabla f}\n  . The nabla symbol \n  \n    \n      \n        \u2207\n      \n    \n    {\\displaystyle \\nabla }\n  , written as an upside-down triangle and pronounced \"del\", denotes the vector differential operator.\nWhen a coordinate system is used in which the basis vectors are not functions of position, the gradient is given by the vector whose components are the partial derivatives of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  . That is, for \n  \n    \n      \n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle f\\colon \\mathbb {R} ^{n}\\to \\mathbb {R} }\n  , its gradient \n  \n    \n      \n        \u2207\n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\nabla f\\colon \\mathbb {R} ^{n}\\to \\mathbb {R} ^{n}}\n   is defined at the point \n  \n    \n      \n        p\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle p=(x_{1},\\ldots ,x_{n})}\n   in n-dimensional space as the vector\nNote that the above definition for gradient is only defined for the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  , if it is differentiable at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  . There can be functions for which partial derivatives exist in every direction but still fail to be differentiable. For example, the function \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          \n            \n              \n                x\n                \n                  2\n                \n              \n              y\n            \n            \n              \n                x\n                \n                  2\n                \n              \n              +\n              \n                y\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x,y)={\\frac {x^{2}y}{x^{2}+y^{2}}}}\n   unless at origin where \n  \n    \n      \n        f\n        (\n        0\n        ,\n        0\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(0,0)=0}\n  , is not differentiable at origin as it does not have a well defined tangent plane despite having well defined partial derivatives in every direction at the origin. In the particular example, under rotation of x-y coordinate system, the above formula for gradient fails to transform like a vector (gradient becomes dependent on choice of basis for coordinate system) and also fails to point towards the steepest ascent in some orientations. For differentiable functions where the formula for gradient holds, it can be shown to always transform as a vector under transformation of the basis so as to always \"point towards the fastest increase\".\nThe gradient is dual to the total derivative \n  \n    \n      \n        d\n        f\n      \n    \n    {\\displaystyle df}\n  : the value of the gradient at a point is a tangent vector \u2013 a vector at each point; while the value of the derivative at a point is a cotangent vector \u2013 a linear functional on vectors. They are related in that the dot product of the gradient of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at a point \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   with another tangent vector \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   equals the directional derivative of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   at \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   of the function along \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  ; that is, \n  \n    \n      \n        \u2207\n        f\n        (\n        p\n        )\n        \u22c5\n        \n          v\n        \n        =\n        \n          \n            \n              \u2202\n              f\n            \n            \n              \u2202\n              \n                v\n              \n            \n          \n        \n        (\n        p\n        )\n        =\n        d\n        \n          f\n          \n            p\n          \n        \n        (\n        \n          v\n        \n        )\n      \n    \n    {\\textstyle \\nabla f(p)\\cdot \\mathbf {v} ={\\frac {\\partial f}{\\partial \\mathbf {v} }}(p)=df_{p}(\\mathbf {v} )}\n  . \nThe gradient admits multiple generalizations to more general functions on manifolds; see \u00a7 Generalizations."
    },
    "Overfitting": {
        "url": "https://en.wikipedia.org/wiki/Overfitting",
        "summary": "In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.:\u200a45\u200aUnderfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.\nThe possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend. \nAs an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. \nThe potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.\nTo lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter."
    }
}
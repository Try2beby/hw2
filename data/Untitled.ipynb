{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c427152e-fb64-4fd2-a1d5-e14f848019aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:57:48.815737Z",
     "iopub.status.busy": "2024-02-21T07:57:48.815737Z",
     "iopub.status.idle": "2024-02-21T07:57:48.819684Z",
     "shell.execute_reply": "2024-02-21T07:57:48.819684Z",
     "shell.execute_reply.started": "2024-02-21T07:57:48.815737Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import math\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import ahocorasick\n",
    "import networkx as nx\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import torch\n",
    "import transformers\n",
    "from fuzzywuzzy import fuzz\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dataDir = \"../data/\"\n",
    "dataName = \"Deep Learning.pdf\"\n",
    "import codecs\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# openai.api_base = \"https://api.chatanywhere.com.cn/\"\n",
    "openai.api_base = \"https://api.chatanywhere.cn/\"\n",
    "openai.api_key = \"sk-LzwgVgu5xvNPpwoqCdeeVcAt7Tu7ZoZICXzzkheldIbXA60h\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f4fef-8b2d-41ae-8af6-552e79070234",
   "metadata": {},
   "source": [
    "# 1. 获取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb9e3a41-3514-4a05-ba63-0459ef06b1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:33:37.177648Z",
     "iopub.status.busy": "2024-02-21T07:33:37.176648Z",
     "iopub.status.idle": "2024-02-21T07:35:17.816953Z",
     "shell.execute_reply": "2024-02-21T07:35:17.816953Z",
     "shell.execute_reply.started": "2024-02-21T07:33:37.176648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4ba26d1cce4fdc94509a48c196676b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in G:\\Crime\\BERT\\large\\models--bert-large-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22cbbe37e974e24b7da133cc6659fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e6f3f98e6c4754ae57c7338769b992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78161159244c43c3be13361917eb2aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased', cache_dir=\"../../../BERT/large\")\n",
    "model = BertModel.from_pretrained(\"bert-large-cased\", cache_dir=\"../../../BERT/large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364fd41-4285-4f7d-bdee-b743a822c9de",
   "metadata": {},
   "source": [
    "# 2. 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8647860d-012f-40e4-8eba-7b61399be761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:24:34.772336Z",
     "iopub.status.busy": "2024-02-21T09:24:34.772336Z",
     "iopub.status.idle": "2024-02-21T09:24:34.774880Z",
     "shell.execute_reply": "2024-02-21T09:24:34.774880Z",
     "shell.execute_reply.started": "2024-02-21T09:24:34.772336Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6304d6-f72e-4c38-b3a2-5548ca3a6459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:06:59.601657Z",
     "iopub.status.busy": "2024-02-21T07:06:59.601657Z",
     "iopub.status.idle": "2024-02-21T07:08:29.596720Z",
     "shell.execute_reply": "2024-02-21T07:08:29.596720Z",
     "shell.execute_reply.started": "2024-02-21T07:06:59.601657Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c589298bd94afd98c712d5b9b873c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pdfplumber.open(dataDir + dataName) as f:\n",
    "    # 目录架构生成\n",
    "    c, p, n = [], [], []\n",
    "    for i in range(7):\n",
    "        page = f.pages[i]\n",
    "        text = page.extract_text()\n",
    "        text_split = text.split(\"\\n\")\n",
    "        for i in text_split:\n",
    "            if bool(re.match(\"[0-9]+\\.[0-9]+\", i.split(\" \")[0])):\n",
    "                c.append(i.split(\" \")[0])\n",
    "                p.append(int(i.split(\" \")[-1]) + 15)\n",
    "            if bool(re.match(\"[0-9]+\", i.split(\" \")[0])):\n",
    "                for j in i.split(\" \"):\n",
    "                    if bool(re.match(\"[A-Za-z]+\", j)):\n",
    "                        n.append((i.split(\" \")[0], j))\n",
    "                        \n",
    "p_range = list(zip(p, p[1:]))\n",
    "p_range.append((735, 800))\n",
    "c_p_range = list(zip(c, p_range))\n",
    "index_dict = collections.defaultdict(list)\n",
    "for k, v in c_p_range:\n",
    "    index_dict[k.split(\".\")[0]].append((k, v))\n",
    "\n",
    "with pdfplumber.open(dataDir + dataName) as f:\n",
    "    content_dict = collections.defaultdict(list)\n",
    "\n",
    "    for k, v in tqdm(index_dict.items(), total=len(index_dict)):\n",
    "        for i in v:\n",
    "            page_range = i[-1]\n",
    "            if page_range[0] == page_range[1]:\n",
    "                page_range = (page_range[0], page_range[1] + 1)\n",
    "            for j in range(int(page_range[0]) - 1, int(page_range[1]) - 1):\n",
    "                page = f.pages[j]\n",
    "\n",
    "                text = page.extract_text().replace(\"\\n\", \" \")\n",
    "\n",
    "                content_dict[i[0]].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bf916877-3181-43fe-982d-a01b65019e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:04:08.794020Z",
     "iopub.status.busy": "2024-02-21T10:04:08.793020Z",
     "iopub.status.idle": "2024-02-21T10:04:09.438041Z",
     "shell.execute_reply": "2024-02-21T10:04:09.438041Z",
     "shell.execute_reply.started": "2024-02-21T10:04:08.794020Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = content_dict['1.1'][1]\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1b38cbdc-a105-4b75-a049-cc06d171d8de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:06:58.022544Z",
     "iopub.status.busy": "2024-02-21T10:06:58.022544Z",
     "iopub.status.idle": "2024-02-21T10:06:58.025866Z",
     "shell.execute_reply": "2024-02-21T10:06:58.025866Z",
     "shell.execute_reply.started": "2024-02-21T10:06:58.022544Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(101)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "abf2d1f4-4005-4205-bc7d-ee5cac04745d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:04:23.894258Z",
     "iopub.status.busy": "2024-02-21T10:04:23.893257Z",
     "iopub.status.idle": "2024-02-21T10:04:23.897512Z",
     "shell.execute_reply": "2024-02-21T10:04:23.897512Z",
     "shell.execute_reply.started": "2024-02-21T10:04:23.894258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] CHAPTER 1. INTRODUCTION Deep learning Example : Shallow Example : Example : Example : autoencoders Logistic Knowledge MLPs regression bases Representation learning Machine learning AI Figure 1. 4 : A Venn diagram showing how deep learning is a kind of representation learning, which is in turn a kind of machine learning, which is used for many but not all approaches to AI. Each section of the Venn diagram includes an example of an AI technology. 9 [SEP]']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5dbcc9ee-e1d2-4acd-8a89-cae79f65f795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:42:37.504459Z",
     "iopub.status.busy": "2024-02-21T07:42:37.504459Z",
     "iopub.status.idle": "2024-02-21T07:42:37.508065Z",
     "shell.execute_reply": "2024-02-21T07:42:37.508065Z",
     "shell.execute_reply.started": "2024-02-21T07:42:37.504459Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1128,  112,  173,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tokenizer(\"you'd\", return_tensors='pt')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a384404d-811c-465c-b54c-5ab0a8190459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:42:37.712195Z",
     "iopub.status.busy": "2024-02-21T07:42:37.712195Z",
     "iopub.status.idle": "2024-02-21T07:42:37.715386Z",
     "shell.execute_reply": "2024-02-21T07:42:37.715386Z",
     "shell.execute_reply.started": "2024-02-21T07:42:37.712195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] you'd [SEP]\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(test['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58af77aa-04e5-42b2-9482-5a10b3cc17f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T07:36:48.206397Z",
     "iopub.status.busy": "2024-02-21T07:36:48.205396Z",
     "iopub.status.idle": "2024-02-21T07:36:48.209975Z",
     "shell.execute_reply": "2024-02-21T07:36:48.209975Z",
     "shell.execute_reply.started": "2024-02-21T07:36:48.206397Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3712, -0.6087,  0.6811,  ..., -0.1764, -0.2981,  0.1912],\n",
       "         [ 0.2954,  0.1486, -0.4758,  ..., -1.0722,  0.7647, -0.2719],\n",
       "         [-0.0229,  0.2538,  0.4917,  ...,  0.1088, -0.3639,  0.7795],\n",
       "         ...,\n",
       "         [ 0.0234,  0.4296,  0.1787,  ...,  0.2195,  0.0112,  0.0642],\n",
       "         [ 0.9951, -0.5820,  0.9646,  ..., -0.1429, -0.2179, -0.2663],\n",
       "         [ 1.2194, -1.3121,  0.2408,  ..., -0.6321, -0.3233,  0.8943]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9641,  0.9999,  0.9999,  ..., -1.0000,  0.5894,  0.9971]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "74f8717b-9aa9-42ad-9c14-290092ff25b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:33:34.102084Z",
     "iopub.status.busy": "2024-02-21T09:33:34.102084Z",
     "iopub.status.idle": "2024-02-21T09:33:34.105930Z",
     "shell.execute_reply": "2024-02-21T09:33:34.105930Z",
     "shell.execute_reply.started": "2024-02-21T09:33:34.102084Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, conversation_list=[]):\n",
    "        self.conversation_list = conversation_list\n",
    "        self.costs_list = []\n",
    "\n",
    "    def show_conversation(self, msg_list):\n",
    "        for msg in msg_list[-2:]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                pass\n",
    "            else:\n",
    "                message = msg[\"content\"]\n",
    "                pass\n",
    "                # print(f\"\\U0001f47D: {message}\\n\")\n",
    "\n",
    "    def ask(self, prompt):\n",
    "        self.conversation_list.append({\"role\": \"user\", \"content\": prompt})\n",
    "        openai.api_key = \"sk-LzwgVgu5xvNPpwoqCdeeVcAt7Tu7ZoZICXzzkheldIbXA60h\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-16k\", messages=self.conversation_list\n",
    "        )\n",
    "        answer = response.choices[0].message[\"content\"]\n",
    "\n",
    "        self.conversation_list.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        self.show_conversation(self.conversation_list)\n",
    "\n",
    "        # cost = total_counts(response)\n",
    "        # self.costs_list.append(cost)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3640d992-d4e0-4de4-8209-7ecabd55e5da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:36:08.229147Z",
     "iopub.status.busy": "2024-02-21T09:36:08.228147Z",
     "iopub.status.idle": "2024-02-21T09:36:08.231752Z",
     "shell.execute_reply": "2024-02-21T09:36:08.231752Z",
     "shell.execute_reply.started": "2024-02-21T09:36:08.229147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NER_prompt = f\"\"\"\n",
    "角色：\n",
    "你是一个深度学习领域的实体标注专员\n",
    "\n",
    "任务：\n",
    "给定字符串，请找出全部深度学习领域的实体\n",
    "\n",
    "步骤：\n",
    "请以以下步骤执行：\n",
    "1. 找出句子中的所有深度学习领域的实体\n",
    "2. 依次检查实体是否属于深度学习领域\n",
    "3. 将属于深度学习领域的实体返回\n",
    "4. 若没有深度学习领域的实体，则返回()\n",
    "\n",
    "格式：\n",
    "请以以下格式返回：\n",
    "('entity1', 'entity2', ...)\n",
    "\n",
    "举例如下：\n",
    "input: An illustration of how the gradient descent algorithm uses the derivatives of a function can be used to follow the function downhill to a minimum.\n",
    "output: (gradient descent algorithm)\n",
    "\n",
    "input: an encoder or reader or input RNN processes the input sequence. The encoder emits the context C, usually as a simple function of its final hidden state.\n",
    "output: (encoder, RNN, hidden state)\n",
    "\n",
    "input: There is no constraint that the encoder must have the same size of hidden layer as the decoder\n",
    "output: (hidden layer, decoder)\n",
    "\n",
    "input: Computer vision has traditionally been one of the most active research areas for deep learning applications, because vision is a task that is effortless for humans and many animals but challenging for computers (Ballard et al., 1983)\n",
    "output: (Computer vision, deep learning)\n",
    "\n",
    "input: Dataset augmentation may be seen as a way of preprocessing the training set only.\n",
    "output: (Dataset augmentation)\n",
    "\n",
    "input: CHAPTER 1. INTRODUCTION of the flowchart of the computations needed to compute the representation of each concept may be much deeper than the graph of the concepts themselves.\n",
    "output: ()\n",
    "\n",
    "注意事项：\n",
    "1. 请严格遵循字符串中的原本表述\n",
    "2. 除返回结果外，不要返回任何其他内容\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6277d2bc-5c2b-4aa1-a604-fb4b825ae6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T09:48:04.087938Z",
     "iopub.status.busy": "2024-02-21T09:48:04.087938Z",
     "iopub.status.idle": "2024-02-21T09:48:04.090724Z",
     "shell.execute_reply": "2024-02-21T09:48:04.090724Z",
     "shell.execute_reply.started": "2024-02-21T09:48:04.087938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_prompt = f\"\"\"\n",
    "任务：\n",
    "请检查所给实体是否属于深度学习领域\n",
    "\n",
    "格式：\n",
    "请以以下格式返回：\n",
    "如果该实体是深度学习领域的实体，返回True，否则返回False\n",
    "\n",
    "举例如下：\n",
    "input: deep learning\n",
    "output: True\n",
    "\n",
    "input: AI system\n",
    "output: True\n",
    "\n",
    "input: image\n",
    "output: False\n",
    "\n",
    "input: Image Net\n",
    "output: True\n",
    "\n",
    "input: face\n",
    "output: False\n",
    "\n",
    "注意事项：\n",
    "1. 请仅返回True或者False，不要返回任何其他内容\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "caabba2f-9476-43b1-8e7f-61c96d38080d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:45:57.615752Z",
     "iopub.status.busy": "2024-02-21T10:45:57.615752Z",
     "iopub.status.idle": "2024-02-21T10:49:32.176168Z",
     "shell.execute_reply": "2024-02-21T10:49:32.176168Z",
     "shell.execute_reply.started": "2024-02-21T10:45:57.615752Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 21\u001b[0m\n\u001b[0;32m     14\u001b[0m     conversation_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m         {\n\u001b[0;32m     16\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: NER_prompt,\n\u001b[0;32m     18\u001b[0m         }\n\u001b[0;32m     19\u001b[0m     ]\n\u001b[0;32m     20\u001b[0m     bot_ner \u001b[38;5;241m=\u001b[39m Chat(conversation_list)\n\u001b[1;32m---> 21\u001b[0m answer_ner \u001b[38;5;241m=\u001b[39m bot_ner\u001b[38;5;241m.\u001b[39mask(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mcontent)\n\u001b[0;32m     22\u001b[0m entity_list_temp \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer_ner)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m conversation_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     24\u001b[0m         {\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: check_prompt,\n\u001b[0;32m     27\u001b[0m         }\n\u001b[0;32m     28\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[106], line 18\u001b[0m, in \u001b[0;36mChat.ask\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt})\n\u001b[0;32m     17\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-LzwgVgu5xvNPpwoqCdeeVcAt7Tu7ZoZICXzzkheldIbXA60h\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-16k\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_list\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m answer \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer})\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\openai\\api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[0;32m    291\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    292\u001b[0m         supplied_headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    293\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    294\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    295\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[0;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\openai\\api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m _thread_context\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    597\u001b[0m         method,\n\u001b[0;32m    598\u001b[0m         abs_url,\n\u001b[0;32m    599\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    600\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    601\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    602\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    603\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mrequest_timeout \u001b[38;5;28;01mif\u001b[39;00m request_timeout \u001b[38;5;28;01melse\u001b[39;00m TIMEOUT_SECS,\n\u001b[0;32m    604\u001b[0m         proxies\u001b[38;5;241m=\u001b[39m_thread_context\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    716\u001b[0m     conn,\n\u001b[0;32m    717\u001b[0m     method,\n\u001b[0;32m    718\u001b[0m     url,\n\u001b[0;32m    719\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    720\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    721\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    722\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    723\u001b[0m )\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\urllib3\\connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mG:\\Crime\\Anaconda3\\envs\\pytorch\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for k, v in content_dict.items():\n",
    "    if k in ['4.2', '10.1', '15.1', '20.1']:\n",
    "        if total >= 1500:\n",
    "            break\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "        docs = text_splitter.split_text(' '.join(i for i in v))\n",
    "        # 将句子输入给大模型提取命名实体\n",
    "        for index, content in enumerate(docs):\n",
    "            total += 1\n",
    "            if total == 1500:\n",
    "                break\n",
    "            if index % 5 == 0:\n",
    "                conversation_list = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": NER_prompt,\n",
    "                    }\n",
    "                ]\n",
    "                bot_ner = Chat(conversation_list)\n",
    "            answer_ner = bot_ner.ask(\"input: \"+content)\n",
    "            entity_list_temp = re.sub(\"\\(|\\)|\", \"\", answer_ner).split(\", \")\n",
    "            conversation_list = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": check_prompt,\n",
    "                    }\n",
    "                ]\n",
    "            bot_check = Chat(conversation_list)\n",
    "            entity_list = []\n",
    "            for e in entity_list_temp:\n",
    "                answer_check = bot_check.ask(\"input: \"+e)\n",
    "                if answer_check == 'True':\n",
    "                    entity_list.append(e)\n",
    "            # 将句子进行tokenizer\n",
    "            content_token = tokenizer(content, return_tensors='pt')['input_ids']\n",
    "            # 将实体进行tokenizer\n",
    "            entity_token = []\n",
    "            for e in entity_list:\n",
    "                entity_token.append(tokenizer(e, return_tensors='pt')['input_ids'])\n",
    "            # 找到对应位置\n",
    "            label = [0]*content_token.shape[1]\n",
    "            for e in entity_token:\n",
    "                e_temp = []\n",
    "                for i in range(1, e.shape[1]-1):\n",
    "                    e_temp.append(e[0,i])\n",
    "                temp = e_temp.copy()\n",
    "                i, j = 0, 0\n",
    "                while j < content_token.shape[1]:\n",
    "                    if content_token[0,j]!=temp[0]:\n",
    "                        temp = e_temp.copy()\n",
    "                        j += 1\n",
    "                        i = j\n",
    "                    else:\n",
    "                        temp.pop(0)\n",
    "                        j += 1\n",
    "                    if not temp:\n",
    "                        temp = e_temp.copy()\n",
    "                        while i < j:\n",
    "                            label[i] = 1\n",
    "                            i += 1\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                    [[tokenizer.batch_decode(content_token), label]],\n",
    "                    columns=[\"text\", \"label\"],\n",
    "                )\n",
    "            df.to_csv(\n",
    "                os.path.join(dataDir + \"/relations\", f\"sample.csv\"),\n",
    "                mode=\"a\",\n",
    "                header=not os.path.exists(\n",
    "                    os.path.join(dataDir + \"/relations\", f\"sample.csv\")\n",
    "                ),\n",
    "                index=False,\n",
    "            )\n",
    "            # 返回原句和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "35ee6242-c675-42e5-b346-241bbbc10db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:30:24.565791Z",
     "iopub.status.busy": "2024-02-21T10:30:24.565791Z",
     "iopub.status.idle": "2024-02-21T10:30:24.570139Z",
     "shell.execute_reply": "2024-02-21T10:30:24.570139Z",
     "shell.execute_reply.started": "2024-02-21T10:30:24.565791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    dataDir + \"relations/\" + \"sample.csv\",\n",
    "    index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "01857fb5-64c0-425e-b602-31d11184f9e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:31:45.984990Z",
     "iopub.status.busy": "2024-02-21T10:31:45.984990Z",
     "iopub.status.idle": "2024-02-21T10:31:45.988582Z",
     "shell.execute_reply": "2024-02-21T10:31:45.988582Z",
     "shell.execute_reply.started": "2024-02-21T10:31:45.984990Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['[CLS] CHAPTER 1. INTRODUCTION of the flowchart of the computations needed to compute the representation of each concept may be much deeper than the graph of the concepts themselves. This is because the system ’ s understanding of the simpler concepts can be refined given information about the more complex concepts. For example, an AI system observing an image of a face with one eye in shadow may initially only see one eye. After detecting that a face is present, it can then infer that a second eye is [SEP]']\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5d46b509-43a3-4261-963b-aff9281b210a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T10:30:50.548052Z",
     "iopub.status.busy": "2024-02-21T10:30:50.548052Z",
     "iopub.status.idle": "2024-02-21T10:30:50.550969Z",
     "shell.execute_reply": "2024-02-21T10:30:50.550969Z",
     "shell.execute_reply.started": "2024-02-21T10:30:50.548052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b141b7-2df3-4ec4-8e04-3d7380eb4dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

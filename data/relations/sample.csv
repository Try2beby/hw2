text,label
['[CLS] Chapter 4 Numerical Computation Machine learning algorithms usually require a high amount of [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] a high amount of numerical compu - tation. This typically refers to algorithms that solve [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] that solve mathematical problems by methods that update estimates of the solution via an iterative [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] via an iterative process, rather than analytically deriving a formula providing a symbolic [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] a symbolic expression for the correct so - lution. Common operations include optimization ( finding [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] ( finding the value of an argument that minimizes or maximizes a function ) and solving systems of [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] solving systems of linear equations. Even just evaluating a mathematical function on a digital [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] CHAPTER 4. NUMERICAL COMPUTATION stabilized. Theano ( Bergstra et al., 2010 ; Bastien et al., 2012 ) is [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] et al., 2012 ) is an example of a software package that automatically detects and stabilizes many [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] and stabilizes many common numerically unstable expressions that arise in the context of deep [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] the context of deep learning. 4. 2 Poor Conditioning Conditioning refers to how rapidly a function [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] rapidly a function changes with respect to small changes in its inputs. Functions that [SEP]'],"[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] Functions that changerapidly when their inputs are perturbed slightly can be problematic for [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] be problematic for scientific computation because rounding errors in the inputs can result in large [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] can result in large changes in the output. Consider the function f ( x ) = A 1x. When A Rn n has an [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] When A Rn n has an eigenvalue − × ∈ decomposition, its condition number is λ i max. ( 4. 2 ) i, j λ j [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] max. ( 4. 2 ) i, j λ j This is the ratio of the magnitude of the largest and smallest [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] and smallest eigenvalue. When this number is large, matrix inversion is particularly sensitive [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] sensitive to error in the input. This sensitivity is an intrinsic property of the matrix itself, [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] the matrix itself, not the result of rounding error during matrix inversion. Poorly conditioned [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] Poorly conditioned matrices amplify pre - existing errors when we multiply by the true matrix [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] by the true matrix inverse. In practice, the error will be compounded further by numerical errors [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] by numerical errors in the inversion process itself. 4. 3 Gradient - Based Optimization Most deep [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] to the task of either minimizing or maximizing some function f ( x ) by altering x. We usually phrase [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] We usually phrase most optimization problems in terms of minimizing f ( x ). Maximization may be [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] Maximization may be accomplished via a minimization algorithm by minimizing f ( x ). − The function we [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] − The function we want to minimize or maximize is called the objective func - tion or criterion. [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] tion or criterion. When we are minimizing it, we may also call it the cost function, loss function, [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]"
"['[CLS] loss function, or error function. In this book, we use these terms interchangeably, though some [SEP]']","[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] though some machine learning publications assign special meaning to some of these terms. We often [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] terms. We often denote the value that minimizes or maximizes a function with a superscript. For [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] a superscript. For example, we might say x = argminf ( x ). ∗ ∗ 82 [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] CHAPTER 10. SEQUENCE MODELING : RECURRENT AND RECURSIVE NETS 10. 1 Unfolding Computational Graphs A [SEP]'],"[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]"
"['[CLS] Graphs A computational graph is a way to formalize the structure of a set of computations, such as [SEP]']","[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] such as those involved in mapping inputs and parameters to outputs and loss. Please refer to [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]"
['[CLS] Please refer to section 6. 5. 1 for a general introduction. In this section we explain the idea of [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] explain the idea of unfolding a recursive or recurrent computation into a computational graph that [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0]"
"['[CLS] graph that has a repetitive structure, typically corresponding to a chain of events. Unfolding this [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] Unfolding this graph results in the sharing of parameters across a deep network structure. For [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]"
"['[CLS] structure. For example, consider the classical form of a dynamical system : s ( t ) = f ( s ( t 1 ) ; θ ), [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] s ( t ) = f ( s ( t 1 ) ; θ ), ( 10. 1 ) − where s ( t ) is called the state of the system. Equation 10. 1 is [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] Equation 10. 1 is recurrent because the definition of s at time t refers back to the same definition [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] the same definition at time t 1. − For a finite number of time steps τ, the graph can be unfolded [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] can be unfolded by applying the definition τ 1 times. For example, if we unfold equation 10. 1 for τ [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] equation 10. 1 for τ = 3 time − steps, we obtain s ( 3 ) = f ( s ( 2 ) ; θ ) ( 10. 2 ) = f ( f ( s ( 1 ) ; θ ) ; θ ) ( 10. 3 ) [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] ; θ ) ; θ ) ( 10. 3 ) Unfolding the equation by repeatedly applying the definition in this way has yielded [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] way has yielded an expression that does not involve recurrence. Such an expression can now be [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] can now be represented by a traditional directed acyclic computational graph. The unfolded [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] graph. The unfolded computational graph of equation 10. 1 and equation 10. 3 is illustrated in figure [SEP]'],"[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] in figure 10. 1. ss ( (...... ) ) ss ( ( tt 11 ) ) ss ( ( tt ) ) ss ( ( tt + + 11 ) ) ss ( (...... ) ) −− ff ff ff ff Figure [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] ff ff ff ff Figure 10. 1 : The classical dynamical system described by equation 10. 1, illustrated as [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] illustrated as an unfolded computational graph. Each node represents the state at some time t and [SEP]'],"[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] at some time t and the function f maps the state at t to the state at t + 1. The same parameters ( the [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] parameters ( the same value of θ used to parametrize f ) are used for all time steps. As another [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] steps. As another example, let us consider a dynamical system driven by an external signal x ( t ), [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] signal x ( t ), s ( t ) = f ( s ( t 1 ), x ( t ) ; θ ), ( 10. 4 ) − 375 CHAPTER 10. SEQUENCE MODELING : RECURRENT AND [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] RECURRENT AND RECURSIVE NETS where we seethat the state now containsinformation about the whole [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] about the whole pastsequence. Recurrent neural networks can be built in many different ways. Much [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] ways. Much as almost any function can be considered a feedforward neural network, essentially any [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
['[CLS] essentially any function involving recurrence can be considered a recurrent neural network. Many [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0]"
['[CLS] network. Many recurrent neural networks use equation 10. 5 or a similar equation to define the [SEP]'],"[0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] to define the values of their hidden units. To indicate that the state is the hidden units of the [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] hidden units of the network, we now rewrite equation 10. 4 using the variable h to represent the [SEP]']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] h to represent the state : h ( t ) = f ( h ( t 1 ), x ( t ) ; θ ), ( 10. 5 ) − illustrated in figure 10. 2, typical [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] 10. 2, typical RNNs will add extra architectural features such as output layers that read [SEP]']","[0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] layers that read information out of the state h to make predictions. When the recurrentnetwork is [SEP]'],"[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] recurrentnetwork is trainedto perform a task that requirespredicting the future from the past, the [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] from the past, the network typically learns to use h ( t ) as a kind of lossy summary of the [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] summary of the task - relevant aspects of the past sequence of inputs up to t. This summary is in [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] This summary is in general necessarily lossy, since it maps an arbitrary length sequence ( x ( t ), x ( t [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] sequence ( x ( t ), x ( t 1 ), x ( t 2 ),..., x ( 2 ), x ( 1 ) ) to a fixed length vector h ( t ). Depending on the − − [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] on the − − training criterion, this summary might selectively keep some aspects of the past [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] aspects of the past sequence with more precision than other aspects. For example, if the RNN is [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] if the RNN is used in statistical language modeling, typically to predict the next word given [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] the next word given previous words, it may not be necessary to store all of the information in the [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] information in the input sequence up to time t, but rather only enough information to predict the [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] to predict the rest of the sentence. The most demanding situation is when we ask h ( t ) to be rich [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] ask h ( t ) to be rich enough to allow one to approximately recover the input sequence, as in [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] sequence, as in autoencoder frameworks ( chapter 14 ). hh hh ( (...... ) ) hh ( ( tt 11 ) ) hh ( ( tt ) ) [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] 11 ) ) hh ( ( tt ) ) hh ( ( tt + + 11 ) ) hh ( (...... ) ) −− ff ff ff f ff Unfold xx xx ( ( tt 11 ) ) xx ( ( tt ) ) [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] 11 ) ) xx ( ( tt ) ) xx ( ( tt + + 11 ) ) −− Figure 10. 2 : A recurrent network with no outputs. This recurrent [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] This recurrent network just processes information from the input x by incorporating it into the [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] it into the state h that is passed forward through time. ( Left ) Circuit diagram. The black square [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] The black square indicates a delay of a single time step. ( Right ) The same network seen as an [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
"['[CLS] network seen as an unfolded computational graph, where each node is now associated with one [SEP]']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
['[CLS] associated with one particular time instance. Equation 10. 5 can be drawn in two different ways. One [SEP]'],"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"

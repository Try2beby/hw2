[
    "atrophy",
    "back-propagation algorithm",
    "7082",
    "recurrent encoder and recurrent decoder",
    "IEEE Transactions on Audio",
    "echo state network",
    "n-gram model",
    "Illustration",
    "positive definite",
    "encoding schemes",
    "phylogenetic tree",
    "linear factor model",
    "INFERNO",
    "Gaussian-Bernoulli RBM energy function",
    "deep feedforward",
    "turbulent drag",
    "antipathy",
    "ImageNet",
    "runtime",
    "Context-specific independences",
    "architectures",
    "model averaging",
    "gated recurrent net",
    "optimization problem",
    "thetangent prop algorithm",
    "equilibria",
    "normalized score matching",
    "polyak averaging",
    "amplifying",
    "COVID-19 pandemic 2020",
    "Inference as Optimization",
    "\u03b13",
    "2.4 Stochastic Encoders and Decoders",
    "intertwining",
    "computational biology",
    "representation learning",
    "Hessian matrix",
    "Deep Blue",
    "sigmoid",
    "France",
    "RMSProp",
    "uncertainty",
    "local kernels",
    "Neal (2001)",
    "gated",
    "supervised learning",
    "feature space",
    "DEEP FEEDFORWARD NETWORKS",
    "2009",
    "parametrized",
    "X",
    "temperatures",
    "Deep Networks",
    "2.3 Softmax Units for Multinoulli Output Distributions",
    "sparse autoencoder",
    "Macron",
    "face",
    "fully connected layer",
    "natural language",
    "Markov chain",
    "RNNs",
    "eye",
    "neural network",
    "gradient norm",
    "TOPS100",
    "overflow",
    "matrix inversion",
    "machinelearning",
    "overfit",
    "RECursive neural network",
    "convolutions",
    "Recursive network",
    "sparse initialization",
    "NIPS",
    "non-distributed algorithm",
    "commutative",
    "autoencoder network",
    "minimally significant event",
    "learning algorithm",
    "test set",
    "Levenberg\u2013Marquardt algorithm",
    "data set",
    "Gaussian distribution",
    "7 Contractive Autoencoders",
    "brain area",
    "FitNets",
    "INTRODUCTION",
    "linear network",
    "unnormalized",
    "2nd layer",
    "editing",
    "supervised neural net",
    "2-D convolution",
    "uniform distribution",
    "enhanced gradient",
    "extended state space",
    "3771",
    "InProceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2011).",
    "fully-visible Bayes network",
    "bidirectional recurrent neural network",
    "8.5.1 AdaGrad",
    "sparse representation",
    "algorithm 18.1",
    "Neural auto-regressive network",
    "mean-product of t-distribution",
    "gradient ascent",
    "linear models",
    "GENERATING THE PARTITION",
    "greedy supervised pretraining",
    "6.2 Sigmoid Units for Bernoulli Output Distributions",
    "Google Street View",
    "education",
    "gradient descent",
    "minimizes",
    "software engineering",
    "linear model",
    "underfitting",
    "6",
    "neural networks",
    "sexual reproduction",
    "asymmetric",
    "2006",
    "subjugation",
    "visualization",
    "\u03b1",
    "sparsely connected generalizations",
    "streamlining",
    "structured probabilistic model",
    "Leth+",
    "\u03c32/m",
    "ensemble",
    "weak prior",
    "variational",
    "8.5",
    "brain",
    "independence",
    "GoogLeNet",
    "information retrieval",
    "AI disciplines",
    "University of Toronto",
    "rodent",
    "Bidirectional recurrent neural network",
    "GSNs",
    "Softmax function",
    "FIRST resampled dataset",
    "WordNet",
    "LSTM cell",
    "distributive",
    "minimally estimate the gradient",
    "unstructured noise",
    "AI deep learning",
    "algorithmically",
    "minimally significant",
    "Gradient-based optimization",
    "neural net",
    "Fully-visible Bayes network",
    "Cambridge, MA",
    "models",
    "NADE architecture",
    "optimistically",
    "stochastic gradient algorithm",
    "memory cell",
    "\u039b m",
    "8.7",
    "intertwine",
    "cumulative distribution",
    "Introduction",
    "Convolution stage",
    "Restricted Boltzmann Machines",
    "optimizes",
    "maximally large",
    "deep circuits",
    "Hidden layer",
    "6.5 Symbol-to-Symbol Derivatives",
    "Part II",
    "Natural language processing",
    "second derivative",
    "auto-regressive neural network",
    "Polyak averaging",
    "paramount components analysis",
    "Probability theory",
    "intelligence",
    "RECurrent Neural Networks",
    "distributive property",
    "summation",
    "\u03b2",
    "computational graph language",
    "Gradient-based learning",
    "L2",
    "decoder",
    "autoencoder networks",
    "TOPS",
    "optimising",
    "information",
    "normality",
    "integrate",
    "Bayesian",
    "minimally supervised",
    "locally linear Gaussian",
    "cognitive scientists",
    "AIS procedure",
    "deep linear network",
    "8.6.3 Adam",
    "complexity",
    "learning algorithms",
    "CONVOLUTIONAL NETWORKS",
    "ancestral sampling",
    "Advances in Neural Information Processing Systems 16 (NIPS\u201903)",
    "women",
    "neurotransmitter",
    "total objective function",
    "multi-channel convolution",
    "Deepbelief network",
    "differentiable transformation",
    "previous",
    "neuralnetworks",
    "genealogy",
    "memory",
    "approximation",
    "software framework",
    "neural-net",
    "Explicit Memory",
    "utilising",
    "Bagging predictors",
    "recurrent decoder",
    "REM sleep",
    "implicit ensemble method",
    "optimization strategy",
    "GPU",
    "one-hot code",
    "strong prior",
    "McGill University",
    "32nd International Conference on MachineFactoryReloaded",
    "7.1",
    "University of Montreal",
    "CIFAR-10",
    "deep neural networks",
    "recurrent neural networks",
    "Street View cars",
    "\u03c6",
    "2 Challenges in Neural Network Optimization",
    "log-likelihood",
    "separating",
    "time period",
    "deterministic operation",
    "natural object classes",
    "Monte Carlo sampling",
    "numerical optimization",
    "algorithm 8.5",
    "MIT",
    "sparse",
    "Street View",
    "lower layers",
    "rounding error",
    "biology",
    "thetangent prop",
    "stochastic back-propagation",
    "Ensemble",
    "Tangent propagation",
    "equation 6.53",
    "Deep Learning",
    "complex-valued",
    "norm",
    "Teacher forcing",
    "multiple dimension",
    "mythical figures",
    "manifold learning",
    "hyperparameter",
    "DEEP FEEDFORWARD",
    "optimization algorithms",
    "measure theory",
    "Neural network",
    "DeepMind",
    "cognitive science",
    "Generative moment matching network",
    "probability",
    "tensor",
    "iteration",
    "Jarzynski",
    "estimator",
    "sum-product network",
    "parametric method",
    "Bias",
    "dialectic",
    "Regularization",
    "transpose operator",
    "massively parallel network",
    "Probabilistic modeling of natural images",
    "Gradient",
    "egregiousious",
    "7.1 The early stopping meta-algorithm for determining the best TAMADRAamount of time to train",
    "explicit constraints",
    "BFGS",
    "Springer",
    "egregious",
    "t-SNE",
    "tr(A)",
    "Microsoft",
    "retesting",
    "Convolutional Layer",
    "n-gram language model",
    "computer vision",
    "2010",
    "iteratively",
    "training algorithm",
    "minimally",
    "regularization",
    "classifier",
    "input space",
    "implicit ensemble",
    "minimization algorithm",
    "embodying",
    "Bayesian network",
    "IPv9",
    "recurrent networks",
    "Gaussian case",
    "dependence",
    "Bernoulli distribution",
    "Neural Networks",
    "Jacobian",
    "machine learning algorithm",
    "distribution",
    "tangent plane",
    "NETS",
    "nearly deterministic",
    "deep neural network",
    "functional form",
    "ASICs",
    "Gated RNN",
    "penalty",
    "Student-t prior",
    "programming",
    "rectified linear unit",
    "AIS estimator",
    "Germany",
    "model architecture",
    "log\u03b1",
    "deep rectifier net",
    "minimibus",
    "stochas",
    "feedforward network",
    "deep Boltzmann machine",
    "suppressing",
    "D\u00e9partement d\u2019Informatique et de Recherche Op\u00e9ra-\ufffdtionnelle",
    "asymptotically consistent",
    "CONFIGURATION",
    "abnormal",
    "gradient-based approximate minimization",
    "normalize",
    "Laplacian pyramid",
    "probabilistic models",
    "algorithmic",
    "\u03c3",
    "supervised paradigm",
    "a",
    "McCulloch-Pitts Neuron",
    "training technique",
    "hill climbing",
    "information theory",
    "two-dimensional",
    "8.5.2 RMSProp",
    "minimally invasive",
    "generative model",
    "stochastic maximum likelihood",
    "7083",
    "Principality Analysis",
    "Bengio",
    "Fully parametric generative model",
    "computer scientist",
    "root mean square",
    "Convex optimization",
    "Anthropometric",
    "rare disease",
    "learning mechanism",
    "Torsten Wiesel",
    "2nd International Conference on MachineFactoryReloaded",
    "enslaved",
    "clustering algorithm",
    "CONVOLUTIONAL NETWORK",
    "encoder-decoder",
    "Second resampled dataset",
    "L2 weight decay",
    "SETTMENTS",
    "conclusively",
    "Convolution with a stride",
    "7.2 A meta-algorithm for using early stopping",
    "Bayesian probability",
    "gradientdescent",
    "Gaussian probability density function",
    "zero-shot learning",
    "GSN",
    "deep linear networks",
    "Most birds fly",
    "mathematical",
    "Exact inference",
    "deterministic",
    "9.9.1",
    "deep network",
    "maximally",
    "RNN",
    "Deeplearning",
    "recurrent net",
    "sparsely populated",
    "simulating",
    "inference algorithm",
    "mPoT",
    "converge",
    "MNIST",
    "Deep networks for speech recognition",
    "Pylearn2",
    "Jacobian matrix",
    "SLOW feature analysis",
    "IEEE Transactions on Automatic Control",
    "Advances in Neural Information Processing Systems 16",
    "Neural Machine Translation",
    "sampled",
    "data point",
    "RECURRENT",
    "0-1 loss",
    "multiple dimensions",
    "2015",
    "first hidden layer",
    "span",
    "algorithm 8.9",
    "minimax",
    "recognizing",
    "squared error",
    "GPUs",
    "auto-regressive network",
    "canyon",
    "CPU",
    "massively parallel",
    "adequately",
    "loss",
    "non-parametric model",
    "constrained maximization",
    "vertically",
    "2.3 The Partition Function",
    "IEEE Transactions on Circuits and Systems",
    "Hinton et al., 2006",
    "Torch",
    "unsupervised feature learning",
    "strided convolution",
    "MAP inference",
    "metadata",
    "Deep learning",
    "Heidelberg",
    "statistics",
    "2.4 Linear Dependence and Span",
    "contrastive divergence",
    "tiled convolutional",
    "greedy",
    "Amazon",
    "Geoffrey Hinton",
    "temperature",
    "nearest neighbor algorithm",
    "congeniality",
    "IPv6",
    "Input noise injection",
    "k-nearest TAMADRAneighbors algorithm",
    "flow chart",
    "asymptotically unbiased",
    "maximum likelihood",
    "graph structure",
    "neoclassical",
    "minimally linear",
    "tiling pattern",
    "converge criterion",
    "Statistical learning theory",
    "algorithm 8.8",
    "minimizing this KL divergence",
    "L2 norm",
    "FIRST derivative",
    "Generative neural network",
    "statistical challenge",
    "hidden layer",
    "minimally stable variant",
    "IEEE",
    "g(c)",
    "nonlinear projection",
    "\u201csquashing\u201d activation function",
    "Scalars, Vectors, Matrices and Tensors",
    "Recurrent neural network",
    "minimization",
    "optimize",
    "declarative",
    "computer science",
    "perfect optimization",
    "cross-entropy loss",
    "deep networks",
    "explicit",
    "Sejnowski",
    "manifold",
    "International Conference on Learning Representations",
    "curvature",
    "equivariance",
    "noise distribution",
    "data structures",
    "LSTM",
    "2 Bridge Sampling",
    "consumer electronics",
    "tiled",
    "unsupervised pretraining",
    "parsing",
    "PR2",
    "encoding",
    "convolutional",
    "P(y C)",
    "y",
    "first layer",
    "Deep Boltzmann machine",
    "single layer models",
    "pathetically",
    "suppressing variance",
    "Dirational graph",
    "back-propagation through time",
    "libraries",
    "neural auto-regressive network",
    "allele",
    "factored",
    "p",
    "global minimum",
    "propagate",
    "one-dimensional",
    "deep circuit",
    "IPv8",
    "supervisedlearning",
    "bias",
    "The Statistician",
    "convolutional layer",
    "bipartite graph",
    "horrendously expensive",
    "error bars",
    "approximating",
    "mean and TAMADRAcovariance RBM",
    "BREATHING",
    "David Hubel",
    "NVIDIA",
    "retina",
    "supervised training",
    "GENERATIVE STochastic Networks",
    "linear algebra",
    "RECirculation",
    "probability theory",
    "linguistics",
    "Ensipio graph",
    "Part III",
    "QMULMultiviewFaceDataset",
    "AI",
    "GRU",
    "undirected",
    "3-D",
    "max-pooling",
    "usurpation",
    "paramountously",
    "Pattern Analysis and Machine Intelligence",
    "GRU recurrent net",
    "parametric",
    "gradient-based optimization",
    "sentiment analysis",
    "unsupervised learning",
    "learning fixmealgorithm",
    "Gaussian factor",
    "IPv7",
    "minimally intrusive",
    "deep belief",
    "graphs",
    "factor analysis",
    "fully connected layers",
    "horizontally",
    "cost function",
    "optimization",
    "minimally distributed networks",
    "feedforward networks",
    "restricted Boltzmann machine",
    "PARzen window algorithm",
    "Convolutional Boltzmann Machines",
    "encoder",
    "2 Continuous Variables and Probability Density Functions",
    "recognizing speech",
    "L1 norm",
    "Nesterov momentum",
    "sparsely parametrized",
    "Monte Carlo algorithm",
    "0.2",
    "sentiment",
    "9 Leaky Units and Other Strategies for Multiple=-=-=-=-=-=-=-=-=-=-=-=-",
    "minimization problem",
    "Uniform distribution",
    "layer",
    "3772",
    "RBM",
    "pandemic",
    "normalized",
    "Deep SFA",
    "contrast",
    "generative learning",
    "9.4 Convolution and Pooling as an Infinitely StrongBuyablePrior",
    "convolutional neural network",
    "XOR function",
    "Montreal",
    "nervous system",
    "cells",
    "overfitting",
    "unnormalized probability distribution",
    "9.1 The Convolution Operation",
    "Batch normalization",
    "10",
    "Freebase",
    "Neal and Hinton",
    "sparse initialization scheme",
    "generativelearning",
    "Stochastic gradient ascent",
    "neural language model",
    "2003",
    "eigenvectors",
    "knowledge base",
    "Yoshua Bengio",
    "least-squares",
    "implicit",
    "tangent line",
    "maxout network",
    "minimally supervised learning",
    "Support vector machine",
    "maximize",
    "Stochastic gradient descent",
    "maximization in the outer loop",
    "encoder function",
    "k-nearest TAMADRAneighbors",
    "Deep Learning Research",
    "mean squared error",
    "convolutional networks",
    "self-loop",
    "proba-=-=-=-=-=-=-=-=-=-=-=-=-",
    "GeneOntology",
    "human brain",
    "2011",
    "negative definite",
    "2-D",
    "deep convolutional network",
    "k-nearest neighbors algorithm",
    "Monte Carlo method",
    "BEcker and Hinton",
    "supervised model",
    "DropConnect",
    "Expected Value",
    "transfer learning",
    "levelling",
    "faces",
    "Hebbian learning rule",
    "Deep feedforward network",
    "single layer model",
    "curvaceous",
    "parametric analysis",
    "logit",
    "unbiased Monte Carlo estimator",
    "inferotemporal cortex",
    "Noise injection",
    "Linear algebra",
    "A directed graphical model",
    "model",
    "log-uniform",
    "variance",
    "Deep convolutional network",
    "algorithm 8",
    "CPUs",
    "COVID-19 pandemic",
    "function",
    "computational resources",
    "artificial neural net",
    "generative modeling",
    "Regularized autoencoders",
    "probabilistic model",
    "Ensemble of subnetworks",
    "Ice Age",
    "norm penalty",
    "2.3 Identity and Inverse Matrices",
    "The Restricted Boltzmann Machine",
    "2nd-order optimization",
    "SETTLEMENT",
    "probability distribution",
    "2 Other Gated RNNs",
    "DBNs",
    "trained",
    "BPTT",
    "factorial distribution",
    "convolutional net",
    "recurrent network",
    "convolutional network layer",
    "algorithm 8.5.1",
    "undirected graph",
    "7 Boltzmann Machines for Structured or SequentialBuyableOutputs",
    "schools",
    "singular vector",
    "embeddings",
    "supine",
    "hyperparameter optimization",
    "Wh",
    "graph language",
    "International Conference on Artificial Intelligence and Statistics",
    "Spearmint",
    "noisy",
    "valley",
    "annealed importance sampling",
    "deep Boltzmann machine training",
    "unsupervised representation learning",
    "ISTA",
    "Vapnik-Chervonenkis dimension",
    "Euler\u2019s method",
    "Bernoulli",
    "complexity theory",
    "subgraph",
    "L1",
    "neuroscience",
    "Python",
    "data",
    "Las Vegas algorithm",
    "8.6",
    "autoimmune",
    "contrastive divergence algorithm",
    "Recurrent neural networks",
    "fully connected Boltzmann machine",
    "paths",
    "saddle-like",
    "ReLU",
    "Expectation maximization",
    "z",
    "Memory networks",
    "Generative component",
    "convolutional structure",
    "algorithm 8.2",
    "minimally-controlled",
    "SETTLEMENTS",
    "tree",
    "linear operator",
    "Royal Society of New South Wales",
    "computational graph",
    "approximate inference",
    "GENERATING THE PARTITION FUNCTION",
    "learning",
    "REINFORCE TAMADRAalgorithm",
    "decoder RNN",
    "deep feedforward networks",
    "7.2",
    "linear",
    "German",
    "International Conference on Machine learning",
    "data model",
    "natural language processing",
    "deep models",
    "conditional distribution",
    "Metropolis-Hastings algorithm",
    "minimally modified",
    "algorithm 8.5 The RMSProp algorithm",
    "graphical model",
    "minimally sampled",
    "ACM",
    "underfit",
    "eigenvalue",
    "GRUs",
    "derivative",
    "probability mass function",
    "minimally distributed",
    "parametric model",
    "eigenvalue spectrum",
    "datasets augmentation",
    "2\u03c3(2z)",
    "2 Sigmoid Units for Bernoulli Output Distributions",
    "computational",
    "arithmetic mean",
    "L2 parameter norm penalty",
    "neuron",
    "build_grad",
    "prior probability distribution",
    "iteratively descending conjugate directions",
    "pretraining algorithm",
    "Caffe",
    "simulation",
    "computational cost",
    "Metropolis-Hastings",
    "Springer New York",
    "0",
    "indices",
    "maximum likelihood estimation",
    "DBN",
    "IBM",
    "speech recognition",
    "Japan",
    "sources of error",
    "sparse coding",
    "datasets",
    "Advances in Neural Information Processing Systems",
    "variational inference",
    "Gradient-based",
    "cRBM",
    "numerical",
    "ICLR",
    "gated RNN",
    "Ensipio diagram",
    "DBM",
    "logistic regression",
    "Undirected Models",
    "network",
    "pandemic pandemic",
    "Convolutional network",
    "Neural Turing machine",
    "recurrent neural network",
    "Gaussian mixture",
    "normalizing constant",
    "regions",
    "retaining",
    "Parametric model",
    "2020",
    "2.4 Modeling Sequences Conditioned on Context with RNNs",
    "Zero-data learning",
    "3rd layer",
    "branches",
    "maxout",
    "family",
    "orthogonal",
    "deep learning",
    "language",
    "Neural language model",
    "Gradient estimator",
    "unnormalized probability",
    "equilibrium",
    "Atkins Medal",
    "minimally-valued",
    "Machine\u80fd",
    "optimal point",
    "feedforward neural networks",
    "Galatea",
    "tree structure",
    "Autoencoder network",
    "Applied Math",
    "neural Turing machine",
    "multi-channel",
    "one-shot learning",
    "stochastic autoencoder",
    "dialect",
    "\u03c6(x)",
    "Distributed asynchronous gradient descent",
    "summarily",
    "InProceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics",
    "ranking loss",
    "data distribution",
    "L2 penalty",
    "P(i C)",
    "Miocene",
    "estimation error",
    "RECurrent neural network",
    "language model",
    "computer program",
    "deep, generative model",
    "CONFIGURAL NETWORKS",
    "2-D space",
    "variational autoencoder",
    "1",
    "Generator network",
    "one-hot code vector",
    "televisions",
    "A directed graphical model depicting the relay race",
    "graph theory",
    "graph",
    "linear function",
    "denoising autoencoder",
    "physical energy",
    "Convolutional networks",
    "linear classifiers",
    "minimum",
    "7 Factor Graphs",
    "transformed space",
    "\u03bb",
    "\u03b1peperous",
    "Convolutional Networks",
    "stochastic gradient descent",
    "bipartite structure",
    "algorithm 8.9 The conjugate gradient method",
    "cross-entropy",
    "iterative optimization",
    "transform",
    "word class",
    "7",
    "software library",
    "hidden Markov model",
    "Zero-shot learning",
    "decomposition",
    "minimizes generalization error",
    "memory consumption",
    "exponential",
    "dialectical",
    "classifiers",
    "BFGS algorithm",
    "integrals",
    "knowledge",
    "probability density function",
    "mean absolute error",
    "suppressively",
    "storylines",
    "2 Independent Component Analysis",
    "variational stochastic maximum likelihood algorithm",
    "BPTT algorithm",
    "undirected probabilistic",
    "International Conference on Computer Vision",
    "supervised learning algorithm",
    "minimally estimated",
    "normal distribution",
    "first-order optimization algorithm",
    "learning representation",
    "squared errors",
    "GMM-HMM",
    "Batch Normalization",
    "Boltzmann machine",
    "trace operator",
    "inference",
    "regression",
    "generative adversarial",
    "ISTA technique",
    "root mean square reconstruction",
    "minimally distributed distribution",
    "linguistic",
    "ancestral",
    "Feedforward",
    "V1 cortex",
    "OpenCyc",
    "neoclassifier",
    "Stochastic gradient descent (SGD)",
    "6.3",
    "data points",
    "slavery",
    "principal components analysis",
    "parametrization",
    "coverage",
    "educators",
    "LISA",
    "convolutional network",
    "stochastic gradient-based optimization",
    "Machine\u80fdLearning",
    "minimising",
    "SMAC",
    "obscured",
    "three-dimensional",
    "eigenvector",
    "horrendously",
    "amplifying amplifiers",
    "multidimensional array",
    "inference problem",
    "ketamine",
    "Interspeech 2010",
    "noise contrastive estimation",
    "Manifold learning",
    "Convolution",
    "2000",
    "CONFIGAL NETWORKS",
    "Gibbs",
    "greedy layer-wise pre-Buyabletraining",
    "P(x,y = y)",
    "Restricted Boltzmann Machine",
    "0.3",
    "test",
    "deep belief networks",
    "normalization",
    "fully connected",
    "6.1",
    "Parametric",
    "maximum",
    "hierarchy",
    "MIT Press",
    "\u03b1 coefficient",
    "supervised tasks",
    "softmax",
    "\u03b1x",
    "Google Maps",
    "matrix multiplication",
    "functional",
    "neural autoregressive density estimator",
    "algorithm 20",
    "curse of dimensionality",
    "a priori",
    "au-\u88fdtoencoder",
    "Autoencoder",
    "gater",
    "non-diagonal",
    "parametrize",
    "FPCD",
    "tiled convolution",
    "minimally invasive methods",
    "data used for the entire training process",
    "GENERATIVE MODELS",
    "locally constant",
    "\u03b11",
    "GET_operation",
    "maxout unit",
    "subjugated",
    "gradient",
    "Machine Learning",
    "machine translation",
    "early stopping",
    "decision trees",
    "recurrent encoder",
    "constrained optimization",
    "sparse connectivity",
    "'[h v",
    "covariance",
    "supervised network",
    "REINFORCE algorithm",
    "Gradient descent rule",
    "non-parametric learning algorithm",
    "Machine learning",
    "clique",
    "supervised",
    "8.1 Stochastic Gradient Descent",
    "computation graph",
    "curvature information",
    "gradient clipping",
    "DRAW model",
    "writer",
    "Gradient-Based Learning",
    "minimally large",
    "sympathetic",
    "image",
    "computation",
    "computational step",
    "classify",
    "eigenvalue decomposition",
    "Transfer learning",
    "LAPGAN",
    "k-nearest neighbors",
    "minimizing",
    "sparse activity regularization",
    "\u03b12",
    "Traditional convolution",
    "rectified linear transformation",
    "stochastic operations",
    "unsupervised network",
    "branching factor",
    "legendary figure",
    "P(h v)",
    "LASSO",
    "underflow",
    "\u03b1w",
    "m m matrix",
    "vector",
    "Pandora",
    "neural language",
    "encoder RNN",
    "maxima",
    "\u03c0",
    "Randomized algorithm",
    "conversely",
    "back-off",
    "loss function",
    "h",
    "operable",
    "INFERENCE",
    "unrecognized",
    "Journal of the ACM",
    "hidden unit",
    "MLP",
    "stochastic maximum likelihood algorithm",
    "Canada",
    "DEEP GENERATIVE MODELS",
    "GET_operation function",
    "DeepBoltzmann machine",
    "Recurrent network",
    "sparse coding model",
    "unsupervised",
    "undirected model",
    "generative",
    "2nd derivative",
    "generative models",
    "MATLAB",
    "path",
    "initial point",
    "Undirected graphical model",
    "Markov chain Monte Carlo",
    "retinal",
    "representations",
    "Encoder-Decoder Sequence-to-Sequence",
    "Graphical model",
    "artificial neural networks",
    "robustness",
    "A directed graphical model depicting the relay race example",
    "sequence-to-sequence RNN architecture",
    "quantifying",
    "2D plane",
    "comprehensively recovered",
    "unbiased",
    "dataset",
    "prevalence",
    "normalization operations",
    "locally connected layers",
    "k-nearest PsyNetneighbors algorithm",
    "8.7 Optimization Strategies and Meta-Algorithms",
    "Harris chain",
    "explicit memory",
    "loss functions",
    "pretraining",
    "minimally-distributed",
    "3",
    "Memory network",
    "isotropic",
    "precision",
    "meta-algorithm",
    "train",
    "gradient clipping heuristic",
    "differentiable",
    "autoencoders",
    "distributions",
    "data generating distribution",
    "deep autoencoder",
    "matrices",
    "data processing",
    "SFA algorithm",
    "drag",
    "negative semidefinite",
    "Boltzmann machine learning",
    "Courville et al. (2011)",
    "TensorFlow",
    "Anthropometricist",
    "apathetic",
    "t-SNE dimensionality",
    "cybernetics",
    "bipartite",
    "normalizing",
    "clusters",
    "L1 regularized objective function",
    "exact inference",
    "exact optimization algorithm",
    "atrociously",
    "minimally-supervised net",
    "deep LSTM",
    "paramount",
    "deep probabilistic models",
    "cosmoid",
    "machine learning",
    "recall",
    "Transmission learning",
    "kernels",
    "estimator of the gradient",
    "conjugate gradient algorithm",
    "dividing line",
    "Deep belief network",
    "k-means clustering",
    "orthonormal basis",
    "probability distribution function",
    "levelling out",
    "estimations",
    "neurons",
    "minimally parametrized",
    "dream",
    "convolution",
    "endorsed by",
    "Prediction",
    "con-",
    "2-D plane",
    "male",
    "2 Independent Component Analysis (ICA)",
    "6.5",
    "functions",
    "online advertising",
    "Theano",
    "mathematics",
    "perceptron",
    "Softmax",
    "minimally Euclidean norm",
    "Non-parametric manifold learning",
    "P",
    "Statistical efficiency",
    "'[logp(h,v)",
    "unrecognised",
    "linear equation",
    "V1",
    "SETTLEMENT II",
    "cosine function",
    "back-propagation",
    "\u039b",
    "Awakens",
    "Gonget al",
    "\u1e0f",
    "REINFORCE",
    "RECURRENT AND RECURSIVE NETS",
    "linear classifier",
    "minimally-polynomial",
    "online shopping",
    "AISTATS 2003",
    "encodings",
    "phylogenetic",
    "Bernoulli or multinoulli",
    "singular value decomposition",
    "utilised",
    "AI system",
    "OMP-1",
    "3rd derivative",
    "stochastic cost function",
    "denoising autoencoders",
    "Gaussian",
    "NADE",
    "hyperparameters",
    "encoder-decoder architecture",
    "saddle point",
    "eigenvectors of H",
    "software",
    "supervised neural nets",
    "sign",
    "deep probabilistic model",
    "scalar",
    "Bagging",
    "tree of depth",
    "optimization algorithm",
    "9",
    "Willow Garage",
    "\u03c3 1(x)",
    "iteratively apply to the solution",
    "topological",
    "algorithm 4.1",
    "generative component",
    "training",
    "data structure",
    "upper layers",
    "2",
    "algorithmists",
    "Markov chain Monte Carlo methods",
    "iterative",
    "Bayesian Linear Regression",
    "artificial neural network",
    "word categories",
    "minimally distributed representation",
    "Generative stochastic network",
    "linear regression",
    "dropout",
    "greedy search",
    "Autoencoders",
    "minimally small",
    "locally connected layer",
    "amplifying noise",
    "undi-rected graphical model",
    "inactive path",
    "eddy currents",
    "inflows",
    "conditional probability distribution",
    "egregiously",
    "Ensipio",
    "momentum",
    "APPROXIMATE INFERENCE",
    "autoencoder",
    "decision tree",
    "long short-term memory",
    "second-order gradient method",
    "equivariance to translation",
    "\u03c32",
    "semi-supervised learning",
    "Gaussian-Bernoulli RBM",
    "linear projection",
    "artificial intelligence",
    "encoding scheme",
    "integral",
    "operation",
    "Parzen window algorithm",
    "recurrent convolutional network",
    "men",
    "supervised single hidden layer MLP",
    "concentrated probability distribution",
    "bosom",
    "stochastic",
    "array",
    "Mean squared error",
    "2 2",
    "BREXIT",
    "algorithm",
    "maxout networks",
    "hardware",
    "Lipschitz continuous",
    "Boltzmann machine training",
    "MONTE CARLO METHODS",
    "logarithm",
    "conditional recurrent neural network",
    "nonlinear generative model",
    "reinforcement learning",
    "gradient-based learning",
    "transpose",
    "Support Vector Machines",
    "prior",
    "2 Conjugate Gradients",
    "female",
    "networks",
    "Gradient descent",
    "gradient descent algorithm",
    "normally distributed noise",
    "sparsely connected",
    "MAP Bayesian inference",
    "optimally",
    "minimally efficient",
    "Batch gradient descent",
    "2 Local Minima",
    "space",
    "CUDA",
    "linear transformation",
    "computational burden",
    "expectation",
    "m n matrix",
    "minimally posterior probability"
]
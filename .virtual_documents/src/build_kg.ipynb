# import re
# import collections
# import pdfplumber
# import os
# import json

# from classBook import Book


# dataDir = "../data/"
# dataName = "Deep Learning.pdf"


import json
import os
import re

import pdfplumber

dataDir = "../data/"
dataName = "Deep Learning.pdf"


class Book:
    num_pages = -1
    book_end_page = "735"
    page_offset = 15

    def __init__(self, name=dataName, dataDir=dataDir):
        self.name = name
        self.dataDir = dataDir
        self.pdf = self.loader()

    def loader(self):
        print("Reading book: ", self.name, "from directory: ", self.dataDir)
        try:
            pdf = pdfplumber.open(os.path.join(self.dataDir, self.name))
        except:
            print("Error: File not found")
            return None
        self.num_pages = len(pdf.pages)
        print("Book loaded successfully")
        print("Number of pages: ", self.num_pages)
        return pdf

    def close(self):
        self.pdf.close()

    def readPage(self, page=-1):
        if page == -1:
            return self.pdf.pages
        else:
            return self.pdf.pages[page - 1]

    def readPageInInterval(self, start_page, end_page, offset=page_offset):
        return self.pdf.pages[start_page - 1 + offset : end_page - 1 + offset]

    def searchStrInPage(self, page, str):
        page_text = self.readPage(page)
        return page_text.extract_text().lower().find(str.lower())

    def getToc(self):
        pages = []
        for i in range(8):
            if self.searchStrInPage(i, "Contents") != -1:
                pages.append(i)
        # reg expression to match '6 Deep Feedforward Networks 168'
        pattern_chapter = re.compile(r"(\d+)\s+(.*)\s+(\d+)")
        # match '6.1 Example: Learning XOR . . . . . . . . . . . . . . . . . . . . . . . 171',
        pattern_section = re.compile(
            r"(\d+)\.(\d+)\s+([\?\,\'\â€™\(\)a-zA-Z\:\s\-]+)\s+.*\s+(\d+)"
        )

        # save to dict
        toc = {}
        for page in pages:
            page_text = self.readPage(page)
            text = page_text.extract_text()
            lines = text.split("\n")
            for line in lines:
                match_chapter = pattern_chapter.match(line)
                match_section = pattern_section.match(line)
                if match_chapter:
                    chapter = {
                        "chapter": match_chapter.group(1),
                        "title": match_chapter.group(2),
                        "page": match_chapter.group(3),
                    }
                elif match_section:
                    section = {
                        "chapter": match_section.group(1),
                        "section": match_section.group(2),
                        "title": match_section.group(3),
                        "page": match_section.group(4),
                    }
                    if chapter["chapter"] not in toc:
                        toc[chapter["chapter"]] = {
                            "title": chapter["title"],
                            "page": chapter["page"],
                            "sections": [],
                        }
                    toc[chapter["chapter"]]["sections"].append(section)

        # add end page
        for chapter in toc:
            try:
                toc[chapter]["end_page"] = toc[str(int(chapter) + 1)]["page"]
            except:
                toc[chapter]["end_page"] = self.book_end_page
            for section in toc[chapter]["sections"]:
                try:
                    section["end_page"] = toc[chapter]["sections"][
                        int(section["section"])
                    ]["page"]
                except:
                    section["end_page"] = toc[chapter]["end_page"]
        # write to json
        with open(os.path.join(self.dataDir, "toc.json"), "w") as f:
            json.dump(toc, f, indent=4)
        return len(toc)

    def loadToc(self):
        with open(os.path.join(self.dataDir, "toc.json"), "r") as f:
            toc = json.load(f)
        return toc

    def getChapter(self, chapter: str):
        toc = self.loadToc()
        page = toc[chapter]["page"]
        end_page = toc[chapter]["end_page"]
        return self.readPageInInterval(int(page), int(end_page))

    # read by sections, since we need to extract the relations between section and entity.
    def getSection(self, chapter: str, section: str):
        # transform section number to index
        section_idx = int(section) - 1
        toc = self.loadToc()
        page = toc[chapter]["sections"][section_idx]["page"]
        end_page = toc[chapter]["sections"][section_idx]["end_page"]
        return self.readPageInInterval(int(page), int(end_page))


book = Book()
# book.getToc()
# toc = book.loadToc()


import math

import IPython
import torch
import wikipedia
from pyvis.network import Network
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer


# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("Babelscape/rebel-large")
model = AutoModelForSeq2SeqLM.from_pretrained("Babelscape/rebel-large")

# Specify the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move the model to the device
model.to(device)


def extract_relations_from_model_output(text):
    relations = []
    relation, subject, relation, object_ = "", "", "", ""
    text = text.strip()
    current = "x"
    text_replaced = text.replace("<s>", "").replace("<pad>", "").replace("</s>", "")
    for token in text_replaced.split():
        if token == "<triplet>":
            current = "t"
            if relation != "":
                relations.append(
                    {
                        "head": subject.strip(),
                        "type": relation.strip(),
                        "tail": object_.strip(),
                    }
                )
                relation = ""
            subject = ""
        elif token == "<subj>":
            current = "s"
            if relation != "":
                relations.append(
                    {
                        "head": subject.strip(),
                        "type": relation.strip(),
                        "tail": object_.strip(),
                    }
                )
            object_ = ""
        elif token == "<obj>":
            current = "o"
            relation = ""
        else:
            if current == "t":
                subject += " " + token
            elif current == "s":
                object_ += " " + token
            elif current == "o":
                relation += " " + token
    if subject != "" and relation != "" and object_ != "":
        relations.append(
            {"head": subject.strip(), "type": relation.strip(), "tail": object_.strip()}
        )
    return relations


class KB:
    def __init__(self):
        self.entities = {}
        self.relations = set()
        self.raw_relations = set()

    def get_wikipedia_data(self, candidate_entity):
        try:
            page = wikipedia.page(candidate_entity, auto_suggest=False)
            entity_data = {
                "title": page.title,
                "url": page.url,
                "summary": page.summary,
            }
            return entity_data
        except:
            return None

    def print(self):
        print("Entities:")
        for e in self.entities:
            print(f"  {e} -> {self.entities[e]}")
        print("Relations:")
        for r in self.relations:
            print(f"  {r}")

    def add_entity(self, e):
        self.entities[e["title"]] = {k: v for k, v in e.items() if k != "title"}

    def add_relation(self, r):
        triplet = (r["head"], r["type"], r["tail"])
        self.raw_relations.add(triplet)
        # check on wikipedia
        candidate_entities = [r["head"], r["tail"]]
        entities = [self.get_wikipedia_data(ent) for ent in candidate_entities]

        # if one entity does not exist, stop
        if any(ent is None for ent in entities):
            return

        # manage new entities
        for e in entities:
            self.add_entity(e)

        # rename relation entities with their wikipedia titles
        r["head"] = entities[0]["title"]
        r["tail"] = entities[1]["title"]

        # manage new relation
        triplet = (r["head"], r["type"], r["tail"])
        self.relations.add(triplet)

        # print(f"Added relation: {triplet}")
        # print(self.relations)
        # raise Exception and exit
        # raise Exception("here")


def from_small_text_to_kb(text, verbose=False, max_length=512):
    """This function takes a text and returns a KB. Note that the text should be small enough to fit in the model's input.
    Specifically, the text should be less than 512 tokens, which corresponds to about 380 English words.

    Args:
        text (_type_): text used to generate the KB.
        verbose (bool, optional): If True, prints the number of tokens in the text. Defaults to False.
    Returns:
        KB: Knowledge base generated from the text.
    """

    kb = KB()

    # Tokenizer text

    model_inputs = tokenizer(
        text, max_length=max_length, padding=True, truncation=True, return_tensors="pt"
    ).to(device)

    if verbose:
        print(f"Num tokens: {len(model_inputs['input_ids'][0])}")

    # Generate

    gen_kwargs = {
        "max_length": 512,
        "length_penalty": 0,
        "num_beams": 3,
        "num_return_sequences": 3,
    }

    generated_tokens = model.generate(
        **model_inputs,
        **gen_kwargs,
    )

    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)

    # create kb

    for sentence_pred in decoded_preds:
        relations = extract_relations_from_model_output(sentence_pred)

        for r in relations:
            kb.add_relation(r)

    return kb


def from_text_to_kb(text, span_length=128, verbose=False):
    """This function takes a text and returns a KB. There is no limit on the text length.
    The text is split into spans of length span_length, and a KB is generated for each span.

    Args:
        text (_type_): text used to generate the KB.
        span_length (int, optional): length of each span. Defaults to 128.
        verbose (bool, optional): Defaults to False.

    Returns:
        KB: Knowledge base generated from the text.
    """

    # tokenize whole text
    inputs = tokenizer([text], return_tensors="pt")

    # compute span boundaries
    num_tokens = len(inputs["input_ids"][0])
    if verbose:
        print(f"Input has {num_tokens} tokens")
    num_spans = math.ceil(num_tokens / span_length)
    if verbose:
        print(f"Input has {num_spans} spans")
    overlap = math.ceil((num_spans * span_length - num_tokens) / max(num_spans - 1, 1))
    spans_boundaries = []
    start = 0
    for i in range(num_spans):
        spans_boundaries.append(
            [start + span_length * i, start + span_length * (i + 1)]
        )
        start -= overlap
    if verbose:
        print(f"Span boundaries are {spans_boundaries}")

    # transform input with spans
    tensor_ids = [
        inputs["input_ids"][0][boundary[0] : boundary[1]]
        for boundary in spans_boundaries
    ]
    tensor_masks = [
        inputs["attention_mask"][0][boundary[0] : boundary[1]]
        for boundary in spans_boundaries
    ]
    inputs = {
        "input_ids": torch.stack(tensor_ids),
        "attention_mask": torch.stack(tensor_masks),
    }

    # generate relations
    num_return_sequences = 3
    gen_kwargs = {
        "max_length": 256,
        "length_penalty": 0,
        "num_beams": 3,
        "num_return_sequences": num_return_sequences,
    }
    generated_tokens = model.generate(
        **inputs,
        **gen_kwargs,
    )

    # decode relations
    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)

    # create kb
    kb = KB()
    i = 0
    for sentence_pred in decoded_preds:
        current_span_index = i // num_return_sequences
        relations = extract_relations_from_model_output(sentence_pred)
        for relation in relations:
            relation["meta"] = {"spans": [spans_boundaries[current_span_index]]}
            kb.add_relation(relation)
        i += 1

    return kb


entityDir = os.path.join(dataDir, "entities")
if not os.path.exists(entityDir):
    os.makedirs(entityDir)
relationDir = os.path.join(dataDir, "relations")
if not os.path.exists(relationDir):
    os.makedirs(relationDir)
rawRelationDir = os.path.join(dataDir, "raw_relations")
if not os.path.exists(rawRelationDir):
    os.makedirs(rawRelationDir)


def from_chapter_to_kb(chapter_str: str, verbose=False):
    print("Generating KB for chapter", chapter_str)

    chapter = book.getChapter(chapter_str)

    kb = KB()
    for page in chapter:
        text = page.extract_text()

        kb_page = from_small_text_to_kb(text, verbose=verbose, max_length=1024)
        kb.entities.update(kb_page.entities)
        kb.relations.update(kb_page.relations)
        kb.raw_relations.update(kb_page.raw_relations)

    with open(os.path.join(entityDir, f"kb_chapter_{chapter_str}.json"), "w") as f:
        json.dump(kb.entities, f, indent=4)
    with open(os.path.join(relationDir, f"kb_chapter_{chapter_str}.json"), "w") as f:
        json.dump(list(kb.relations), f, indent=4)
    with open(os.path.join(rawRelationDir, f"kb_chapter_{chapter_str}.json"), "w") as f:
        json.dump(list(kb.raw_relations), f, indent=4)

    print(f"number of entities in chapter {chapter}: {len(kb.entities)}")
    print(f"number of relations in chapter {chapter}: {len(kb.relations)}")
    print(f"number of raw relations in chapter {chapter}: {len(kb.raw_relations)}")

    return kb


chapters = [str(i) for i in range(1, 21)]

for chapter in chapters:
    from_chapter_to_kb(chapter, 1)
    # pass


def read_chapter_by_sections(chapter, save=False):
    toc = book.loadToc()

    res = {}
    for section in toc[chapter]["sections"]:
        print(f"Reading chapter {chapter}, section {section['section']}")
        section_text = [
            page.extract_text().replace("\n", " ")
            for page in book.getSection(chapter, section["section"])
        ]
        kb = from_small_text_to_kb(section_text, verbose=True)
        res[
            section["chapter"] + "." + section["section"] + " " + section["title"]
        ] = kb.relations

    if save:
        with open(os.path.join(dataDir, f"chapter_{chapter}.json"), "w") as f:
            json.dump(res, f, indent=4)

    return res
